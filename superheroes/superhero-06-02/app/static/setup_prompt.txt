You are an expert software architect with extensive knowledge of architectural and design patterns.
To further enhance your expertise, enabling you to provide better answers to subsequent user's query,
you are provided with the following literature. Carefully analyze and internalize the material, integrating
any new insights into your existing architectural knowledge base.
'''
1. Cloud Adoption Patterns
  Patterns for Developers and Architects building for the cloud 
================================================================

This is a work in progress. We hope you will find it useful even in its infancy as we continue to build out this site.

This project began as a series of (loosely) related papers submitted to the PLoP conference in 2016 - one by Kyle Brown and Bobby Woolf and another by Cees De Groot - at the conference we decided to explore the idea of joining the two papers into a common pattern language. Container DevOps Patterns from a PLoP 2018 paper by Kyle Brown and Chris Hay were added to the growing language, and likewise Kyle also added a set of Event-based systems patterns from a 2006 paper Kyle co-wrote for an internal IBM conference. Joseph Yoder also joined the collaboration, adding Quality Delivery Pipeline patterns from SugarLoafPLoP 2018, Deployment Patterns for Confidence from AsianPLoP 2019, and a set of Strangler Patterns extracted from a PLoP 2020 paper.

This set of patterns is intended for use by Architects, Lead Developers or Senior Developers who are thinking about adopting the cloud, especially as they evolve toward a cloud-native architecture in their projects. It is intended to provide guidance on how cloud-native applications should be designed, how they fit into a larger architectural picture, and how they can be built to operate efficiently. This version specifically addresses issues of systems design, microservices design and microservices efficiency, security and development process. They are currently being evolved, but we have developed at least a basic point of view on each section.

- Cloud Adoption is the place to start the journey. It contains a set of patterns about the different architectural approaches needed to build new cloud-native applications or to evolve existing applications toward a cloud-native approach.
- Cloud Client Architecture is a good place to go next. It starts at the top of the application tree - with a discussion of current client-side application patterns that facilitate cloud-native development.
- Cloud Native Architecture is the logical continuation of the story. It begins to explore the process patterns for identifying microservices and events.
- Microservices then brings in the set of patterns for implementing the microservices found by following the process patterns in the previous section
- Strangler Patterns are important in making the transition from a traditional monolithic architecture to Microservices.
- Event Based Architecture is the core set of patterns for building microservices that are oriented around event processing.
- Coexistence patterns provide guidance for teams implementing Microservices and Event Based Architectures when there is the need to coexist with an existing system for an extended period of time.
- Scalable Store contains the patterns for identifying the right persistence architecture and persistence mechanism for a particular microservice implementation.
- Cloud Native DevOps introduces the DevOps conversation by discussing many of the fundamental patterns around developing, testing and managing a cloud-native architecture.
- Container DevOps continues the discussion of DevOps by introducing patterns around building a DevOps pipeline for and managing the containers that your microservices will be deployed into.
- Image Building follows on from the discussion of Container DevOps by diving into the problem of building container images that are efficient, secure and performant.
- Organization and Process Patterns are required to provide the constraints that team size and composition put on the types of practices that teams can implement.

The relationship between the different sets of patterns in this language are shown below:




################################################################################


  Cloud Adoption 
=================

This part of our pattern language contains patterns that help guide the strategy for cloud adoption. This includes addressing questions on how to move to the cloud. This is influenced by a number of factors–greenfield projects have an easier time than legacy software–and the patterns on cloud adoption should prove useful in selecting what strategy to use in what context.

- Cloud Centric Design should be the starting point for new greenfield projects.
- Implement Monolith First is the right approach for many, perhaps most, new projects rather than going directly to microservices.
- Lift and Shift is the way to quickly move applications onto VM’s in the cloud with minimal changes. It’s a good way to start a cloud journey, but it shouldn’t be the end of the journey.
- Hybrid Cloud Model is something that teams should consider when applying any of the above patterns, as it enables teams to choose the right environment for any application.
- Containerize The Monolith is another option for getting started in the Cloud that sets you up for later refactoring. It’s a good “half-step” onto the road toward Cloud Native.
- Cloud Refactoring is where you begin for existing applications that need to take advantage of Cloud capabilities.
- Hairline Crack shows where to look for places to begin the Cloud Refactoring process of a monolith.
- Strangler Application is the way to perform Cloud Refactoring incrementally while still keeping the application running




Table of contents
=================

-  Cloud Centric Design 
-  Cloud Refactoring 
-  Containerize The Monolith 
-  Hairline Crack 
-  Hybrid Cloud Model 
-  Implement Monolith First 
-  Lift and Shift 
-  Strangler Application 


################################################################################


  Cloud Centric Design 
=======================

Many teams are able to work within what can be termed a greenfield. In that situation, you have a problem to solve, and also have the ability to write a brand new application (or rewrite an existing one) and the time in which to do that application development work. When you are in that situation you still have all of the same project development issues to resolve that you have in an existing application, (e.g. you still need to balance time, cost and people) but you often have more flexibility to choose among how you will trade off each of these.

How do you design your application to take the maximum advantage of all the features of the cloud for the best future proofing and agility? What can a team do to make sure that they are balancing speed, cost and people utilization in the best combination?

Therefore,

Begin any new greenfield projects by performing a Cloud-Centric Design of your application to run natively on a cloud (PaaS) environment such as a container-based environment.

The difference between a greenfield and a brownfield is not always clear-cut. Especially when corporate data is part of the equation, it’s often challenging to completely dissociate an application from the IT environment in which it is conceived. Nonetheless, when you have the opportunity to make a clean (or mostly clean) break from existing standards and processes, it allows you more freedom to adopt new approaches that can be more productive in the long run.

So, whenever you are developing a new application that can run in the cloud, instead of asking yourself “How do we conform to existing standards for application development” ask yourself “What can be gained by trying new approaches?”

Some of the important considerations that you will want to consider when embracing a Cloud-Centric design include:

- Building with Fine-grained components that give you the ability to test more easily than large monolithic components.
- Employing decoupling measures such as REST API’s or Event-driven approaches to keep boundaries between components clean.
- Minimizing the use of state (it is impossible to entirely eliminate state, but you should only keep the state that is absolutely required).
- Following a zero-trust model to minimize privileges for security.
- Practicing Immutable deployment models to remove runtime changes and eliminate runtime administration.

As a result of addressing these considerations, a Cloud-Centric Design would embrace the following approaches:

- It would be based on the Microservices Architecture in order to minimize the Blast Radius of any code changes.
- It might take advantage of Polyglot Programming in order to make sure that the right languages and development tools are chosen for each part of the job.
- It would take advantage of Scaleable Stores that allow development teams to match the data storage technology to the type of data that being stored and to the data storage and retrieval patterns that are most prevalent for that application
- It can take advantage of Cloud DevOps.


################################################################################


  Cloud Refactoring 
====================

You want to migrate an existing application onto the cloud. You have a little bit of leeway on how many changes you can make to the application and its configuration but you are under time and budget constraints.

How do you minimally adapt an existing application to work on the Cloud? You can’t afford to rewrite the application from scratch, but you do need to adapt it to take advantage of the features of a PaaS.

One of the misconceptions of Cloud computing is that you have to adopt all of the new technologies that make up a Platform as a Service all at once. However, it doesn’t have to all happen at once. You can incrementally update your application code, your deployment and management processes and your user experience.

Therefore,

Perform Cloud Refactoring to change the most egregious dependencies on existing environments until the application can easily be deployed as PaaS components. Over time you may refactor more components one at a time to adopt more cloud-native technologies.

Some steps that can guide you along that journey are:

Fix the biggest issues only. There are certain aspects of running on a PaaS that are, in some ways non-negotiable. PaaS’s are generally not going have the same networking configuration as your on-premise systems, so reconfiguring your application to, for instance, look up IP addresses and host names from a configuration system or file will be a step in the right direction. The individual runtime instances in a PaaS (such as a Cloud Foundry runtime or a Docker image) are generally not going to have the same level of uptime as a carefully managed local machine or on-premise VM. So another step would be to make sure that you can support a high enough level of redundancy in your application that the overall application functionality can survive the loss of a single runtime instance.

As part of this effort, you want to also think about revisiting your application packaging structure and adapting some new packaging practices without even changing your code. A common issue in many application teams using Java is that over time they have built ever-larger EAR files to contain our logical applications so that those applications could be more easily deployed to all the application servers in a corporate farm. The problem is this tied each piece of code in that application to the same deployment schedules, and the same physical servers. If you changed anything you’d have to retest everything, and that made any changes too expensive to consider. But now with PaaS technologies the economics have changed. So one simple place to start is in reconsidering your packaging. There are three principles that you can apply:

Look for ways to evolve your operations processes and toolsuite incrementally. One of the most common complaints about moving to a PaaS is that operations teams claim that they will not have the same level of insight or control over production applications as they currently do over on-premises versions of those applications. But instead of making this a blocker for a move to a PaaS, look for ways to compromise without sacrificing the most important aspects of the partnership between development and operations. One way to do this is to keep two separate, but very similar development pipelines. Instead of starting off with a single development pipeline that can push an application all the way to production, have one pipeline that can be kicked off by development that can deploy into every stage aside from production. Build a second development pipeline derived from the first, but that may perform additional checks and tests such as security scans and corporate compliance scans and that is capable of pushing the application into production, and then lock down that pipeline to only members of the operations team.

Apply the Strangler Application Pattern. The Strangler Application is an approach that works very well in Cloud Refactoring as an aid to incrementally updating the technologies used in an application, especially new user interface technologies.


################################################################################


  Containerize The Monolith 
============================

You are migrating an existing system into the Cloud. Your application is relatively up-to-date, in that it is based on versions of standards and frameworks (such as JEE or Spring) that are supported by recent versions of middleware. What’s more, your middleware vendors support a containerization strategy.

What is the best approach for moving to the cloud quickly, but still giving you the flexibilility to do development or deployment on-premises?

Most traditional application development has been done on bare-metal hardware or in VM’s. However, in the last few years, a new option has emerged; container platforms like Docker.

Containers have several advantages over VM’s. Notably, a Docker container will run identically on any host, anywhere, that supports Docker. that means that when you containerize an application that you have maximal flexibility in where to deploy your application. By contrast, VM’s are not nearly as portable - a KVM VM image will not run natively on AWS or in a VMWare-based private cloud.

However, not all middleware is supported in containers. Even when a vendor (such as Red Hat or IBM) supports middleware on containers, they generally only support relatively recent versions of that middleware. If your application is running on an older version, either you are on your own in trying to containerize the middleware (often running into support issues) or it simply will not run on containers at all.

But when you find yourself in that sweet spot where your application is compatible with a vendor-supported middleware stack, you have more options than you otherwise would.

Therefore,

Containerize your monolithic application as a first step onto the cloud. You can continue to run your application on-prem in Docker if needed, but this allows you to move to multiple public and private cloud options over time.

Containerizing the Monolith is better option for those cases where a Lift-And-Shift will result in more work over time, e.g. when the application is being actively maintained and will eventually need to be moved to the cloud or refactored. However, containerization is only an option when the application itself is maintained by an application team; COTS applications are not good candidats for containerization in most cases because the vendors will not support them in that form.

Containerizing the Monolith is often only a half-step, however. In some cases, particularly small monoliths, if the team is able to deliver features at the rate and pace you need, then containerization may be the final step in your journey to the cloud. However, for larger monoliths where the complexity of the monolith and/or testing time makes it impossible to deliver features at the rate needed, you will still want to look at Cloud Refactoring over time as that will result in the best overall value if the application is sufficiently complex to warrant division into microservices. In any case, once you decide to adopt containers, you will need to deal with the changes to your DevOps processes tha containerization will warrant by moving to a Container Build Pipeline.


################################################################################


  Hairline Crack (aka Fracture Plane) 
======================================

You are evaluating an existing application for cloud adoption. You know you want to take some or all of the functionality of the application and re-architect it for microservices in order to improve the maintenance of the application and to reduce the blast radius of new changes to the application.

But how do we migrate a monolith into a collection of services? It’s not easy to understand all at once how a monolithic application can be broken apart into microservices.

How do you find a way to identify the areas within an application that might be amenable to refactoring into a microservice?

What you must look for are the places where the pieces of a monolith that will be easy to “force apart”. This is usually indicated by the presence of an existing API to the outside world - if an application has multiple API’s that it exposes then there are often good odds that one or more of them can be split apart from the others.

Therefore,

Look for Hairline Cracks - these are the places where the monolith will be easy to break.

The places where a monolith can be forced apart most easily are those places in the application where there are already some sort of interface between sections of the monolith. This is analogous to a hairline crack in a piece of metal or a Fracture plane in some rocks. In large monolithic Java applications, there are at least three very simple cases where you can find obvious cracks that can be exploited regarding services. These three cases are:

Case 1: Existing REST or JMS services – This is by far the easiest case for refactoring. It may be that you have existing services that are already compatible with a Microservices architecture, or that could be made compatible. Start by untangling each REST or simple JMS service from the rest of the WAR and then deploy each service independently. At this level duplication of supporting JAR files is fine – this is still mostly a question of packaging. Here you have begun the process by splitting things off into what may be called a “Macroservice”, but remember that this is just a step toward a refactoring into Microservices. Your application will still need to not only follow the principle of doing one and only one business function, but also you will need to make sure you’re following accepted Cloud DevOps principles.

Case 2: Existing SOAP or EJB services – If you have existing services, they were probably built following a functional approach (such as the Service Façade pattern). In this case, functionally based services design can usually be refactored into an asset-based services design. This is because in many cases, the functions in the Service façade were originally written as CRUD operations on a single object. In the case where that is true, the mapping to a RESTful interface is simple – just re-implement the EJB session bean interface or JAX-WS interface as a JAX-WS interface – you may need to convert object representations to JSON in order to do this, but that’s usually not very difficult, especially where you were already using JAX-B for serializations. In cases where it’s not a simple set of CRUD operations (for instance account transfer) then you can apply a number of different approaches for constructing RESTful services (such as building simple functional services like /accounts/transfer) that implement variants of the Command pattern.

Case 3: Simple Servlet/JSP interfaces. Many Java programs are really just simple Servlet/JSP front-ends to database tables. They may not have what is referred to as a “Domain Object” Layer at all, especially if they follow design patterns like the Active Record pattern. In this case, creating a domain layer that you can then represent as a RESTful service is a good first step. Identifying your domain objects by applying Domain Driven Design will help you identify your missing domain services layer. Once you’ve built that (and packaged each new service in its own WAR) then you can either refactor your existing Servlet/JSP app to use the new service or you can build a whole new interface following the Multichannel Architecture approach.

In any of these cases, what you want to do is then to apply Microservice Design in order to determine what design you want to refactor toward.


################################################################################


  Hybrid Cloud Model 
=====================

One of the earliest decisions many teams make (although perhaps too early) when moving to the cloud is deciding what destination that they should move to. A common misconception is that moving to the cloud means moving workloads to a specific public cloud. The problem is that this is too simplistic to reflect the real world - you can have cloud computing without necessarily moving to a public cloud, and what’s more, systems can actually cross multiple clouds, both public and private.

How do you balance the needs of security, productivity, cost-effectiveness and skills development when moving to a cloud computing approach?

In order to understand why the simplistic model of “cloud==one public cloud” is insufficient, let’s go all the way back to the 2011 National Institutes of Standards (NIST) definition of Cloud Computing. NIST defined cloud computing as “a pay-per-use model for enabling available, convenient and on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.” Now let’s break that apart. Pay-per-use does NOT necessarily mean you are paying another company - pay-per-use is a statement of the accounting on the user side. Many large IT shops have cost accounting systems that allow them to bill resources by the use, per user. What’s more the self-provisioning statement also does not necessarily imply that the computing resources must be held by a separate company. Again, many large IT organizations also pool resources, and provide self-provisioning capabilities within their IT infrastructures.

In short, cloud computing can embrace multiple models, both on-premise and off-premise. While there are obviously many cases in which a public cloud provider is exactly the right choice for many projects, there are perhaps just as many in which it is the wrong choice for a particular workload. There are several factors which weigh on this:

-  Security and Data Residency: While public cloud security has improved to the point that it is possible to build cloud-based systems that are as secure if not more secure than on-premises systems, the fact remains that the perception of cloud-based systems as being less secure remains. So long as that perception remains, many companies will be reticent to put their most important data onto a public cloud. However, while the overall security perception is only in the mind, what is a hard reality is the fact that many countries have a data residency requirement that insists that private information about their citizens remain within their borders. This means that for those solutions that are based in a public cloud that does not have a presence in that particular country that the best choice for compliance in storing that data remains in an on-premise data store. 
-  Productivity and Skills: Teams that have built and operate entirely cloud-native systems can be enormously productive, as research such as Forsgren has decisively proved. However, being that productive requires that teams aquire a skill set around Agile methods, particularly the technical practices of Agile methods, DevOps and cloud native technologies such as Containers and Automation. Many teams do not yet have that skill set. What’s more, they do not have the support within their organization to enable them to acquire that skill set. What we have seen is that there is no magic in the cloud - if you build, manage, and operate a system on the cloud in the same way in which you have built, managed, and operated systems on-prem, then there is no net new benefit derived from being on the cloud in this regard. It will take just as long to build systems, it will take just as long to test them, and tit will take perhaps even more time to operate them and solve production issues due to the extra complexity added by an unfamiliar VM, storage and networking environment. 
-  Cost-Effectiveness: No myth is more persistent than that that the cloud is always cheaper than any on-premise option. The fact is that the cloud can be cheaper, and substantially so, but only if an application is designed and enabled to take advantage of the technical features that make the cloud cheaper. There are many of these, but the simplest to understand is elasticity - if your application is able to scale horizontally both up and down in response to traffic, then you can save a substantial amount of cost related to compute infrastructure by only paying for the infrastructure you need, when you need it. However, all applications are not designed to take advantage of horizontal scalability. If an application is built around the assumption that servers are forever fixed in size, number and connectivity to one another, then this cost savings is illusory. 

Security and Data Residency: While public cloud security has improved to the point that it is possible to build cloud-based systems that are as secure if not more secure than on-premises systems, the fact remains that the perception of cloud-based systems as being less secure remains. So long as that perception remains, many companies will be reticent to put their most important data onto a public cloud. However, while the overall security perception is only in the mind, what is a hard reality is the fact that many countries have a data residency requirement that insists that private information about their citizens remain within their borders. This means that for those solutions that are based in a public cloud that does not have a presence in that particular country that the best choice for compliance in storing that data remains in an on-premise data store.

Productivity and Skills: Teams that have built and operate entirely cloud-native systems can be enormously productive, as research such as Forsgren has decisively proved. However, being that productive requires that teams aquire a skill set around Agile methods, particularly the technical practices of Agile methods, DevOps and cloud native technologies such as Containers and Automation. Many teams do not yet have that skill set. What’s more, they do not have the support within their organization to enable them to acquire that skill set. What we have seen is that there is no magic in the cloud - if you build, manage, and operate a system on the cloud in the same way in which you have built, managed, and operated systems on-prem, then there is no net new benefit derived from being on the cloud in this regard. It will take just as long to build systems, it will take just as long to test them, and tit will take perhaps even more time to operate them and solve production issues due to the extra complexity added by an unfamiliar VM, storage and networking environment.

Cost-Effectiveness: No myth is more persistent than that that the cloud is always cheaper than any on-premise option. The fact is that the cloud can be cheaper, and substantially so, but only if an application is designed and enabled to take advantage of the technical features that make the cloud cheaper. There are many of these, but the simplest to understand is elasticity - if your application is able to scale horizontally both up and down in response to traffic, then you can save a substantial amount of cost related to compute infrastructure by only paying for the infrastructure you need, when you need it. However, all applications are not designed to take advantage of horizontal scalability. If an application is built around the assumption that servers are forever fixed in size, number and connectivity to one another, then this cost savings is illusory.

Therefore,

Choose a Hybrid Cloud model that allows project teams to choose the right location, on-premise or off-premise, private or public, that fits their needs for security, data protection, cost-effectiveness, and skills

Existing systems that were not built to take advantage of the cloud will gain little from being moved to the cloud without change. In fact, they may become harder to manage, debug and operate. On the other hand, there are many ways in which you can gain advantages if you consider a model that may mix the public cloud with on-premise private cloud and traditional models.

If you have an existing system that contains valuable data that could be useful to your clients or customers, then it may be helpful to expose that data as an API from your on-premise system to a new application that can be built in a cloud-native way. You can then deploy that application in a public cloud, or perhaps in a private cloud.

Another common option when modernizing an application is to apply the Strangler Application and to deploy the application in a split mode across both a traditional environment and either a public or private cloud. This may be combined with Containerize the Monolith if adoption of containers and a Container Orchestrator such as Kubernetes is also part of a long-term strategic plan.


################################################################################


  Implement Monolith First 
===========================

Sometimes, time is of an essence in a greenfield project and microservices are known to add overhead.

How do I launch a new application quickly?

Greenfield projects can be built either in completely new companies (like a startup company), or in projects run within an existing company but having a lot of the properties of a startup. It is quite possible that, for example for compliance reasons, the company’s existing infrastructure is not available for deployment, leaving a project team with a completely blank slate.

Regardless of whether in a startup or as part of an existing company, such projects usually also face significant time pressure. An early software startup needs to prove, with code, that its ideas are sound. A team in an existing company often has a sponsor that may very well act in the same manner as a venture capitalist.

In either case, time is of an essence and developing or acquiring a microservices platform to do it right the first time usually does not provide any business value at this stage. Ideas need to be proven, and whether these ideas are quickly jotted down or carefully calligraphed is of no consequence.

Therefore:

Implement a monolith first. Refactor to a microservices architecture only if the application is sufficiently complex to warrant refactoring, and after the application has proven it is worth its salt.

Teams in this situation often grab for very fast development stacks. An extremely popular choice in the startup world is, for example, Ruby on Rails with deployment on a public PaaS provider like Heroku. There is almost zero friction in moving such an application from the development environment to the production environment, and pretty much from day one the team is able to continuously release and show progress to their sponsor. With the focus on business logic, the team’s time is spent very effectively at this stage; more effectively than when the team would distract itself with doing things “the right way”, because the right way is simply not known at this stage: when the application is not successful, the right decision is to shut it down, not to rearchitect it into something more beautiful.

It’s also quite possible (and common) to containerize these “mini-monoliths”. This is not quite in conflict with the advice to implement One Service Per Container in that given the size of the application, the container footprint would still be relatively small. Grabbing a technology stack image from DockerHub and then deploying to a public Container-As-A-Service is also a quick, easy, lightweight way to get started quickly.

What does need to be kept in mind, is that if the application is successful, it will probably see more investment in terms of adding development capacity and more load in terms of usage. There will be a point in time when the initial monolith will be deemed cumbersome because neither development effort nor usage scale well with monolithic applications.

It is therefore extremely helpful if the team aims for success and keeps in mind that later on, having Hairline Cracks in the application will make breaking it up simpler. If not, a Strangler Application strategy may need to be followed to replace the monolith with new services, which is usually more expensive.

Martin Fowler first discovered this pattern.


################################################################################


  Lift and Shift 
=================

You’re evaluating a number of different cloud provider options. You have an existing application in mind you want to move to the cloud, but you don’t have the luxury of rewriting it. The application may have dependencies on existing databases, or may be written using middleware that is not available from a PaaS provider, or may simply be architected in a way that makes it incompatible with refactoring using the Strangler Application approach. In other situations, the development team is not available to work on refactoring the application - as may be true if the application is a COTS application or if the development team is an offshore team.

In this case, how do you gain the advantages of cloud by deploying an application to the cloud while minimizing the changes needed to be made to the application? You need to make the application environment in the cloud match as closely as you can to the environment it originally ran in on-premise in order to avoid changing your application code.

Therefore,

Choose an IaaS cloud provider that can create the closet possible match to the environment the application was originally built for. Then perform a Lift and Shift of an existing application onto the cloud platform.

A Lift and Shift migration is the most straightforward of the types of cloud migration that you can take on. In particular, the first rule of a Lift and Shift migration is in this migration approach you try to avoid changing any of the code of your application - the point is instead to change the cloud to match your existing environment. There are three basic steps that you need to ensure in order to make a Lift and Shift possible.

Note that a Lift-And-Shift is just the starting point in a cloud journey. It’s not where you want to end up. Even to be successful with a Lift-And-Shift you need to adopt several Cloud-Native DevOps practices. At a minimum, Automating VM Deployments will be required in order to improve your overall ROI and begin to gain any benefits from Cloud. Likewise, the same thoughts about reliance that apply in cloud-native applications such as Three Data Centers and One Coast apply to traditional application resiliency as well, although the implementation choices may differ on which technologies you choose to perform data replication. A common approach, for instance, is to use existing approaches like Oracle DataGuard for replication across two datacenters, with backups being regularly taken and restored to a third datacenter.

Finally, a Lift-And-Shift is an opportunity to begin thinking about how you can then move on to a more cloud-native approach. After the Lift-And-Shift is completed, you should begin the process of looking for Hairline Cracks and consider taking a Strangler Application approach to refactoring in place.


################################################################################


  Strangler Application 
========================

You are starting with a legacy monolith that is implemented as a Web application. When migrating legacy applications to the cloud you often need to consider how to replace the application in a piecemeal way. Often, a legacy monolith does not show obvious Hairline Cracks. How can we replace it with a Microservices Architecture without doing a full rewrite? A full all-at-once rewrite and replace is a risky operation and would also be expensive and would not show value until the complete rewrite is finished.

You need an approach that allows you to avoid a full “big-bang” rewrite and replace, yet still allows you eventually replace the entire monolith.

Therefore,

Use the structure of the web application and HTTP to redirect requests for parts of the application away from the legacy monolith and towards a new implementation of those features. Add features incrementally to the new application (the strangler application) and update the redirection logic towards the new features as they are added.

An important consideration when applying the Strangler Application is that you have to consider that there is a difference between the release management of an entire application and the release management of an individual microservice. The concept that has become most important for teams adopting this process is the concept of a Chunk. Chunks that are groups of Microservices and associated user interface components that are tied together either by implementation restrictions or User-Experience relationships that become initial large-scale release units.

Identifying a chunk is done by comparing the existing functionality of the application with the desired functionality of the application while you specifically focus on the user interface flows that the new microservices will change or interrupt. For example, consider that you have a retailing website implemented as a monolith built using page-at-a-time templating technologies that you want to refactor into microservices. A retail website has several common aspects - you want to be able to browse a catalog, search for items, add items to a shopping cart, and check out. Some of these elements, like item search, are relatively self-contained and so would make good first choices for refactoring into microservices. However, if we consider a more complicated aspect of the website like checkout you can see that it may contain multiple different aspects to a user interface flow - you want to verify availability of items, show tax calculations, obtain shipping information, and obtain payment information. Each of these aspects individually may make good microservices. But you can’t very easily split them apart from the other parts of the checkout flow.

So you may rebuild the checkout process as a new Single Page Application, that then calls down through one or more BFF’s into Domain microservices for Inventory, Tax, Shipping and Payment. But for the initial release, the entire Checkout process must be updated at once, or the resulting flow could be jarring to the user as they flip back and forth between two different user experiences. After the initial release, each individual microservice could then be updated separately from the rest as the individual delivery pipelines can be independently managed.

Reference: Martin Fowler’s Strangler Application


################################################################################


  Cloud Client Architecture Introduction 
=========================================

This set of patterns is built around developing a consistent architecture for application clients that is compatible with a cloud-native application development style. It may seem odd that we devote an entire chapter of this pattern language to front-end patterns that may not depend on the cloud at all, but there are several really good reasons for this that are worth discussing.

The first reason clients are important for cloud-native applications is business value. One of the hardest parts of a cloud adoption, and the part that most IT teams fail at, is demonstrating real value to the business in tangible terms. One of the easiest ways to demonstrate value to the business is by improving the user experience (UX) of customers or associates to either produce more direct sales, simplify interactions and improve efficiency, or to enable value-add services that increase revenue. Most product managers have a long list of desired improvements to their user experience, but these are often hampered by issues below the user interface layer itself, which may include problems with both technology and process. Thus, demonstrating the value of cloud as part of improving user experience often helps sell the effort and expense of converting to cloud. What’s more, once the user experience code is on the cloud, it’s often easier to either move or reimplement since it is already both close to the user (reducing latency) and the front end is cleanly separated from the complexities of the application’s back-end. As a result, most companies tend to start with client-facing applications as their first forays into cloud technologies.

However, there is a second issue, even bigger but more nuanced, that companies must consider when rebuilding or refactoring existing applications, and that is the cost of training teams on new front ends. One of the most easily overlooked costs in adopting a new system is training cost and the costs to the business of lost productivity when users must become productive on a new system. As a result, training and productivity costs are an important consideration when deciding how to segment a larger monolithic application into smaller pieces for refactoring. Essentially, in a minimum viable product (MVP) approach, the word “viable” becomes much more important when considering front-end refactoring and reimplementation. In an article on the Thoughtworks Blog , Meaghan Waters points out three potential approaches that may be helpful:

- End user segmentation: This approach is useful when you have several disjointed (heterogenous) groups of users. If you can focus on the experience of a single user community at a time, then that gives you a potential way of segmenting a large monolithic application and moving it into the cloud as a series of smaller applications, each broken off from an original monolithic system.
- Process segmentation: This approach is useful when your users are more homogenous. In this case, you look for boundaries of business processes and rewrite each individual business process. Handoffs between users, even of the same general user group, are good indications of the boundaries of each process. Likewise, if a process comes to completion or transitions to a new stage in a point in time, this is also a good place to break up a larger interface into smaller aspects.
- “Competitive” system: With this less common approach, you replace one existing system bit by bit with a new competitive system, perhaps for new customer acquisition while existing customers use the current system. In this way, you can start new customers with a less complete system and let them gain more features over time.

Any of the three segmentation approaches can work; choose the one that fits the current application best. All three require you to build new front ends that fit the approach as part of an architectural process that also refactors the back-end business logic to correspond with the front end. Whenever you refactor the front end and back end simultaneously, be careful to keep the two ends separated, to keep business logic separated from UI logic, and to encapsulate that business logic in the back end. In a 2021 Twitter thread , Betsy Haibel illustrates a particular trap that is easy to fall into:

“Web Frontends and their backends are logically one product. You get a lot of feature requests that require changes to both. When this happens, developers have a choice. They can build the feature “the right way” and put the business logic in the backend and go through the moderate pain of shipping both changes, or they can put it all in the frontend. Over time, the weight of all these changes puts ever more business logic in the front end.”

Practically, what this means is that you should NEVER assign a front end and its associated back-end to separate teams. Both should be built by a single Stream Team that owns the saga that describes the feature as a whole. This applies to all of the patterns that we are describing. Teams should never be split vertically by role (front-end and back-end developer) but should instead be organized by feature split in any of the ways illustrated in the Waters article referenced above. The patterns in this section are all intended to be applied within that organizational framework.

One of the fundamental principles that sits behind all of the advice we will provide in these patterns is that there should be a distinct and firm boundary between UI code and business logic. While this advice is so old that giving it again here may seem almost quaint, the problem is that in practice, many teams fail to follow this principle. Perhaps one of the best examples of this architectural principle is Alistair Cockburn’s Hexagonal Architecture , sometimes called the “Ports and Adapters” architecture. In this approach, he separates the core business functions of an application from the four different styles of interaction that humans or other systems have with the application (see below). These are user interface, persistence, administration and notification. The concept is that the core business logic is isolated from these interactions by specialized “adapters” that operate on more abstract “ports” for each style of interaction.



If we only concern ourself with the user interface portion of this approach, then we realize that what this approach was presaging was the separation of the code of a Domain Microservice from the code that calls the Domain Microservice. In fact, the entire Microservices Architecture is an architectural and organizational mechanism for enforcing this separation.

This concept of an application core surrounded by different types of adapters for different purposes illustrates a key concept that the patterns in this section implement. The different UI styles we will introduce are all meant to be separated from the business logic they rely upon. In a sense, they are all “adapters” in a Hexagonal Architecture.

What this discussion illustrates is that there are a handful of basic principles about client architecture that both ties back to many of the other principles and patterns we have discussed so far in this book, and that also inform the patterns that we will introduce in this section. These principles are:

The patterns in this section all stem in one form or another from these basic principles. The patterns include the following:

- Multimodal Architecture is the root pattern of this section and describes the overall structure of applications that must work across multiple types of user interaction. This structure is important because most modern applications have multiple types of user interfaces that all must come together in a consistent user experience that may cross mobile applications, web applications, and various types of social media interaction.
- Interaction Model is a way of explicitly capturing and representing each particular style of user interaction. An Interaction Model is a way of representing an interaction with a user in a particular technology stack or channel. It allows for separation of user interface and business logic, while encouraging commonality across interactions of different types.
- Web Applications are often the default interaction model option for small, simple applications. This approach combines server-side rendering (using a template language) with simple forms-based navigation. It is well suited for interacting with users who are on less-than-ideal hardware platforms, and places few requirements on the user’s side beyond opening a URL in a Browser application.
- Single Page Applications are a common approach to building complex client-side web applications that fully leverage the power of modern browsers. These applications take advantage of the power of browser-based JavaScript, CSS and HTML to build highly interactive user experiences.
- Micro Frontends are a way to avoid recommitting the mistakes of monoliths within a client-side application built using the Single Page Application architectural approach.
- Native Mobile Application describes the most customizable solution for consumer-facing client applications. While Web Applications are still enormously common, and are often the best solution for internal corporate applications, consumers now expect the bulk of their interactions with many companies to take place through a dedicated mobile application.
- Social Media Plugins are a way to interact with the now ubiquitous social media platforms. Given how much time users spend on social media platforms, organizations would be wise to ensure that they can provide services to their customers and employees through these platforms.
- Chatbots are a common and important user interface approach for specialized types of user interaction such as customer support. Chatbots are almost never the sole means of user interaction with a client, but are instead an important component of other client interaction styles such as mobile applications, web sites and social media plugins.
- Document Generators are a way of interacting with humans through a more than 500-year-old technology – the printed page – and its digital equivalent. Many business processes still require or use physical or digital documents for legal, regulatory compliance or other purposes. A Document Generator provides a way to create these documents and transmit or store them.
- Command Line Interface is a way of interacting with the user at an operating system command line. This is important because many simple types of actions the user may want to take in order to automate interaction with the system are more easily done at an OS command line than through other mechanisms.
- Public API is the mechanism for making your application services available to the world of developers outside of your own team. They are important wherever you have interactions with third parties such as suppliers, business partners or governmental entities or if you want to build an ecosystem of independent developers around your services.

The relationships between the patterns in this section are shown below:




Table of contents
=================

-  Chatbot 
-  Command Line Interface 
-  Document Generator 
-  Interaction Model 
-  Micro Frontend 
-  Multimodal Architecture 
-  Native Mobile Application 
-  Near Cache 
-  Public API 
-  Single Page Application 
-  Social Media Plugin 
-  Web Application 


################################################################################


  Chatbot 
==========

Technologies like a Single Page Application or Native Mobile Application (or any traditional page-based GUI) lends themselves well to a finite set of actions arranged as a common progression. What happens when the starting place is essentially undefined and the progression unpredictable?

How do you help a user navigate in order to perform an action or find data if they are not sure what they’re interested in?

In many types of applications, such as customer support, it is often hard to build a web site or application that naturally allows people to go directly to the specific piece of information that they need. Users resist navigating deep hierarchies of static pages, and FAQs become unwieldly when there are more than a few dozen entries. Search tools like Google can alleviate this somewhat, but that depends on both the information developer and the user seeing eye-to-eye about the phrasing of keywords and other aspects of search engine optimization.

What is needed is some way to meet these users where they are - to give them an interaction that either feels like interacting with a human, or that may be actually interacting with a human, intermediated through a program that may first organize the category of help and other prerequisites that a human requires to solve their problem efficiently.

Therefore,

Communicate with users using their own Natural Language. Build a chatbot as a service that can be plugged into different web sites, mobile applications and social media platforms.

Chatbots are not a new form of technology - in a sense they can be traced back all the way to the Eliza program written by Joseph Weizenbaum in 1966. However, recent improvements in Natural Language Processing have made it easier and more efficient to build them than before. Most chatbots share a common set of architectural features that allow them to be built easily from existing open-source libraries or commercial web-based API’s. Therefore, they can be implemented as microservices that can be called from within mobile apps, web apps, chat applications (such as Slack or Discord) and social media platforms. An example of how a Chatbot would be invoked (or embedded within) one or more other types of Interaction Models, along with the relationship between the Chatbot and other patterns is shown below:



An important aspect of a Chatbot that is different than many of the other Interaction Models we have seen is that a chatbot can help the user navigate amongst through a decision tree of choices. Rather than a window with fields for name, address, and phone, the chatbot can ask questions of the user interactively: What is your name? What is your Address? What is your phone number? If the information has already been provided, it can simply skip the question, or ask the questions in different orders.

The Chatbot itself must take advantage of a set of Natural Language Processing Services that typically parse incoming text and then extract the “intent” (e.g., the relevant domain concept in terms of predetermined entities and actions) from the incoming text. Likewise, the Chatbot will then either directly or through the intermediation of the Natural Language Processing Services call one or more Domain Services to perform queries whose results are then formatted by the Natural Language Processing Services as output text, or to perform some set of actions based on the user’s intent. This sort of typical Chatbot architecture is shown below:



For instance, for simple cases, the open-source Chatterbot library is a machine-learning based conversational dialog engine for Python. It allows you to build or customize logic adaptors that take in preprocessed statements from the user and either provide you the best match to a response, or invoke other adaptors for more specific domain types (like date/time or math). The system learns from a set of provided example statements in order to determine the best response.

The system that brought Chatbots back into common use, and still one of the most sophisticated, was the IBM Watson natural language engine, derived from the research system that won the game show Jeopardy! against top human competitors in 2011. This has now evolved into the Watson Assistant suite of services, that work from a set of provided Intents (which are goals learned from multiple examples) in order to understand how to respond to a particular statement. Likewise in Watson, you can define specific Entities that are found in the conversation, and even specify a conversational flow for a complex interaction that includes slots to hold the different entities you identify (such as the time for a dinner reservation). Actions are the tasks that the system undertakes on a user’s behalf (such as searching for information as in what time tables are open at the restaurant).

Somewhere in between the two ends lie the Amazon Lex service, which is connected to the suite of services that includes the Alexa voice service. It also allows you to define Bots built up from Intents derived from samples, but fulfills each matched intent with a custom function built on AWS Lambda.

An important consideration in building a Chatbot is that you have to consider that it is a user interface channel into an Enterprise application much like any other user interface choice. That means you have to plan for connecting requests from your chatbot into the back-end API’s that your system presents, although due to the conversational nature of the interaction, these API’s often end up being heavily mediated to strip down large amounts of data into smaller answers more uniquely suited to the nature of chat. That means that patterns like Backend for Frontend become even more important as you consider chatbots as part of a front-end architecture.


################################################################################


  Command Line Interface 
=========================

You are building an application following a Multimodal Architecture, and your application may need to be used in a repeated or automated way – e.g. you may want to repeat an operation on a schedule, or repeat an operation many times, such as over a set of inputs.

How do you plan for automation of the services in your system? How can you make it easy to automate activities like bulk loads, bulk changes, or scheduled execution of activities?

Let’s face it, when most of us think about the user interface of a system, the most common interpretation of what the term “user interface” means brings to mind a graphical user interface of some sort. Since the advent of the WIMP (Windows, Icons, Mice, Pointers) paradigm in the 1980’s and early 1990’s, it’s hard to conceive of any other way to interact with a computing system. In fact, two entire generations of developers have grown up in which this is may be the only way in which they have ever interacted with computers.

However, there are times in which a WIMP approach, be it on the Web, or in a mobile or desktop application has its limitations. The simplest case of this is when a procedure needs to be automated. Let’s says that you have a simple Web page that shows you how many people are registered for an event, such as a Meetup or a book club. If no one registers for a particular event, you will still be charged for the event venue, even if no one shows up. It would be great if you could look at the event signup just prior to the cutoff time, see if anyone is registered, and if not, cancel the event.

If you have only a Native Mobile Application or Web Application, then unless the developer adds this functionality to the application, you are out of luck. You might be lucky and the developer may have provided you with an API to the application, but that is not guaranteed, since Web Facing API’s present their own challenges to the application developer, nor is it necessarily something that would make it easy for you to write your simple check and cancel action.

What we need is something that facilitates these simple types of automation tasks, yet at the same time, doesn’t require the complexity of programming to an API.

Therefore,

Build a Command Line Interface (CLI) for your system that allows you to execute APIs at the operating system command line.

A Command Line Interface will be built up out of individual CLI Commands that each correspond to the major functions of your application. If you build a Command Line Interface to your application, then it allows you to invoke functionality in several ways; from scripts, from unit testing tools, or through an automated platform such as a Robotic Process Automation tool.

A key element of a good Command Line Interface is composition. This is a great way to facilitate the Pipes and Filters Architecture. The Pipes and Filters Architecture is a classical way to implement an extensible architecture, as described in POSA. In that approach, as exemplified by Unix pipes and filters at the command line, the output of one Command Line Interface command can be piped into the input of another command, allowing for composition of multiple tools. In any case, each CLI command (composable or not) serves as a point of connection between the backend business logic (the BFF) and the scripting code that calls it. This is shown in below.



Perhaps the simplest implementation of a CLI (perhaps a degenerate case) would be to implement an API on to your server-side application, and then use CURL (or another equivalent tool) to invoke the REST endpoints in place of a more specifically implemented CLI. However, that’s not a great solution to the problem in that the parameters required to run CURL in this way would be quite complex – CURL just moves the parameters of the HTTP request into the command line, so while you make it possible to script or automate different combinations, it does not make it easy to do so.

Going back to our simple hypothetical Meetup example, we can easily imagine that there could be two commands, one equivalent to “Get Event Registration Details” and another one equivalent to “Cancel Event”. They could be tied together with a simple shell script in Linux which could then be scheduled to run every night by using a tool like CRON. However, this is not the only program that could be easily written using a CLI. With a little bit of an extension of the CLI (to include creating events) you could imagine an application to look for open dates and times for events at the venue, and then to schedule events at those times. The possibilities become quite expansive when you think about the compositional options.

However, there are requirements on the CLI to make this kind of scripting feasible. The Pipes and Filters pattern discusses that a common data format needs to be in place among the different commands in the CLI to facilitate piping output of one command into input of another. There also need to be facilities for managing identity of the user of the CLI integrated as part of the command set in order to manage access to the back-end services that the CLI will need to call.

One factor worth considering is thinking about the order of development of the different patterns in this section if you are building user interactions for a complex business process. What we have found is that in many cases, the appropriate order of development should be API, followed by CLI and then GUI. This is because the most important thing to do is to develop the back-end services that make up your business logic (e.g. the API). However, once you have done that, then the question of understanding and facilitating end-to-end testing of that API often becomes the most difficult issue. Writing a web based or mobile user interface and then testing that user interface with a tool like Selenium is possible, but often challenging.

Instead, if you begin by building a simple CLI that embeds the most important commands, you can use that together with simple scripting tools (such as Shell) to try out different combinations of the API’s. This will often expose integration issues that would otherwise be difficult to identify and fix when also facing the added complexity of UI testing. Once the basic flows have been built and tested, then you can move on to building the user interface, but this always allow you to have the scripting alternative for more complex interaction combinations.

Dynamic Hook Points [Acherkan] can be used as a way to override behavior at the common API points. This is especially useful when you are using pipes and filters and want to provide a place to override behavior (hook points) as various points in the process.

An added benefit of a Command Line Interface is that it makes it easier to test things quickly manually while also facilitating automated testing. Commands can also serve as simple event triggers in an Event Driven Architecture (EDA). In fact, you might even consider an Event Driven Architecture to be an alternative to a Command Line Interface, but the problem with this view is that while an EDA solves the problem of hooking into different stages of the business process easily, you are still completely locked into the event system as the only means of expansion or automation – which may or may not be what you want.


################################################################################


  Document Generator 
=====================

You are building an application following the Multimodal Architecture, but you need to communicate with people that (for various reasons) will only accept, or desire documents in a standard form (such as PDF, or Microsoft Office).

How do you integrate printed or digital documents into a user interaction scenario?

Even well into the age of the web, as we cross from Web 2.0 into the first glimmerings of Web 3.0, there are still plenty of opportunities to interact with users through that 500+ year precursor to the Web, the printed page. Documents, whether printed or in simple digital forms like Word or PDF, are still at the heart of many business processes. This is particularly true when the document is a form that must be filled out to comply with a regulatory procedure, or when this is part of a well-known process such as an RFP (Request for Proposal) response.

The problem is that many of the simplest mechanisms for attempting to comply with these processes is often unsatisfactory. For instance, it is possible to print a Web page to PDF, but the result is often difficult to follow, as column widths, diagram sizing and page breaks do not match between the web and the printed page, and thus are usually not well formatted for off-line reading. If a document must follow a complex format, such as an RFP response or a government form, then the simple printing approach will not work. The application developers would spend more time formatting their Web site to conform with the output format than they would developing the site functionality itself. What is needed is a way to separate the document generation function from the other functions of the application. This begins by realizing that documents are their own user interaction approach, and thus requires its own unique Interaction Model.

Therefore,

Build a Document Generator that constructs the document on the fly, using similar approaches to the template construction approaches used by Web Applications.

A Document Generator will often work with a Document Transmission API that sends the document to the user. This could be an Email API, an API to a file sharing system such as Microsoft SharePoint, or even something as simple as SFTP. Likewise, a Document Generator will require a set of libraries and services that perform the Document Building function. An example of this kind of architecture is shown below:



There are many libraries, both commercial and open source, that enable document building. One particularly good example for building documents that follow the Microsoft Office formats (for Word, Excel, etc.) is the venerable Apache POI project. This project (which is built by open-source developers sharing a particularly biting sense of humor) allows you to create documents using the HWPF (Horrible Word Processor Format) or HSSF (Horrible SpreadSheet Format) and even to embed within them drawings created using the DDF (Dreadful Drawing Format). (It should be noted that these are the names for the older .doc and .xls formats – the names for the newer XML based formats are less derogatory). Likewise, it is possible to start with template Word files, Powerpoint and Excel files and fill in missing bits using the POI libraries.

Another example for building documents in the Adobe PDF format is the open source PDFKit library for Node.js. This library allows you to again create complex, multi-part, multipage documents that contain both drawings (rendered in the document) and text, along with formatting. The results can be transmitted, saved or printed directly.

Document Generators are often combined with other digital-native user interaction formats. For instance, a Web Application or Single Page Application may allow the user to save a nicely formatted Word or PDF document that meets unique presentation requirements.


################################################################################


  Interaction Model (aka Application Model) 
============================================

You are building an application (or application suite) in a Multimodal Architecture that needs to interact with the user on multiple technology platforms, such as a Mobile application, a Web Site, or through a Social Media platform.

How do you avoid mixing business and UI logic and duplicating effort and code when you are building a Multimodal Architecture?

Mixed business logic and user interface logic is the bane of many corporate developer’s existence. Unfortunately, it is more common than not. This is one reason why “refactoring the monolith” has become so popular – often little attention was paid to the separation of UI and business logic, resulting in business logic that is spread throughout different layers of a system, and that is inconsistent in its handling of user input and requests, sometimes leading to security issues.

What’s more, in a Multimodal Architecture, there are concepts that span multiple different technology platforms. The concept of a User is something that exists regardless of whether the user is interacting with a system through a web application or through a mobile application. Allowing the user to carry their “identity” through the different channels is important to the user in order to follow the principle of “least astonishment”, but also is vitally important in debugging problems where users may find different results for the same interaction on different platforms. This also leads to the need for consistent logging in order to ensure that these complex multimodal systems can be debugged.

Therefore,

Explicitly create components that model the interaction with the user. An Interaction Model captures a particular form of user interaction on a particular channel or technology stack.

The primary reason for the Interaction Model is to separate the domain logic from the user interface logic. That also separates the channel-specific libraries such as those for mobile applications on iOS, or browser-based applications in React from the rest of the application as well. Thus, the Interaction Model will interface with both these libraries and the back end microservices model (either through a Backend for Frontend, or directly). It may also respond to business events emitted from an Event Backbone if those events are relevant to the user. This interaction is shown below.



Interaction Models are related to specific business processes. The same boundaries that apply to Domain Models (for instance, those found in the Event Storming process) also apply to the Interaction Model, since the Interaction Model captures the interactions with the users that lead to the actions and events in the Event model. Likewise, Interaction Models are also often specific to a particular User Persona as a single business process usually has transition points where the focus shifts between different personas; each individual section between the intersection of business process and persona would be its own Interaction Model.

Interaction Models can be composed (See Micro Frontend and Chatbot for examples of this). In fact, you can even have interaction models of one type embedded into interaction models of another type (e.g. Hybrid Mobile Applications are a way of embedding Single Page Applications inside a Native Mobile Application).

One important consideration is that you should have common code (or at least dependency injection aspects) to deal with tracing and observability that make it possible to trace requests through all layers of the system regardless of the particular UI being used. It’s also important to have a common user representation that goes across all of the different interaction models in the system.

Finally, relating back to the discussion at the beginning of the chapter, Interaction Models can be classified as “Adapters” in a Hexagonal Architecture from Cockburn. However, there are other types of adapters in this model that are not Interaction Models – you don’t want to take the analogy too far. An Interaction Model may be distinguished by the fact that there is a human ultimately on one end of the interaction and not a system or other piece of software. This is not a hard and fast rule, as there will always be ways to simulate human interaction or automate a system that is designed to interact with a human (such as user interface testing software like Selenium). However at least part of the software design process should be devoted to making sure that the interaction can be carried out with a human in mind. Thus, it is intimately and inextricably tied into issues of user experience and UI design. An upfront Design Thinking approach should always be employed in the development of an Interaction Model to ensure these issues are considered.

The Interaction Model is a broadened view of the Application Controller pattern from POEAA. That pattern was also concerned with the separation of UI from business logic, and sequencing or selecting UI operations, but is more tightly tied to page-based web interface technology, which is only one subset of the Interaction Model – see Web Application and its newer descendants, Single Page Application and Micro Frontend. However, the idea of business logic separation and explicit modeling of user interaction expands to many other areas as well – such as Native Mobile Applications, Chatbots, and Social Media Plugins.


################################################################################


  Micro Frontend 
=================

You are developing a new web application or you are refactoring a section of a web application to make it more modern. You realize that the temptation exists to place unrelated functionality for several business processes together in the same Single Page Application, even though that “feels” contrary to the microservices approach. You are already applying a microservices architecture to divide up your domain into separate services.

How do you avoid creating a monolithic Single Page Application by placing too much functionality in a common front-end?

Just as the desire to separate concerns and avoid placing business logic that need not go together into a monolith led to the microservices approach, the same type of concerns apply to front-end UI presentation and logic written in HTML, CSS and JavaScript as well. In particular:

- You want to allow teams to be able to independently develop and release end-user functionality without having to retest or redeploy the entire UI
- You want to allow cross functional teams that own a feature from front-end to back-end.
- You want to allow teams to have the freedom to choose from multiple frameworks (such as Angular or React) when appropriate.

While the Single Page Application approach is fantastic in that it allows teams to entirely replace the UI layout and controls within a particular screen region at any time, when a team building a large application tries to apply that approach to many different areas of the business, the result can often be delays and unexpected conflicts as different aspects of the business may need slightly different UI representations. For instance, the optimal screen layout in an airline web site for choosing a flight may not be the optimal layout for viewing your frequent flyer status. What’s more, you want to allow for different customizations in different parts of your application; The payment flow of your airline website may be different, for instance if the customer is paying with airline points (and started in the loyalty section of the application) than if they are paying by credit card.

Therefore,

Split your Single Page Application into several Micro Frontends that align to specific business features. Allow the team responsible for that business feature to implement the full flow of the front-end for the feature.

A Micro Frontend, is a Single Page Application which consists of HTML, CSS and JavaScript. Each Micro Frontend has its own main container region and is loaded in a single HTML upload. Like both microservices and other Single Page Applications, a Micro Frontend should be an independent, self-contained application that has no dependencies on other Micro Frontends or shared libraries. For example, a change to a JavaScript library or CSS in one Micro Frontend should have little or no impact on any other Micro Frontend.

This level of isolation allows several Micro Frontends to be placed on a single web page that can operate independent of each other, be deployed independently and even use different frameworks, for example two different Micro Frontends on the same page could use Angular and React respectively. In order to make this function, you will often have to use a Micro Frontend framework like single-spa, which is a router that allows the different frameworks to communicate.

However, this can result in of the downsides of the Micro Frontend approach in practice. While teams have the flexibility to choose any MV* framework they choose, and also to specify their own look and feel choices through CSS, this freedom can lead to inconsistency in the UI as you move from view to view. Instead, we have often found it to be better if teams in an enterprise are consistent with the use of a single MV* framework, or at most a very small set of them, and that they also should share some common CSS files to specify a consistent look and feel for the entire web application. Note that it is an anti-pattern to have a separate team define that common set of styles and CSS files – this is actually a better application of a loose “guild” approach to ensure that your UX designers and HTML developers are working together toward common goals.

To allow the team to own features and functionality from Frontend to Backend then the Micro Frontends should be aligned to the backend microservices – especially those microservices that implement the Backends for Frontends pattern. The best way to do this is not to separate these teams, but to keep development of an end-to-end feature together as a single Stream Team.

An example of an architecture (and team organization) that follows this pattern is shown below:



In this example, a team building a customer website for an airline is divided into three Stream Teams, each one responsible for one major epic that represents a major chunk of functionality in the application. Each Stream Team is responsible for both the front-end and back-end development of their epics, although teams may connect at the microservices level, such as the booking team needing to be able to find flight availability if a flight is cancelled and must be rebooked.

By implementing the Micro Frontend pattern you can develop and deploy small features with more agility than in a monolithic Single Page Application This enables cross functional teams, ownership of frameworks, end to end ownership of features and reduces complexity.

You can read more about Micro Frontends in the article that originally introduced them to the world.


################################################################################


  Multimodal Architecture (aka Multichannel Architecture) 
==========================================================

You are building a new application or refactoring an existing application that needs to provide a rich user interface that can be usable on many different devices such as smartphones, tablets, or laptops. However, you need to provide the absolutely best user experience on each device, and need the interface to be best-in-class on each platform.

This is complicated by the fact that you also need to support diverse user communities served by your application, which may mean that different user segments may prefer or default to one particular device type. For example, in a financial services application, younger users may default to using a mobile application, while slightly older users may prefer the larger screens of a desktop application. At the same time, the investment needs and preferences of younger and older users may also differ slightly, leading to small differences in capability or navigation between the application on different devices.

How can an application’s user interface work on different device types, incorporating the strengths of each type, while providing consistent functionality across the types?

When applications for mobile devices and even web applications were new, they were often implemented to duplicate the functionality of existing applications with less standardized user interfaces. This lead at best to duplicate logic requiring separate maintenance, and at worst to applications that provided inconsistent functionality depending on the user interface. (E.g., if the mobile app won’t allow something, maybe the web app will!) Business functionality must be separated and encapsulated, allowing multiple user interfaces to reuse it for consistency. Your application design process is complicated by the following issues:

- Modern devices have many more user-interaction capabilities than previous generations of devices that only supported mice and keyboards. Mobile devices in particular support many more interaction styles than desktop applications do (for example, gestures) and also support input devices not supported by laptops (e.g. cameras and near-field communication).
- Users expect more from their applications and websites today than they did when the web was first introduced. Most users are sophisticated enough to know the capabilities of each platform from using other applications on the platform. When an application runs on a particular platform, they expect it to leverage the capabilities of the platform, like wide screens on laptops, and GPS location on mobile devices.

However, there are also new capabilities that modern platforms give you that were not possible in earlier generations of web applications. Web-based protocols such as REST make communication between applications running on any device and back-end services both straightforward and standardized. Likewise, the growth of computing capability on devices (such as mobile devices and tablets) makes it possible to move as much interaction logic as needed directly into the device without unduly impacting performance.

But the point remains that it is difficult to build a single application in one form factor that serves all of a diverse user community at all points in the interaction lifecycle. Perhaps the best example of this that we have encountered took place at a major airline. In this airline, there are three major vehicles for customer interaction; the airline’s mobile application, the airline’s website, and the check-in kiosks available at the airport. What we saw is that the same users would sometimes use all three points of interaction; they would buy tickets on the website, check in on the mobile application, but print bag tags from the kiosk! Of course, there was significant functional overlap between all three points of interaction; you could, for example, print a boarding pass at home, or at the airport, or display a mobile boarding pass in the mobile application.

What is needed is a way to allow for all these different forms of interaction (and ones we have not even yet considered, such as SMS texting, social media sites, command line interfaces (CLIs), etc.) within a consistent and coherent architecture. The application needs to enable the user to interact with the system the way they want, where they want.

Therefore,

Structure an application’s user interaction layer as a Multimodal Architecture that separates interaction logic from business logic and enables a custom user interface for each device type.

An example of this type of architecture (drawn from the airline example) is shown in below.



In this particular example, we were replacing three existing monolithic systems (one for each channel, e.g. web, kiosk, and mobile) by refactoring the code into two new Single Page Applications that both shared common code and that used a common Backend for Frontend. We likewise refactored the existing mobile application to share the new common Domain Microservices and Adapter Microservices through a new Backend for Frontend.

This refactoring enabled us to keep the different user interaction types but take advantage of the commonality between them at the microservices layer, and thus reduce the overall code size. It also enabled us to replace the existing systems over time as we applied the Strangler Application pattern as sections of the existing systems moved over to the new approach one at a time. The Multimodal Architecture pattern is independent of whether you are using a monolith or microservices approach, but in this case, it facilitated our transition to the microservices approach.

The Multimodal Architecture we adopted thus allowed us to keep the UI logic separate from the Domain logic, but also allowed us to support the user through multiple different user interface channels, all sharing a common domain back-end. So, we could support ticket printing and bag tag printing on those platforms that had printers attached, but likewise take advantage of unique features like location services to provide features like interactive maps on mobile devices that were not available on either the Web site or Kiosk.

The Multimodal Architecture allows you to leverage different technologies to reach different user communities. As we have seen, you can still build traditional Web Applications or intelligent front-ends such as Native Mobile Applications or Single Page Applications that interface with more general-purpose back-end services representing the resources within a business domain, e.g. Domain Microservices. However, you can also use technologies like Chatbots for specialized user interaction types like customer support. Document Generators are useful when you are producing special forms (such as airline tickets!) that must meet legal or regulatory requirements. You usually will want to use Backends for Frontends where needed to adapt between the needs of specific Native Mobile Applications, Single Page Applications and Domain Microservices.


################################################################################


  Native Mobile Application 
============================

You are building applications that need to be easy to use for a wide range of users and situations. Many of your customers are going to use a mobile device as their preferred means of accessing your application.

How do you provide the most optimized user experience on a mobile device and take advantage of the features that make mobile computing unique?

A user interface that is a Single Page Application that runs well in the browser on a desktop or laptop computer might seem sufficient for a mobile device too. After all, mobile devices also have browsers, and this way the SPA can be reused on multiple devices. Yet an SPA that works well dynamically updating in a laptop browser that has a mouse, keyboard, and a large screen may be difficult to use on a mobile device.

There are several aspects of building mobile applications that are fundamentally different from building web applications (either traditional web apps or SPAs):

- Smaller, more varied screens: Mobile device screens are smaller and vary in size more from phones to tablets, and often a large part of the screen is obscured by the virtual keyboard. The amount of content and detail that may fit well on a laptop screen is often too much for a mobile screen. The amount and layout of the content needs to adjust to the size of the screen and whether a widget like a keyboard or an enumerated list of choices is being displayed.
- Human-machine interaction: Browsers can support interaction conventions that mobile screens typically do not, such as popup menus and multiple tabs or windows.
- Input: A touchscreen with a virtual keyboard works differently than a physical keyboard and mouse. Scrolling and selecting may be more difficult and typing long strings of words can be tedious. Touchscreens often provide unique features like haptic feedback that are not part of the web computing experience.
- Sensors and integrated features: Mobile devices have many attached sensors and include capabilities that users become accustomed to using in a variety of ways. These can include cameras for scanning QR codes and taking pictures of checks, GPS for precise locations, integrated calendars, and notification systems.
- Platform evolution: The two major Mobile ecosystems change rapidly. Applications that emulate a mobile device’s look and feel often seem outdated when the native user interface libraries of the Mobile OS changes.
- Unreliable networks: Mobile networks can be slow and spotty. SPA’s use AJAX, which is specifically designed to be very chatty and assumes a fast, reliable network. A mobile client’s connection to the server needs to be less chatty and transfer less data to consume less bandwidth.
- Local caching: A mobile client can cache state locally so that it connects to the server less often and may even be able to run disconnected, including in airplane mode.
- Mobile home screen: A mobile device features a home screen that displays a catalog of programs it can run. The user of a browser-based application must remember to bookmark the webpage of the application. Failure to do so may make the browser-based application more difficult to find next time.

While Single Page Applications provide a user interface that can be adapted to different screen sizes and orientations—albeit often with a lot of scrolling— no browser-based application can take advantage of all the features and capabilities of a mobile device. What’s more, even though advances have been made in the speed and performance of JavaScript in many browsers, the performance of applications in mobile browsers is still noticeably worse than in laptop browsers.

Therefore,

Write a Native Mobile Application, one for each of the two major platforms (iOS and Android), that enables users to interact with your application running directly on their mobile device without a web browser.

A Native Mobile Application written using the tools and capabilities provide by the native development tool suite does not have many of the problems of an SPA running on a mobile device. What’s more, there are several advantages to writing a Native Mobile Application instead of an SPA. A Native Mobile Application allows the developer to take maximum advantage of the platform’s capabilities. Users can easily locate and download the application through the platform’s application store and add it to the device’s home screen for easy access. For most capabilities in a mobile application, this will be the past of least resistance, and will allow the easiest use of new device capabilities as they evolve.

However, a design choice that many teams building Native Mobile Applications often struggle with is how much of their entire application space to implement in the Native Mobile Application. In a Multimodal Architecture, you often have more user functions in the web application than may be available in the Mobile application. In particular, there is the question of how to allow access to seldom-used functions from within a Native Mobile Application. Sometimes, teams have used native functions such as Web views to access the existing web content from within the Mobile application, but that often leaves users confused as the interface of the web application is not consistent with the native mobile application. Instead, what has emerged as a solution to this problem is the use of frameworks such as React Native and Ionic that allow teams to embed application code written as Single Page Applications inside of existing iOS or Android applications. The advantage of this approach is that React Native is compatible with existing web applications written using React, while likewise Ionic works with web applications written in React, Angular and Vue.

An example of this kind of overall architectural approach is shown below:



Just as with Single Page Applications, microservices are a good match to Native Mobile applications since the business-oriented capabilities of a Domain Microservice map cleanly to the complex screen flow and interaction capabilities of a Native Mobile Application. Native Mobile Applications often are paired with Backend for Frontends that can filter and translate results to data format that are specific to the Mobile platform.


################################################################################


  Near Cache 
=============

You are writing a Native Mobile Application. The application must be able to operate efficiently even when Internet connectivity is not available at the highest speeds.

How do you reduce the total number of calls to back-end microservices (particularly to Backend for Frontends) for repeated information?

-  You don’t want to cross the network any more than necessary, especially when network bandwidth is at a premium in a mobile device. 
-  You don’t want to make the user wait any more than is absolutely necessary. 

You don’t want to cross the network any more than necessary, especially when network bandwidth is at a premium in a mobile device.

You don’t want to make the user wait any more than is absolutely necessary.

Therefore,

Use a Near Cache located within the client implementation. Cache the results of calling the Backend for Frontend services so as to reduce unnecessary round trips to the server.

The simplest type of near cache is a globally scoped variable containing a hashtable data structure - something that is easily supported by JavaScript for Single Page Application or Java or Swift for Native Mobile Applications. In iOS, Core Data or Property Lists can easily be used to store cached data locally. However, that’s not always the right solution – you may need more to store more complex data structures. In this case, SQLite can be used on both iOS and Android as a fast, local structured data store.

The benefit of a Near Cache is that it reduces the total number of times you must call a Backend for Frontend to retrieve repeated information. The drawback is that you must now manage the lifetime of the information in the cache to avoid it becoming stale, which can add complexity to your application code.

The position of all potential caches in this architecture is shown below.




################################################################################


  Public API 
=============

You are building an application following the Microservices Architecture. However, although you may be providing your own user interface, in the form of a Web Application, a Native Mobile Application or even a Chatbot, you also need to be able to enable third-party developers to use the services provided by your application.

How do I best enable third party application developers to interact programmatically with my applications?

A problem we have often encountered in Enterprises that need to work with third parties (business partners, governmental entities, etc.) is that they don’t pay enough attention to the communication between what’s inside their own walls and the entities. There are two extremes in which we have seen companies fail in their public API approach:

Neither end will work - both ends put an undue burden on one or the other partner, often both. What is needed is something that moderates the changes and isolates internal changes to some degree from the external API.

Therefore,

Build a separate, externally facing, microservice that implements a Public API. The microservice is a specialized Backend for Frontend that operates as a facade to internal services.

Public APIs are sometimes subdivided yet again into Partner (closed) APIs and Public (open) APIs. The difference between these two is that partner APIs are usually controlled through some contractual agreement and thus are more restricted in how fast they can change than a Public API. In either case, you need to think about both subtypes as being products – they have consumers outside of your development organization, and you have to think about the impact of any changes on that external user community.

That is why the Backend for Frontend approach of creating unique Public APIs is so powerful – it allows you to isolate changes that affect your external user community (the Public API) from changes to your internal services.

Something that is important to understand is that a Public API is NOT the same thing as an API Gateway. The API Gateway pattern from Richardson solves a slightly different problem caused by the creation of Public API’s - that is that they should be discoverable, documented, versioned, and that access to them should be limited to those clients that are properly authenticated and have the right level of authorization. What has happened is that many API Gateway products also provide a form of the Adapter Microservice pattern as part of their implementation - thus people have often built the implementations of their Public API’s directly into the API Gateway. This is limiting in several ways; first of all, it locks you in to that particular API Gateway vendor implementation, second, these products are often limited in how sophisticated the adaptation can be - resulting in Public APIs that often are barely disguised versions of internal microservices implementations. This is dangerous in that it exposes too much of your private implementation to the outside world, and second, it limits your ability to change your underlying implementation. Taking the architectural approach that a Public API should be its own, carefully designed entity on its own avoids this problem.

However, having said this, the API Gateway is an often-useful addition to the Public API pattern in that you can use these products to secure, manage and track your Public API usage. An example of an architecture that combines both can be seen below:




################################################################################


  Single Page Application 
==========================

You are developing a new browser-based web application that has complex navigation, or you are refactoring a section of a web application to make it more modern. You need your application to be responsive and fast, and to feel as much as possible like a native application. At the same time, you do not want to make your application become unusable when internet connectivity is spotty or slow.

How do you design the front end of your application to take advantage of the capability of modern browsers and provide the best mix of client responsiveness and server optimization?

You find the following things to be true when designing your front-end:

- You want to provide the user with a very responsive application that they can interact with easily despite network lags. This means moving as much processing as possible to the front-end browser.
- You want the user to be able to interact with the application naturally, without arbitrary restrictions in user interface design. This means that you want to allow the use of controls that provide immediate user feedback without requiring long wait times while you contact a back-end system.
- You want to enable your designers to provide the cleanest, most attractive user interface possible, and to be able to make design modifications without requiring them to work through your development team. This means the HTML pages should be served up naturally through a Web Server and should be developed and managed using a Content Management and Authoring system. You don’t want extra steps where a development team would pick up and modify a page after it was built by the design team.

The traditional Web Application (using dynamic page template technologies like ASP.NET or JSP) does not meet these conditions. The page-at-a-time back and forth trips to the server make the applications slow, and at the same time usually require changes to both the front-end and back-end code at the same time since the back-end code is often mixed together with the front-end HTML.

Therefore,

Design your application as a Single Page Application (SPA) using HTML5, CSS3 and JavaScript, which are natively implemented by the browser. Store page state within the client and connect to the backend through REST services. This approach is called the “Single Page Application” because all of the HTML, CSS and JavaScript code necessary for a complex set of business functions, which may represent multiple logical screens or pages, is retrieved as a single page request.

The SPA approach is a variation on the traditional MV* patterns used in the dynamic web approach in that Views are not complete web pages. They are simply portions of the DOM that make up the viewable areas of the screen. The initial HTML load is simply a shell that is broken up into child containers (or regions). Developers often use a MV* framework, such as React, Angular, or Vue, to handle the difficult parts of the application management such as routing to the right view, combining data from AJAX calls with template HTML fragments, and managing the lifecycle of each View.

The main benefit of the Single Page Application approach is that it allows for very responsive, fast applications that render quickly. JavaScript code that executes within a web page can control not only the look and feel of the application by generating and manipulating the client-side DOM in any way it chooses, but can request information from the server at any point based on user interactions – resulting in more responsive user interfaces. The overall flow of information in the SPA approach is shown below.



Another advantage of the approach shown above is that the AJAX calls that are issued by the SPA to the server have generally smaller payloads, and thus are faster to execute, than corresponding calls that would transmit a completely rendered HTML page.

Single Page Applications often are written to take advantage of Responsive Design principles (see Brown) to optimize user experience for screen layout and size. CSS Media queries are often used to include specific blocks that only apply for specific screen types. This technique allows a different set of CSS rules to be specified for tablets, mobile phones or laptops, resulting in screens that are laid out and configured specifically for those devices. In addition, the Single Page Application approach effectively decouples UI logic (particularly rendering and entry validation logic) from server logic – this makes it possible in many cases to make UI changes without having to update a corresponding server component.

In a complex business application, you may implement several Single Page Application instances. Each one represents a single logical set of screen interactions that perform a business function. This approach maps extremely well into the microservices approach, as you can match an SPA to the capabilities of one or more Domain Microservices. However, you may need to perform some translation or conversion of the results of a Domain Microservice in order to match the unique user interface requirements of your SPA. That will naturally lead to the need for the Backends for Frontends pattern.

Single Page Architectures can sometimes, unfortunately, grow to the same size and complexity of the monoliths they are expected to replace. To avoid that, build complex applications as a set of Micro Frontends that are composable.


################################################################################


  Social Media Plugin 
======================

Users have interactions with an organization outside of the typical interactions through media like Native Mobile Applications or Web Applications. Social Media outlets such as Twitter and Facebook, or internal social platforms such as Slack are important forms of interaction that should be part of an overall communication strategy with internal and external users.

How do you attract and interact with users outside of custom applications?

Users aren’t always where you want them to be. They won’t always go to your website or download your application. However, social media is pervasive. Your users are already there. What’s more, social media tends to be where users ask questions that are perhaps relevant to your company.

The explosion of social media is related to the fact that most of people now use a mobile device as their primary means of accessing application functionality. This encourages responding quickly to notifications, of which social media apps generate a plethora. What is more, chat platforms of various types tend to be the predominant application type most in use on these devices . Unfortunately, which platform predominates depends on location, culture, and a variety of other reasons that lead to a proliferation of platforms if you want to have reach across geographies, ages and other demographic factors. The platform that could help you reach your preferred target market could range from WeChat in the Chinese market, to Facebook in the U.S. market (at least for the over-30 age range – for younger users it could be Discord). For the enterprise market, you may be most concerned with reaching your users on platforms like Microsoft Teams or Slack.

What often happens is that people look for help when they are stuck with application issues on social media sites; this could be something like Slack, Facebook or Twitter, or perhaps a more specialized site like LinkedIn. The key here is that humans trust other humans to give them advice - often more so than they would trust the very same information if it were found on a website.

Therefore,

To reach users in the platforms where they spend the most time, build Social Media Plugins that use the API of the social media application to parse and create posts, DM’s and other artifacts of social media.

A Social Media Plugin is often (in fact, usually) paired with a ChatBot that can parse natural language text and formulate responses in natural language. This is especially true of situations where we want to automate or semi-automate the process of responding to messages (such as Tweets) raising complaints or questions about a product or service. This architecture is shown below.



One of the key aspects of a successful Social Media Plugin is that the representation of the User should be carried over from the Social Media Plugin into other Interaction Models that the user interacts with. Thus, if a user asks for help on social media with a task, the results of actions taken through social media should be available on the platform on which they may have originally tried the task, be that a Native Mobile Application, a Web Application, or a Single Page Application. This is yet another reason to tie the different interaction models together not only through common back-end domain representations like a common user model, but also through a common Event Backbone that can signal these changes to interested parties.

A Social Media Plugin could be implemented with Dynamic Hook Points [Acherkan] to override specific behavior depending upon the social media context. This can even be user or client specific.


################################################################################


  Web Application 
==================

You are building an application that needs to reach the largest possible user community. You know you want to follow a Multimodal Architecture, but at the same time you need to get something out quickly to your users.

How do you build a user interface to provide basic functionality to the largest possible set of users with the minimum amount of effort?

Some applications are meant for greatness. Most, however, are not. One of the downsides of taking all of your architectural and business lessons from unicorns and hyperscalers is that it is easy to believe that all applications will become virally successful, require unheard of scalability, and attract millions of adoring fans that hang on every font and icon change with baited breath. The truth is, that while there are applications that fit that category (for instance, Twitter created a mini-storm by merely changing its font in August 2021 - see Peters), the reality is that much more than 90% of all applications written will be written for an unchanging audience, whose size is well-known ahead of time, and whose usage patterns are both consistent and sporadic. For instance, how many times have you had to update your home address at your Auto Insurance company? The total number of times that particular part of their website will be used in a year can be calculated very precisely from the size of their covered population and a coefficient derived some population movement statistics obtained from the Census Bureau.

Even setting aside the fact that most Enterprise applications are much more like our hypothetical address-change app than Twitter or Facebook, there are other fundamental issues that make it often less advisable to try to keep up with the cutting edge of user interface technology. Probably the most important, and least considered is developer skill – if you adopt a new front-end JavaScript framework then you are implicitly saying that it is worthwhile to spend the time for your developers to learn this framework – something that will make you even more likely to want to use it to its fullest extent once that learning time is committed. What this leads to is an example of the sunk cost fallacy – if your developers spent time learning something, then you want to get the benefit out of that time, which will often result in applying technologies to problems for which they are, at best, overkill.

On the other hand, what if there were a set of technologies that were already well understood, widely available in commercial and open-source implementations, completely compatible with cloud-native principles, and what’s more, could give you the fastest path to value for many types of user interfaces? That would seem to be a godsend to teams who struggle to learn all of the other parts of Cloud adoption. And what if those same technologies allowed you to reach the widest possible user community at the same time? In fact, that technology exists.

Therefore,

Build a Web Application that serves up HTML pages from a server-side application and that takes input through HTML forms. This works best for relatively small and simple applications, particularly those that do not absolutely require intense graphical interaction or that are used sporadically or rarely.

A canonical example of this approach is PHP (a recursive acronym for PHP the Hypertext Processor). PHP runs inside virtually any web server software and will work with most databases. PHP programs are written inside of HTML pages where PHP statements are interpreted inside special tags that are differentiated from HTML tags, usually with the syntax <?php…?>. Any PHP code inside the tag is dynamically interpreted and evaluated to render HTML that is then inserted into the appropriate place in the HTML page as it is returned to the page requestor by the web server. This type of architecture is shown in below:



One change that developers soon realized is that making the HTML template the target for requests made it very difficult for a choice to be made in your code on what page to render when an HTTP request is received. That led to a separation of the request processor from the template engine. This approach was commonly used in ASP.NET and Java Enterprise Edition, where the request processor such as a. Java Servlet was separated from the template, which was a Java Server Page or JSP. This is shown here:



Many of you reading this you may already be saying: “That’s what our old monolith was! That’s what we’re trying to move AWAY from!” You will note that we are merely describing the approach that this type of application takes in constructing user interfaces but did not say anything about how large or small each application would be. The rejection of Web Applications as a technology is an unfortunate application of the principle of tarring two very different things with the same brush.

We mentioned in the introduction to this chapter that one of the key principles in building user interfaces should be the separation of domain logic from user interface logic. One of the problems with the two architectures we have just shown is that there is no good way to enforce that separation. In fact, the PHP architecture almost requires that the two be mixed together, and to avoid doing so in the Servlet architecture above takes great discipline in building application Façade objects that would act as adapters to the core of the application; something many teams never took up, instead coding business logic directly from the servlets or other request processors or (even worse) the JSP or ASPs themselves.

Instead, a better way to do this is to formally separate the two by introducing a separate Domain Microservice layer. This version of the Web Application architecture is shown in below:



In this way, the three parts of a “Model View Controller” or “Model View Presenter” pattern are introduced . The Request Processor handles the issues of control flow, the Domain Microservice handles domain logic, while the Template Engine handles building the HTML View. Building applications in this way can be a very fast path to a minimum viable product; as fast or even faster than building a Single Page Application depending upon the complexity of the screen flow. What’s more, building this kind of application is something that many developers already have skills in; Java programmers often learn Servlets and JSP’s first before learning frameworks for building REST services, and PHP skills are common in the industry as well. The tools, frameworks, and runtimes for building this kind of application are very mature and stable, available as open source, and run both on premises and in every available container technology and cloud provider – meaning that developers will not be running on the bleeding edge of a new technology or locked into a particular vendor.

A key principle to follow in sizing a Web Application is that each individual application should perform only one business function. If this sounds like the same principle that applies to microservices, that’s because it is. Not only will that make the business logic more cohesive, but it will keep the user interface logic simpler and more readable as well. If you construct your application with template technologies like JSP, PHP or ASP and, importantly, keep the user interface small (fewer than ~20 pages per application) and directly reflective of the business process flow of the domain, then the issues that led to the problems we saw with large, monolithic Web Applications can be avoided.

Whenever a business process (and screen flow) crosses from one Domain Entity or Aggregate to another, then you should also cross over to another application to handle that flow. Thus, your web application boundaries should roughly correspond to the boundaries of a microservice. Going beyond that can lead to grouping together unrelated logic and leads to building a monolith. There also may be a temptation to build multiple Web Applications that all communicate through a shared database, which again should be avoided.

If you find yourself moving in that direction, or if you have a user interface that is complex and not easy to express as simple forms and results pages, then you probably need a more capable client-side application based in JavaScript with backend logic provided by Domain Microservices. In that case, a Single Page Application is the best option. Of course, the ultimate in user interface customization is possible if you build a Native Mobile Application, so you also need to consider your options in that regard.

This pattern is is related to Template View pattern from POEAA and in fact may be considered a combination of Template View and Application Controller. However, building a traditional web application is only one user interface choice of many that developers now face, so many of the reasons for separating the two are no longer necessarily applicable given this plethora of choices. The Web Application pattern is the one of many such choices in POEAA that “won out” in the end in the market, even to find itself replaced by other technology options.


################################################################################


  Microservices Design Introduction 
====================================

Cloud-native applications are those that are specifically written to run on the cloud, usually within a PaaS. They take advantage of the benefits of the cloud directly - for instance, using cloud functions like elastic scaling to meet capacity needs. What’s more, these applications are built using different tools and runtimes than traditional applications. They tend to be more likely to employ Polyglot Development and Polyglot Persistence. For example, an application might not use a relational database but instead use a NoSQL database, such as Cloudant or MongoDB. Likewise, a Cloud-native application has a different set of assumptions about what is provided by its infrastructure and how.

Over the last several years that we have been building Cloud-Native applications we have found that there is a strong affinity between the 12 factors and the Microservices Architecture. Microservices applications are automatically 12-factor compliant and thus more able to run in a Cloud-Native way. We have found that if you design for microservices and cloud native at the same time then the benefits to the team are multiplied. Some of these benefits are:

- Faster development
- Smaller “Blast radius” for decisions
- Better ability to pick the right tool for the right job

In this section of our pattern language, we introduce some of the most basic patterns for Cloud-Native development. These include:

- Microservice Design is the root pattern of this section. It leads you into a process of discovering your microservices through Domain-Driven and Event-Driven Design.  Bounded Context is a key concept for developing a good microservice Performing Event Storming is the crucial step that helps you set the stage for the other pieces in your microservice design. Identifying Entities and Aggregates is required to find the basic concepts to be implemented as microservices in a domain Identifying Repositories and Services help you identify processing elements in your microservices  
- Bounded Context is a key concept for developing a good microservice
- Performing Event Storming is the crucial step that helps you set the stage for the other pieces in your microservice design.
- Identifying Entities and Aggregates is required to find the basic concepts to be implemented as microservices in a domain
- Identifying Repositories and Services help you identify processing elements in your microservices
- Container Orchestrator solves the problem of deploying numerous containers across multiple servers and managing them while they run.
- Service Registry solves the problem of discovering services when your number of services increases and the complexity of dealing with local and remote services becomes difficult.
- Event Driven Architecture is an important design consideration in building highly performant microservices architectures.
- Service Mesh is an important implementation approach to take care of many of the nonfunctional aspects of building with microservices.
- Results Cache is a fundamental technique used to improve the performance of data access in a microservices design especially when microservices are being called repeatedly.

- Bounded Context is a key concept for developing a good microservice
- Performing Event Storming is the crucial step that helps you set the stage for the other pieces in your microservice design.
- Identifying Entities and Aggregates is required to find the basic concepts to be implemented as microservices in a domain
- Identifying Repositories and Services help you identify processing elements in your microservices

The connections between most of these patterns is shown below:




Table of contents
=================

-  Api or Objects - which first? 
-  Container Orchestrator 
-  Event Storming 
-  Identify Entities and Aggregates 
-  Identify Repositories 
-  Microservice Design 
-  Results Cache 
-  Service Mesh 
-  Services 
-  What's the right size for a Microservice? 


################################################################################


  Which comes first: the API or the objects? 
=============================================

An ongoing argument exists about which comes first: the design of your API or the design of the Objects that implement your API. Many early distributed-computing proponents advocated designing the Objects first and then making your API the same as your Object API. This approach led to problems in the granularity of the API, which often resulted in APIs that represented technical interfaces instead of business interfaces.

When you start with the API, you can focus on solving the business problem and avoid getting lost in the technical details of a particular implementation. Pact testing is a slightly different version of API-first. Pact is a contract unit-testing tool that ensures that services can communicate with each other.

When you write PACT tests, you unit-test the consumer and the provider separately. To test the consumer of an API, you submit an actual API request to a mock provider and receive a response. To test the provider, a mock consumer issues a request and the provider provides an actual response. Verification is done in the mocked code to ensure that the services work as expected. Testing is done only on the specific functions that the customer uses.

The next consideration when you design an API is whether your APIs represent Entities or Functions. The starting point for mapping is the Resource API pattern (Daigneau 2011). This pattern is a stripped-down definition of the central idea of REST. You assign all procedures and instances of domain data a URI and then use the application protocol of HTTP to define the standard service behaviors by using standard status codes wherever possible. In this model, a request consists of a URI, an HTTP server method, and optionally, and rarely, a Media type. That combination uniquely selects the particular Service that fulfills the request.

The key notion is that each URI represents a Resource, not a procedure. Resource APIs must be the bulk of the APIs that you specify. They’re the rule rather than the exception.

However, exceptions exist. The best way to handle non-entity Services is to think through the problem of “noun-ifying” them with the Process API pattern. A Process API is a reification of an action in a domain. If you can name a Service as a noun in Domain-Driven Design, that Service name becomes the URI path. A good rule is that if a paper-and-pencil version of the thing exists, it can become a resource, but if it’s something a person must do, it becomes a process. An example of a process API might be processing a purchase order.

When you start with Domain-Driven Design, you find that the large-grained concepts that are derived through the process are closer to the right level for a good API design:

- Aggregates and Entities become Resource APIs
- Value Objects inform the design of the schema that the Resource API uses
- Services become Process APIs


################################################################################


  Container Orchestrator 
=========================

You are building a new application with a cloud native architecture that runs in multiple containers. Other applications have even more containers. You need a way to manage these containers and keep them running even while circumstances in the cloud keep changing.

How can development teams manage the deployment, placement, and lifecycle of workload containers across multiple server computers?

Deploying multiple containers seems simple: A container engine can run multiple containers, so run them all in there. But a container engine won’t restart a failed container, doesn’t replicate the container for high availability, doesn’t help upgrade the container to a new version, and doesn’t help client requests find the container. And a container engine only runs on a single server computer, which has limited capacity and is a single point of failure.

What you need is a container manager that coordinates container engines on multiuple server computers, distributes containers across those engines, manages the containers, and makes them available for clients to use. You need a manager that adjusts to changes in the cloud–such as a server failing–to keep the containers available.

Therefore,

Install a container orchestrator to create an environment for hosting large numbers of containers across multiple server computers.

An installation of a container orchestrator is called a cluster. A cluster federates a dynamic set of hetrogeneous server computers and performs container orchestration. It has this general structure:



This cluster architecture consists of these components:

- Manager: The brains of the cluster, it manages the nodes and the containers running in the nodes. It runs six services that do much of the work:  Management API: Enables clients to direct the manager, such as to deploy an application Cluster management: Adding and removing nodes, maintaining their configuration, and monitoring nodes Replication: Decides when new containers need to be started and when ones not needed anymore should be stopped Scheduling: Decides which node to start a new container on, striving to distribute replicas across the nodes and load balance the nodes Health management: Detects and replaces unhealthy containers Service discovery: Centralized lookup for services and the network addresses for the containers that provide the service  
- Management API: Enables clients to direct the manager, such as to deploy an application
- Cluster management: Adding and removing nodes, maintaining their configuration, and monitoring nodes
- Replication: Decides when new containers need to be started and when ones not needed anymore should be stopped
- Scheduling: Decides which node to start a new container on, striving to distribute replicas across the nodes and load balance the nodes
- Health management: Detects and replaces unhealthy containers
- Service discovery: Centralized lookup for services and the network addresses for the containers that provide the service
- Image registry: The database of container images that can be started on the nodes
- Discovery DB: Records the state of the cluster, such as which contianers are running on which nodes
- Nodes: The server computers that the containers run on. Each runs three types of processes:  Daemon: Agent that manages the containers in the node as directed by the manager Containers: The processes running the application workloads Container engine: The OS running the containers  
- Daemon: Agent that manages the containers in the node as directed by the manager
- Containers: The processes running the application workloads
- Container engine: The OS running the containers

- Management API: Enables clients to direct the manager, such as to deploy an application
- Cluster management: Adding and removing nodes, maintaining their configuration, and monitoring nodes
- Replication: Decides when new containers need to be started and when ones not needed anymore should be stopped
- Scheduling: Decides which node to start a new container on, striving to distribute replicas across the nodes and load balance the nodes
- Health management: Detects and replaces unhealthy containers
- Service discovery: Centralized lookup for services and the network addresses for the containers that provide the service

- Daemon: Agent that manages the containers in the node as directed by the manager
- Containers: The processes running the application workloads
- Container engine: The OS running the containers

The architecture forms two distinct layers, essentially the cluster’s head and body:

- Control plane: Centralized management of the cluster
- Data plane: Capacity for running workloads, designed for horizontal scaling and high availability of the cluster itself

With this architecture, container orchestration handles responsibilities like deploying an application and enabling clients to access applications on the server.

The container orchestrator distributes responibilities for deploying an application amongst its services:

- It tells the node’s daemon to start the container

- When a container needs to be restarted, it tells scheduling

The container orchestrator likewise distributes responibilities for handling client requests amongst its services:

Interestingly, the container orchestrator architecture that originated with Cloud Foundry in 2011 and Kubernetes perfected in 2015 is quite similar to that of a WebSphere Application Server cell from 2002, where application servers run Java workloads instead of containers:

- Deployment manager: Centralized management of the cell
- Nodes: Server computers that scale horizontally  Node agent: Manages the node Applications: Java (and eventually Node.js) workloads Application server: The Java JVM/JRE with J2EE extensions  
- Node agent: Manages the node
- Applications: Java (and eventually Node.js) workloads
- Application server: The Java JVM/JRE with J2EE extensions

- Node agent: Manages the node
- Applications: Java (and eventually Node.js) workloads
- Application server: The Java JVM/JRE with J2EE extensions

Examples of container orchestrators include:

- Kubernetes (2015): The most widely used container orchestrator today. The project is managed by the Cloud Native Computing Foundation (CNCF).
- Cloud Foundry (2011): Predecessor to Kubernetes, managed by Pivotal Software as part of the Cloud Foundry Foundation.
- Swarm (2015): A mode for managing multiple Docker Engines, managed by Docker, Inc.
- Mesos (2016) and Marathon (2016): Server federation and container orchestration, managed by Apache and Mesosphere respectively.

Each container being orchestrated is often running a microservice as a container per service. Containers support polyglot development because each container runs its own language runtime, which can be any that runs in Linux. The service in the manager that performs service discovery is acting as a service registry.

A service mesh such as Istio has a similar control plane/data plane architecture with multiple management services centralized in the control plane. Indeed, a service mesh such as Istio requires a container orchestrator because its capabilities depend greatly on the orchestrator’s capabilities. A container orchestrator, particularly its service registry, greately simplifies implementing many Cloud Native DevOps techniques including red/black deploy, canary testing, feature toggle, aggregating logs, and autoscaling.

The CI/CD pipeline and its container build pipeline itself can run in the container orchestrator that it also deploys to.


################################################################################


  Event Storming 
=================

You are starting on a Cloud-Native design and have already begun the process of Domain Driven Design. You have identified some candidate Entities, Aggregates, and Potential Services.

How do you figure out how all of these disparate pieces of domain driven design tie together in a dynamic system?

One of the complaints that many people have had about Domain-Driven Design is that it seems to be concerned only with the static functionality of the system. The patterns in Evans are good for understanding the vocabulary (the Ubiquitous Language) of a business domain, but many practitioners have struggled to determine how to translate the pieces of the Ubiquitous Language expressed as Entities, Aggregates and Services into a complete and functioning system.

What is missing is a view of how the data changes through its lifecycle. You need a dynamic view that allows you to understand how data is created, how it flows through the system, and how the system should react in response to changes introduced from the outside world. Events provide just that kind of viewpoint that is needed to augment the static viewpoint of Entities, Aggregates and Value Objects.

Therefore,

Apply Event Storming as a process to elucidate the set of Events that flow through a system and are captured and managed by the Entities, Aggregates and Services of a system

Event Storming is a brainstorming or design thinking technique developed by Alberto Brandolini that begins with a team writing down all of the “facts” about their system that they can think of on sticky (post-it) notes. A fact should be expressed in past tense such as “deposit has been credited”. The team then arranges all of the facts they discovered in linear (time-sequence) order horizontally on a wall. These facts are now Events in that they show how one occurence will be followed in time by another and another and another. Where simultaneous events occur, they can be placed in different horizontal swim lanes separated vertically.

After the team agrees on the sequence(s) of events in time, they can then “decorate” the events by adding data that each event either requires or generates, “commands” that create an event, actors that cause commands to be issued, and policies that automatically turn one event into another. The result of this is that a dynamic system design begins to emerge from the sequence of time-based events.

Finally, the events can be grouped together by data elements and related commands within a scenario to represent a complete bounded context. In this way, the Event Storming approach has thus helped you to validate your Entities and Aggregates (they should be the same as you identified in earlier stages of Domain Driven Design) and you will be able to map specific commands to Services Objects. Each Bounded Context that you identify then may become a candidate microservice.

Once you have applied Event Storming to Identify Domain Events you will find that you are well on your way to beginning to implement an Event-Driven-Architecture


################################################################################


  Identify Entities and Aggregates 
===================================

You are building an application following a Microservices Architecture. You need to understand how to discover your microservices from a complex business domain.

What concepts in a business domain best correspond to microservices?

Designing for a monolith is a relatively easy process. When you are not overly concerned about how difficult testing a system is, you can create objects or data structures corresponding to business concepts within the monolith as much as you please. The problem comes in when you need to start understanding how different viewpoints of the same concept apply. You often end up with either multiple classes with slightly different names and responsibilities, or (even worse) huge classes with very different responsibilities and vast amounts of shared data.

But when you begin with the problem of reducing testing and limiting the blast radius of decisions as part of a microservices approach, you find you have to take a more measured approach to dividing up the concepts in a domain. You have to think carefully about the ownership and lifetime of each piece of data, and under what context different viewpoints apply.

Therefore,

Start your microservices design by identifying the Entities and Aggregates in your domain.

An Entity is an object that is distinguished by its identity. Entities have unique identifiers. Entities also have attributes that might change, but the identifier for the entity stays the same. An example of an entity is a person. A person has an unchanging identifier, such as a Social Security Number in the United States. That same person has a given name, a surname, an address, and a phone number. Any of the attributes can change, but the person doesn’t change.

Evans notes that entity objects must have a well-defined lifecycle and a good definition of what the identity relationship is—what it means to be the same thing as another thing.

From Entity-Relationship modeling, you know that sometimes Entities might be well-defined and have a specific well-known identifier, but might not live independently. Evans calls the combination of Entities an Aggregate.

You can find a simple example of the Entity/Aggregate relationship in a retail store. If you go to the soda aisle in a grocery store, you can buy a 12-pack carton of soda. Each can in the carton has a bar code to identify it individually, but you can’t buy a single can from the carton. The cans are entities that are referred to as Dependent Entities. The root entity is the carton. The carton is the Aggregate because it defines the Dependent Entities’ lifecycle (at least as far as the retail store is concerned)

The opposite of an Entity is a Value Object. Value Objects have no conceptual identity. You can’t tell one Value Object from any other of its type. If you treat all objects in a system as Entities, the process to assess and manage the identity of each object can become overwhelming and hurt performance. Instead, care only about the attributes of a Value Object and that each Value Object can be treated as unchanged from the creation of the object until it is destroyed.

Consider an example of a Value Object. When you go to the bank, you usually want to access your bank account. Your bank account is an entity. If you query your current account balance, the result is a Value Object. The next time that you query your account balance, a different Value Object is returned with your new current balance.

Usually, when you identify the Entities and Aggregates, most of the Value Objects “fall out” of the process. You don’t have to go explicitly looking for them, they simply start to accumulate - you just keep a running list of them as the process carries on.

Another thing to keep in mind is that Aggregates usually either succeed or fail as a whole. They often form the boundaries of transactionality. That is why Aggregates in particular are good first candidates for microservices.

But be aware that Entities and Aggregates do not always equal Microservices on a one-to-one basis. You also have to consider the dynamic flow of information through the system. That is why Identify Domain Events is a crucial step in the design process.


################################################################################


################################################################################


  Tolerant Reader 
==================

You are building an application within a Microservices Architecture and you must deal with the reality of evolving microservice APIs

How do I make sure that a new release of a microservice API does not trigger a cascade of upgrades in the service’s consumers?

If you place all of the responsibility of API change on the service itself, then you may find yourself in the situation of having to support multiple versions of a microservice at the same time. Likewise, if you try to notify each microservice client application of changes in an API, then you may have to go with a relatively heavy-weight Service Registry approach that requires that all clients look up your microservice through a service registry that only returns exact matches for version number requests.

Therefore,

Encourage your microservice consumers to be Tolerant Readers: they just pick out the information from the JSON they are interested in; as most API changes are additions of data, such consumers can happily ignore such changes.

Tolerant readers can benefit from version information in your JSON, whether they receive it through an API or through Topic Messaging. An easy solution is to have two monotonically increasing version numbers:

- A featureVersion that is increased any time that things are added in a backwards-compatible way.
- A compatibilityVersion that is increased any time that things are changed or removed, breaking backwards compatibility.

```
featureVersion
```

```
compatibilityVersion
```

An application can then check whether the feature version is at least what it expects and the compatibility version is smaller than it expects. If that is the case, the data that is expected should be there and in the expected format.

The Tolerant Reader pattern was introduced in Daigneau


################################################################################


  Topology Aware System 
========================

When you are using an Overlay Network and all of the components of your system are in One Region, you can be tempted to view the group of datacenters as one system. Very often, this will just work, until it doesn’t. This is due to the fact that Peter Deutsch’ Fallacies of Distributed Computing are more visible on WAN connections than inside local area networks. Ignoring the topology of the network simplifies systems, but causes needless traffic over WAN connections, risking running into said fallacies sooner.

Therefore,

Make your systems topology aware, so that as much traffic as possible is kept within a datacenter. Having your Service Registry be topology aware so that local instances of services are returned first on a lookup is a good start.

One of Deutsch’ Fallacies is “latency is zero”. When these were written, the most prevalent network probably was Ethernet in either 10base2 or 10baseT physical configurations. Even though 10baseT looked like a star topology, the actual repeaters in use were hubs, basically replicating the coaxial bus of 10base2 inside them; CSMA/CD was used in both, meaning that especially on loaded networks, latencies were widely variable. As latency drives a large number of performance metrics, from partition sensitivity to the number of requests per second that can be made over a logical connection, reducing latency as much as possible is of paramount importance.

“Latency is zero” has become less and less important over time, as traffic inside a datacenter is now switched, fiber connections between racks of machines essentially operate at light speed, and intermediate routers only add miniscule delays. Round-trip times are measured in tens to hundreds of micro-seconds and one can get very far by essentially ignoring the network.

However, when adding multiple datacenters, one is back in the ’90s again. Latencies are widely variable, varying between the tens to hundreds of milli-seconds, and ignoring the network when crossing the WAN will quickly lead to problems. Often, testing is done during “nice Internet weather”, were everything will look nice and latencies are basically 50% of lightspeed, but there is pretty much a guarantee that no matter how traffic is routed between datacenters, there will be bad days. Or weeks. Cable repairs in remote areas are hard, damaging them is not.

In order to be successful in a multi-datacenter setup, it’s therefore very important to keep traffic flows inside the datacenter as much as possible; in fact, WAN traffic should be the exception and every time this happens, there should be a clear and good reason for it. Some examples of “good” WAN traffic are cluster state information and data replication; some examples of “bad” WAN traffic are webservers talking to remote databases or microservices talking to other microservice instances in remote datacenters.

Systems need to be made aware of where services reside, and need to have a strong preference for local instances; in fact, when critical systems (like databases) are not available in a datacenter, it is probably preferable to have client systems fail in that datacenter as well rather than going over the WAN, stressing remote systems needlessly.


################################################################################


  Strangler Patterns Introduction 
==================================

Over time even a great design can be compromised by successive architectural revisions, especially as technical debt grows. In 1998 the claim was made that the architecture that actually predominates in practice is the Big Ball of Mud. Big Ball of Mud (BBoM) architectures are still seen today. They are the culmination of many design decisions that gradually result in a system that can be difficult to change and maintain. However, BBoMs usually do not result from well-intentioned design ideas gone wrong. Nor are they simply an accretion of expedient implementation hacks. Rather, they can be a mix of doing what it takes to meet business requirements along with obliviousness to technical debt growth and no time given to address these needs.

Since 2014 the “Microservices” architectural style has been increasingly adopted by many organizations to better address business needs. Microservices encapsulate different parts of the application as independently deployable units that contain their own application logic, data, structure, and more. After the new term “microservices” appeared, previous systems or architectures developed were labeled as “monoliths.” Unfortunately the term monolith gained a bad connotation inasmuch these systems are viewed as legacy systems or BBoMs. Developing a system using the monolith architecture style is not necessarily a bad design decision as outlined by Richardson.

Many companies are successfully adopting the microservices architectural style and reaping benefits such as shorter development times and increased flexibility for experimenting with new ideas and technologies. However, most organizations have existing systems that were developed before microservices and still provide value. As organizations evolve, a monolithic system can become harder to maintain and hinder the ability to keep up with new business needs. The poor flexibility of monoliths has driven many organizations to apply the microservices architecture style which leads to the question: ‘What to do with the existing monolith?’

Martin Fowler coined the term “Strangler Application” as a metaphor in 2004 to describe a way of rewriting an important system. The “Strangler Application” is based on an analogy to a vine that strangles a tree that it’s wrapped around.1 The main idea is to gradually evolve the system by replacing / rewriting existing components or by adding functionality as new components, all or mostly outside the old system until the old system has been “strangled,” that is replaced. Fowler renamed this to “Strangler Fig Application.” The strangler application idea is independent of services because you can evolve the system around the old without adding or evolving services.

Typically, a monolithic application is packaged as a single deployment file that runs on an application server. The monolith consists of many components that may contain business logic from various subdomains. These monolith components can include services, modules, libraries, or any type of implementation. They also have dependencies among themselves that typically increase over the years. Monolith components that are visible on the network may use protocols, message formats, and API design standards that are not fully compatible with network calls being used in new client applications. For example, the monolith may provide EJB services, and new applications in Python are not able to directly call these services. The general scenario is illustrated in Figure 1. The generic connector symbol (→) in the diagram may represent a HTTP request to REST service, a platform-specific invocation mechanism, or an in-process call to modules in the same deployment unit. In a real project scenario, different types of connectors may be used in the design. Note that in this diagram the components appear to be mostly decoupled but in reality, there is usually a lot of coupling in the monolith architecture. The red “X” illustrates that new code is written to avoid directly accessing the monolith.

   

One of the first decisions to make is whether to completely rewrite the monolith or apply Strangler. Sometimes rewriting the monolith is the right approach. Sometimes the monolith needs to be reconceptualized and implemented from scratch (possibly using microservices). However, it is usually the case that the cost and duration of a complete rewrite make it infeasible. Once you have decided to do an evolutionary application of the Strangler, there are many possible variations.

- Strangler Application If the monolith has become hard to maintain, is hindering new projects, and rewriting it is not a viable path, then it is time to apply the Strangler Application and gradually migrate the monolith. Once the decision has been made to apply Strangler, it is usually a good idea to decide whether you need to protect the system from change by Wrapping the Monolith
- Pave the Road If the team is facing their first microservice project, you need to make sure you have the infrastructure and the environment (both technical and organizational) to make it easier to implement microservices (Pave the Road).
- Start Small Starting Small—by having a team implement some new functionality—is a good way to learn about microservices principles.
- Microservices First Strategy Once one or a few microservices have been successfully created, the team might redirect any new development efforts to add new features or functionality by implementing it with microservices first if possible.
- Macro then Micro Sometimes you are not sure how big the microservice should be so you might start with a bigger service then refactor it to smaller services as you learn the domain and more about microservice design.
- Replace as Microservice Freeze the functionality in the monolith and completely re-implementing the functionality with microservices.
- Extract Component and Add Façade Refactor and move components out of the monolith, implementing them as microservices. Use a Façade to delegate to the new contract or use a different type of connector and message format.
- Proxy Monolith Components to Microservices A design solution is needed to address the discrepancy between old client components, inside or outside the monolith, and the new microservice. This pattern provides a proxy from any existing monolith components to either extracted or new created microservices needed by the monolith.

Below we show a pattern map of various “strangling” approaches and the relationships between the patterns.



Note these patterns were first introduced and evolved from a PLoP 2020 paper by Joseph Yoder and Paulo Merson.

1 *One of the natural wonders of  are the huge strangler figs. They seed in the upper branches of a fig tree and gradually work their way down the tree until they root in the soil. Over many years they grow into fantastic and beautiful shapes, meanwhile strangling and killing the tree that was their host.”          https://martinfowler.com/bliki/StranglerFigApplication.html


Table of contents
=================

-  Extract and Add Façade 
-  Macro then Micro 
-  Microservices First 
-  Pave the Road 
-  Proxy Monolith Components 
-  References 
-  Replace as Microservice 
-  Start Small 
-  Strangler Application 


################################################################################


  Extract Component and Add Façade 
===================================

      (aka Extract Service and Add Façade)

You have Started Small, Paved the Road, and began using microservices as part of your implementation. You have a monolith that has been providing value to your organization, but has become hard to change. There is some functionality you want available as a microservice New client applications need to use functionality in the monolith but require a different contract.

How can we make monolith functionality available as microservices without affecting existing client applications that access the monolith?

Components from a monolith are being transformed into microservices as part of a long-term strangling process. These microservices use new component technologies, message protocols, and data formats that are different from what components in the monolith use. However, adapting the monolith to use the new technologies and message formats used in microservices can be expensive and difficult to implement.

There are existing client applications that use components in the monolith that are being extracted as microservices. However, due to technical or organizational constraints, updating and redeploying these old client applications to call the new microservice instead of the monolith components may not be feasible.

The organization may have an API gateway or a similar application integration element available in their runtime infrastructure. However, this element is not currently being used to centralize calls to the monolith.

Teams developing in the monolith would like to take advantage of features provided by the new microservices being developed. However, due to technical constraints, it can be difficult for components in the monolith to make calls to the new microservices.

Therefore,

Extract functionality out of the monolith into microservices, add a façade to route calls and transform messages as needed.

Components are gradually extracted from the monolith and redesigned as microservices. Sometimes this is straightforward as there are components or services within the monolith that are not tightly coupled and can be extracted and wrapped as a microservice. If this is not the case, you might need to Replace as Microservice. Existing client applications or components that use services moved out of the monolith can be handled as follows:

- They can be rewritten to call the microservices. This option takes time to be rewritten and may not be achievable before these components become microservices.
- They can remain unchanged and have their calls go through a routing interceptor—the façade component. The façade component performs the protocol bridging and message transformations for the existing client components to interact with the new microservices.

Figure 8 illustrates the application of this pattern to component X. The component is extracted and becomes microservice X’. An API Gateway or similar application integration product is a good candidate for the implementation of the façade element. Another option is to use the Backend for Frontend (BFF) pattern to implement the façade logic. This approach is similar to the Wiping Your Feet at the Door pattern [10] and can be considered a type of an anti-corruption layer [11]. Note that façades are related to adapters, decorators, mediators, and proxies [13].

In addition to protocol bridging and message transformations, the façade component, which acts as a reverse proxy, can perform several operations, such as security controls, dynamic message routing, traffic monitoring, circuit breaker, and even caching. The figure below also shows that a new client application may also call microservice X’ directly, if the extra features of the façade just mentioned are not required for this interaction.



Note that in this example, X is decoupled from the rest of the monolith. Usually some detangling is needed to extract the component. If components inside the monolith were clients to X, then you can either adapt those clients to have them calling the new microservice X’ through the intercepting façade, or if possible adapt them to directly call X’. Alternatively, you could Proxy Monolith Components to Microservices, making X be a proxy to X’.

* * *

Whether you are Starting Small or not, you can Extract Component and Add Façade whenever desired functionality in the monolith can benefit from being moved to a microservice. Sometimes while doing this you may need to apply Macro then Micro. Some components can be extracted to microservices even if you need to rewrite some functionality (Replace as Microservice). Either way, for any functionality in the monolith that needs access to the extracted behavior, you can Proxy Monolith Components to Microservices.

The façade component has the benefit of enabling existing clients to seamlessly interact with the newly created microservice. However, there are trade-offs to consider. The façade is an intermediary that introduces a performance overhead. If not properly designed and monitored, it may become a performance bottleneck and a single point of failure. Besides, its implementation may use platform-specific technology that generates coupling to a given framework or platform.


################################################################################


  Macro then Micro 
===================

      (aka Divide and Conquer)

The decision has been made to evolve to using the microservice architectural style. Parts of the monolith include larger pieces that can be componentized and possibly extracted.

How can we pull pieces out of the monolith and migrate them to be implemented with microservices?

The monolith is a large deployment unit that encompasses functionality pertaining to different subdomains. Some changes to the system require changes across subsystems, sometimes creating bugs or other issues. Teams don’t have a full understanding of the domains and subdomains. The monolith has certain larger components where analysis has shown that pieces can be broken out.

There is a desire to pull out pieces that are causing pain and start using new protocols, languages, and more—specifically by implementing pieces using microservices.

There are different development teams that work on different parts of the monolith. These teams would like to change the system without affecting large parts of the monolith or without having to release the monolith.

Static-code analysis and other architecture conformance mechanisms were not used during most of the monolith evolution. Developers had freedom to take shortcuts and add dependencies across components within the monolith when implementing new features or fixing bugs. As a result, the monolith is significantly tangled. Component interdependencies make it difficult to isolate fine-grained, cohesive components.

There are some larger components that contain a lot of smaller pieces of functionality that can be pulled out into their own component(s) or service(s).

Therefore,

Pull out pieces that you can extract even if they might be a larger “macro” service. After extracting, refactor to break the larger service down to smaller microservices.

This strategy is more challenging when trying to extract pieces out of a monolith. A monolith usually has larger pieces or components that are tightly coupled and harder to extract as smaller pieces without a lot of refactoring. In these cases, pulling out a larger piece can make the effort to refactor easier. An example can be seen in Figure 6. In this example the “Buy Service” is first extracted out of the monolith. This service is a first step toward microservices, but it can be a larger service especially if it has some couplings which make it difficult to extract into smaller pieces. Once this intermediary solution is stable, we can more easily separate the “Buy Service” into two separate functions such as “Shopping Cart” and “Checkout” services seen in the example. Once the service is extracted out, new clients can begin to access and use the functionality of the new microservices.



* * *

Sometimes you can apply the Macro then Micro strategy by Extracting Component and Add Façade, then break it down into smaller services. If the functionality is tightly coupled in the monolith, you may need to completely rewrite the functionality (Replace as Microservice). Whenever the monolith needs to access new services, you can Proxy Monolith Components to Microservices.

An advantage to Macro then Micro is that teams can take advantage of microservices sooner while they learn how to evolve the domain to more manageable pieces. Sometimes you don’t know where to partition the domain, and this pattern allows you to extract functional pieces (albeit larger than desired) then break them down later (divide and conquer). These larger functional pieces are usually modeled around bounded context pieces of the domain. Note in Figure 6, “Buy Service” is well defined as a subdomain part of e-commerce and thus could evolve to smaller pieces within that bounded context. Domain Driven Design (DDD) [11] has become a popular technique for modeling the domain and finding the right size services as the system evolves.


################################################################################


  Microservices First Strategy 
===============================

      (aka All new functionality as Microservices)

The decision has been made to evolve to using the microservice architectural style. The organization has Started Small and Paved the Road toward using microservices.

How can we encourage people and teams across the organization to start evolving to microservices?

During the long-running process of strangling a monolith, it’s natural that developers and especially managers feel inclined to add pieces of functionality to the monolith. Doing so is typically faster and less expensive than providing the same functionality as a microservice. If there are no design standards or policies set forth to require new functionality to be created as microservices, the monolith may see occasional growth despite the strangling effort.

Many developers work on the monolith. Some of them may not fully engage in microservice development, perhaps because they didn’t get acquainted with the new technologies and tools. These developers are more prone to keep adding code to the monolith, including services that should be created as microservices.

Some in the organization—possibly new to the organization—feel the urge to create microservices. They see potential technical and business benefits.

The shiny new object effect: microservices is the shiny new technology and some developers want to experiment with it.

There are certain requirements or needs that could benefit from new technologies that are hard to implement within the monolith.

Therefore,

Whenever adding any new functionality, whenever possible add it as a microservice. This directive includes making it easier to add the functionality as a microservice and to also make it harder to add the functionality in the monolith.

The main objective is to avoid or contain the growth of the monolith. Sometimes this is done by communicating and encouraging teams to add new functionality using microservices. Some team members or developers are excited to do this and the system can start to see some of the benefits. Teams can also be encouraged by creating templates or examples (Paving the Road) making it easier to add the functionality with microservices.

On the other hand, it is usually the case that many teams and developers are more comfortable with changing the monolith and will be tempted to take the more expeditious way to add the new feature by relying on what they have always done in the past. In these cases organizations might want to take the approach of adding “speed bumps” such as using a governance committee. This committee permits new code to be added using microservices, but if you want to change the monolith, you need to convince the committee why the change should be done this way. Figure 5 illustrates this situation.



* * *

When applying the Microservices First Strategy, you often Start Small and apply Macro then Micro. While implementing a new requirement with a new microservice, you can sometimes see where to Extract Components and Add Façade; other times you will Replace as Microservice some functionality. While adding new microservices, you will Proxy Monolith Components to Microservices as needed.

An advantage of Microservices First is that the organization can expedite the migration to microservices thus reaping the benefits of microservices throughout the organization sooner. There is an additional advantage of not making things worse in the monolith. However, it is possible that it could take longer to implement new features as you cannot quickly add the feature in the monolith such as using copy/paste techniques. Also, there is a cost for providing the training, tools, people and support for microservices. Finally, there is time and effort for setting up a governance committee which also can slow down any development that needs to be done in the monolith.


################################################################################


  Pave the Road 
================

      (aka Make Microservices Development Easier)

The decision has been made to evolve an existing monolith to use the microservices architecture style. This has organizational consequences.

How can we encourage teams and make it easier to write microservices?

Some developers are excited about creating microservices and hence experimenting with microservice-related frameworks and technologies that are new to them. However, the organization has little or no experience building cloud-native or microservice-based applications.

The monolith is deployed through a ceremonial process that requires the coordination of different development teams and operators. This process hinders organizational agility.

The practices, policies and technologies for establishing a DevOps environment are not in place. Developers are not familiar with containerization, continuous delivery, log consolidation, and other recommended practices for microservice solutions.

Therefore,

Make it easier to develop microservices by providing templates, training, policies, and infrastructure elements that set the fundamental environment for creating microservices.

There are many ways to Pave the Road. The first thing is to get the infrastructure up and running. To be successful with microservices it is important to have a good DevOps environment. This includes an automated pipeline of building, good tests, deployment, and monitoring as part of the process. Documenting this process and sharing the best practices with examples is another good early practice to help Pave the Road.

Another thing beneficial for teams is to provide a way to generate or build the core of a new microservice. Sometimes this is through code generation or using some descriptive data that can be interpreted. Other times it is useful to have templates or examples. A combination of these techniques can be used. Following are a list of several potential solutions that can be applied to ease the tedious programming tasks for creating new microservices:

- Define processes and set up tools that provide the infrastructure for automating the pipeline for building, testing, and deploying the microservices.
- Create simple examples, templates, and/or scripts to show developers how to write the microservice.
- Develop a tool that generates the core microservice from a higher-level specification or a wizard tool such as a DSL for microservices. This requires a lot of effort and is done only when an organization is growing a lot and is mature in microservices development.
- Design and document a reference architecture for microservices. This should include a description of the various components and connectors, and any implementation details. Hire experienced people and provide training and/or mentoring.

There are many things to consider when deciding on an appropriate solution. Our advice is to do the simplest thing possible that minimizes your maintenance effort and evolve as you learn. This includes both the effort required to develop your microservice including building the pipeline and deploying (DevOps).

Paving the road for microservice projects includes several technology elements related to the microservice runtime environment, such as containerization, container orchestration, log consolidation, monitoring, and distributed tracing. It also includes DevOps practices, some of which require infrastructure and tool automation; for example: continuous delivery [7], Externalized configuration [2], and infrastructure as code [8]. Many organizations hire microservice experts to avoid risks and expedite the learning of the new environment. With or without an expert in the ranks, the organization will typically launch a pilot microservice project. The team for this project should have ace developers that are also good at transferring knowledge. They shall pave the road while building the pilot project and documenting what is needed for other teams to follow their steps. The documentation can take the form of README files, instructions on a wiki, architecture decision records [9], a template for microservice projects, a reference architecture, and more.

Another important consideration is to rethink the way applications deal with persisted data, as they move from a more centralized database approach to the typical data decentralization used in microservice architectures. For example, there might be the need to use the Saga pattern [2] in place of the original single-connection transaction in the monolith.

* * *

This pattern goes hand in hand with Start Small. An initial small microservice project might be the pilot project that will shed light on the various new technologies and tools that get to be adopted for microservice development. This pattern is similar to Paving the Wagon Trail [10] from the perspective of creating templates, scripts, or DSLs. However, this pattern also talks about other things that help, such as building the infrastructure, documentation, training, and hiring good people.

One of the main benefits of Paving the Road is that it creates an Easier Path [6] for developing microservices. New teams or people can roll out their first microservices quicker, by learning from the examples, documents, and templates created by the pioneer teams that Paved the Road. On the other hand, it requires a lot of time and effort for building the software, process, template, docs, etc. Some of these can be difficult such as DSLs. Also, there are maintenance issues associated with these items. The initial microservice projects that will Pave the Road will take longer and require a high upfront investment that will only pay off later.


################################################################################


  Proxy Monolith Components to Microservices 
=============================================

      (aka Proxy Monolith Service to Microservice)

You have a monolith that has been providing value to your organization and parts of the system are becoming harder to change.

How can logic that was once in the monolith and is now implemented as microservices be accessible to existing and new client components?

During the long-running process of strangling a monolith, components in the monolith are gradually replaced with microservices. The microservices may use protocols and message formats that are different from what is used in the monolith. However, evolving the monolith to use the same standardized message formats used in microservices can be expensive.

There are old client applications that use a component in the monolith that is being extracted as a microservice. You would like these clients to take advantage of features provided by the new microservices. However, due to technical or organizational constraints, updating and redeploying these old client applications to call the new microservice instead of the monolith component may be challenging.

You would like to have the monolith take advantage of the new features provided in the microservices. It is technically feasible for components in the monolith to make calls to the new microservices. However, the cost and risk of updating a large number of components in the monolith to call the new microservices is high.

Therefore,

As you move functionality out of the monolith components into micro services, keep the old components in the monolith solely as proxies to redirect calls to the new microservices.

In this variation of Strangler, old client components remain unchanged. Monolith components that were rewritten as microservices no longer process the calls. These components still expose the same contract, but all they do now is to route calls to the new, microservice-based implementation. Therefore, instead of adding a façade interceptor component, in this variation we have the monolith components acting as proxies to the new microservices—in a sense, the monolith is being converted into a façade. Because new microservices may have different contracts, monolith components acting as proxies may need to perform the message transformation and protocol bridging.

Figure 9 illustrates the application of the pattern to components X and Y. The components are extracted and become microservices X’ and Y’. The diagram illustrates the fact that extracted services can be synchronous or asynchronous. For example: service Y’ can be a synchronous REST service, and service X’ can be a consumer of a message queue (e.g., a Kafka topic) and hence be activated by asynchronous messages or events. In this example Y is proxying directly to Y’, while X is proxying by sending a message to a queue that X’ subscribes to.



* * *

Whether you rewrite pieces in the monolith (Replace as Microservice) or Extract Components and Add Façade, if components in the monolith need access to the new microservices, it is common to Proxy Monolith Components to Microservices. There are possible failure scenarios inherent in distributed systems that must be dealt with when applying this pattern. These failures could compromise meeting the reliability and performance requirements of the system.

Unlike Extract Components and Add Façade, this pattern does not require creating, configuring, and monitoring a façade component to allow existing clients to seamlessly communicate with new microservices. Therefore, this pattern is in general easier to implement and govern than the previous one. However, similar to the solution with the façade, the performance overhead exists of an extra network hop.

This pattern requires rebuilding and redeploying the monolithic application whenever a component gets extracted as a microservice, which was not the case with the façade solution. The main benefit of this pattern over the façade solution is related to handling calls from within the monolith. Figure 9 shows that components X and Y are called by an existing outside client application and also by other components inside the monolith. These clients in the monolith are unaffected by the solution because they still see the same contract exposed by X and Y, even though the actual business logic got deferred to X’ and Y’. We don’t have that benefit with Extract Components and Add Façade. In that pattern, internal clients would need to be adapted to calling the new microservices, as the façade typically would not be intercepting the in-VM calls within the monolith.

This pattern is a variation of the traditional Proxy pattern [13] for distributed systems. In this case, the proxying components are also known as remote proxies or ambassadors.


################################################################################


  References 
=============

[1] Foote B., Yoder J., “Big Ball of Mud,” 4th Patterns of Programming Language Conference (PLoP 1997), Monticello, Illinois, USA 1997. Pattern Languages of Programs Design 4, Harrison N., Foote B., and Rohnert H., eds. Addison-Wesley, 2000. [2] Richardson, C. Microservices Patterns: With Examples in Java, Manning, 2018. [3] Newman S., Monolith to Microservices: Evolutionary Patterns to Transform Your Monolith, O’Reilly, 2020. [4] Alexander, C., Ishikawa, S., Silverstein. M. A Pattern Language. Oxford University Press, 1977. [5] Ambler, S. & Sadalage, P. Refactoring Databases: Evolutionary Database Design. Addison-Wesley Professional, 2016. [6] Manns, M. L., Rising, L, More Fearless Change: Strategies for Making Your Ideas Happen. Pearson, 2015 [7] Humble, J. & Farley, D. Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation, Addison-Wesley, 2010. [8] Morris, K. Infrastructure as Code, O’Reilly Media, 2016 [9] Nygard, M. “Documenting Architecture Decisions.” 2011 blog post located at: https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions. [10] Wirfs-Brock R. and Yoder J., “Patterns for Sustaining Architecture,” 19th Patterns Language of Programs Conference (PLoP 2012), Tucson, Arizona, USA, 2012. [11] Evans, E., Domain-Driven Design: Tackling Complexity in the Heart of Software, Addison-Wesley, 2003. [12] Yoder, J.W., Aguiar, A., Merson, P., Washizaki, H., “Deployment Patterns for Confidence,” 8th Asian Conference on Pattern Language of Programs (AsianPLoP), Tokyo, Japan, 2019. [13] Gamma, E., Helm R., Johnson R., and Vlissides, J., Design Patterns: Elements of Reusable Object-Oriented Software, Addison-Wesley, 1995.


################################################################################


  Replace as Microservice 
==========================

      (aka Extract to Service, Replace as a Service)

The decision has been made to evolve to using the microservice architectural style. Parts of the system have been written using microservices. Other parts of the system that are changing a lot are still in the monolith and causing some challenges.

How can we extract important tightly coupled monolith components to microservices with minimal impact?

New features typically require several pieces of the monolith to change in a coordinated fashion. You would like to extract parts of the monolith that are changing a lot to use the microservice architectural style. However, the monolith is tightly coupled with many dependencies between internal components.

You would like for teams working with the monolith to evolve and take advantage of new technologies. However, the team that is responsible for the monolith might not be willing to add features or evolve the monolith in any way. The team might have budget or time constraints, or maybe the monolith is a legacy system that uses deprecated technology and they simply decided not to evolve it.

The organization has moved to use continuous delivery for the deployment of its software solutions to improve agility and shorten the build-test-deploy cycle. However, the monolith has grown large and it is getting hard to change it without affecting large parts of the system. Also, its deployment requires the coordination of several development and DevOps teams, making it infeasible for continuous delivery.

Therefore,

Reimplement critical components or functionality from the monolith as microservices. While doing this, lock down this part of the functionality in the monolith.

Some core pieces of functionality are tightly coupled within the monolith, and hence are almost impossible to extract from the monolith. The functionality in these core pieces is nonetheless needed outside the monolith, and it would be useful to make them available as microservices. This scenario warrants the option to rewrite a component that provides core functionality as a microservice—an example of such component is X in Figure 7. The new version is microservice X’, and it becomes the primary locus for that functionality. The original component X in the monolith is locked down (frozen) from evolution. New client components should call microservice X’ instead of X. As microservice X’ evolves, a client component of X inside the monolith may need to access functionality that is in the microservice. In this case, there are two main alternatives. One is to rewrite the client component to call X’, as shown in Figure 7 for component Y. The other is to have component X proxying calls to new features in X’, as described in Proxy Components to Microservices. This alternative could be in use for Z and W in Figure 7.



The following outlines the steps for the Replace as Microservice approach.

- Code-freeze the functionality in the monolith.
- Create a new microservice implementing the functionality that you want to replace.
- Canary Release [7, 12] the new microservice while carefully testing.
- Gradually make old client components call the new microservice instead of the old component in the monolith.
- Eventually remove the extracted functionality if feasible.

* * *

Sometimes the part of the system you want to extract is tightly coupled and hard to extract. In these cases, you lock down (freeze) code changes to this functionality in the monolith. If any components need access to the new features in X’, then they can either directly call X’ or you can Proxy Monolith Components to the Microservice from X to X’. If Y is a larger component, you can apply the Macro then Micro approach.

If on the other hand, you can refactor part of the system into components or modules, removing some dependencies, then you can use the two patterns discussed next, Extract Component and Add Façade and Proxy Monolith Components to Microservices.

Replacing functionality with microservice provides flexibility and the benefits of being able to use new technologies, frameworks, and platforms. Also, teams can experiment with new ideas with less risk of breaking the monolith. On the other hand, the organization loses the benefit of adding features in the monolith for that frozen piece of code. Also it can be complex for pieces in the monolith to take advantage of the new features implemented in microservices. Finally, there could be data-syncing issues between the data store in monolith and data stored in the new microservices; specifically, in the data used in the frozen code.


################################################################################


  Start Small 
==============

      (aka Gradually Evolve the System, Baby Steps)

You have a monolith that has been providing value to your organization. The software is growing and changing quickly, and evolving the monolith is getting harder. The decision has been made to use the microservice architectural style.

How can we start evolving to microservices?

The organization wants to minimize or amortize cost to evolve to the new microservice architectural style and would like to do this fully as soon as possible.

Teams and people want to start right away and successfully implement microservices. However, the organization is not ready for a major move to microservices in terms of infrastructure and operational practices. The operations team worries about the prospect of microservices creeping everywhere.

Only a few of the developers have the technical skills and the drive to create microservices. Thus, we look to these developers to overcome the hurdles in creating the first microservices, and show the way to the others.

Therefore,

Take baby steps when starting. Start small, either by writing some new functionality with a microservice or take out a small piece putting it into its own service.

The main thing here is to start small. This can be done either by implementing something simple or by pulling a few existing items out of the monolith. This is a good way to warm up if you have some simple and fairly decoupled capability. Figure 4 illustrates that these initial steps toward microservices can involve moving a simple component out of the monolith (X in the figure, which is recreated as microservice X’), as well as creating new logic as microservices (Z in the figure). The diagram shows that a new microservice (Z) may need to make service calls to the monolith, which is still a one-stop shop for most functionality. Figure 4 shows a fairly decoupled component (X) being removed from the monolith and reimplemented as a microservice (X’). Extract Component and Add Façade and Proxy Monolith Components to Microservices provide solutions if you need to adapt existing clients to call X’. The scenario where component X is being called by other components within the monolith is addressed either by adapting those calls to call X’ or by the design solution described in Proxy Monolith Components to Microservices.



Note that evolving to microservices requires a minimum level of operational readiness. It requires having a DevOps deployment environment, with a continuous delivery pipeline to independently build, test, and deploy executable services, and the ability to secure, debug, and monitor a microservice architecture. Operational readiness maturity is required whether we are building greenfield services or decomposing an existing system. These early baby steps can help teams understand microservice architecture better, including getting the needed infrastructure in place. This evolution to operational readiness impacts the organization which will need to evolve and adapt practices.

* * *

This pattern is closely related to Pave the Road, which can add organizational and technology elements that encourage and enable the successful initial steps prescribed by Start Small. Adding these elements doesn’t happen at once. More likely, the organization will run a pilot microservice project that will drive the adoption of tools, technologies, and practices. This pattern is similar to Baby Steps in Fearless Change.

The Start Small pattern sometimes uses the Macro then Micro approach where you extract some components from the system into services and if needed Proxy Monolith Components to Microservices. It might be a small simple step to extract something larger from the monolith and as you learn. You then later further refactor them into smaller microservices. Once you have been successful Starting Small, the organization can begin to use the Microservices First strategy, avoiding adding anything to the monolith.

The main advantage to Starting Small is that the organization does not incur the high cost and risk of a widespread change in technology. Initial microservice projects face several challenges and technical roadblocks. By Starting Small, future microservice projects won’t have to pay the same price because you are able to apply principles that you learn from these beginning projects. Also, by Starting Small, you can potentially get some benefits sooner from microservices (new technologies, small changes, etc). On the downside, microservice adoption will take longer and you have to maintain and govern the old monolithic systems as well as the microservice solutions for a long time. The diverse technology increases the total cost of ownership (TCO). Finally, it takes longer to get the full benefits of the new architecture because there is this slower evolutionary process, specifically because you are taking baby steps rather than “commit and move forward” with most of your teams.


################################################################################


  Strangler Application 
========================

      (aka Strangle the Monolith, Evolve System with Microservices)

You have a monolith that has been providing value to your organization for some time. The software requirements are changing more rapidly than your organization can accommodate; adapting the software, adding features, and managing existing features in the monolith are difficult due to: (i) significant coupling between components in the monolith, or (ii) significant synchronization complexity in the deployment process among the teams working on the monolith. Sometimes the need for rapid changes to the software comes with a need for the organization to evolve and grow. The decision has been made to evolve to using the microservice architecture style.

How can we start evolving the overall architecture to better meet the needs of the organization, specifically an evolution to microservices?

A legacy monolith is used by several client applications; the monolith and those applications are still providing value to the organization. There is a lot of code and tight coupling within the monolith that make rewriting it expensive. Adding functionality to the monolith is becoming harder and sometimes creates bugs.

When migrating legacy applications, you often need to consider how to replace the application in a piecemeal way. A legacy monolith rarely shows obvious seams for separating it cleanly. How can we replace it with a Microservices Architecture without doing a full rewrite? A full all-at-once rewrite and replace is a risky operation and would also be expensive and would not show value until the complete rewrite is finished.

There is a desire and sometimes potential benefit to use new protocols and technologies. However, client applications make use of the monolith by calling services that use old protocols and technologies (e.g., SOAP, EJB) or by adding module dependencies to the monolith and directly calling the logic inside it. New applications being developed use different technologies, programming languages, frameworks, and API design standards from those used for the monolith, thus limiting reuse of the monolith by these new applications.

Deployment becomes difficult—it requires testing the whole system because the changes might have affected other functionality. You also have to redeploy the whole system with downtime that is increasing with the size of the deployment unit.

Therefore,

Gradually create microservices that are independent of the monolith, growing them in number over time until the monolith is replaced (strangled) by the new microservices. Redirect requests for these new services as needed.

The main idea of the strangler application is to use an approach that allows you to avoid a full “big-bang” rewrite and replace, yet still allows you gradually evolve and replace the monolith. This is an volutionary process that redirect requests for parts of the applications from the legacy monolith and towards a new “microservices” implementation of those features. This is done by adding these features incrementally (the strangler application) and updating the redirection logic towards the new features as they are added.

You have a useful legacy system that is a monolith. There is the need to develop new applications that use different programming languages, different frameworks, or simply newer incompatible versions of languages or frameworks. Consequently, these new applications cannot directly call components in the monolith. Perhaps the software needs to evolve or grow rapidly, but it is getting harder to evolve the current system. The following situations can complicate the process of transforming a monolith to microservices:

- A monolith uses old versions of libraries and frameworks. Developers want to upgrade to the latest versions, but the upgrades are not fully backward compatible and require updating a lot of code in the monolith. These upgrades have been postponed time and again over the years, and now the discrepancy between the old version and the latest makes the upgrades costly and risky.
- The monolith was written in a programming language that is no longer the best choice for the current context at the organization.

Extracting logic out of the monolith into microservices may create a situation where the same logic needs to be accessed by both old and new client components. Existing clients need to access the logic the old way, and new clients will access the logic the new way, using current protocols and API standards. A general approach is to create a proxy or façade for old external systems or client components (see Figure 3). This façade sits between the client components and the logic that exists in the monolith which is being moved to microservices.



Initially, this façade doesn’t do anything but pass all traffic, unmodified, between old client components and the legacy application (monolith). This approach is a way to Wrap the Monolith to protect old clients from change. As microservices replace monolith components, this façade transcodes protocols from old clients into the protocols, technologies, and contracts used by the new system being created. Note this could be a two-way façade as there could be communication coming back from the monolith to the old client components.

Eventually the legacy monolith becomes strangled and can be removed. This evolution is notionally represented in Figure 3. Note that even when the strangling is complete, there could be some old client components that might not be updated thus still needing the façade.

* * *

Many people don’t consider Strangler because they believe it will cost more—specifically because you have the old and new systems to maintain. However, trying to refactor or completely rewrite the monolith can be costly and time consuming. An important reason to consider Strangler over a cut-over rewrite is reduced risk. Another is cost amortization, because many organizations cannot afford an overall rewrite of the monolith in a single undertaking. The main idea is to incrementally migrate functionality from the monolith to the new strangler application, focusing on high-value items or tasks first.

One goal of strangling is to make it an evolutionary process. It is a good idea to Start Small and Pave the Road. Also it is desirable to make it so that changes do not break existing functionality and to limit making changes to any client code that needs access to the new microservices (Wrap the monolith). There are different variations that can assist with this goal. One way is to Extract Components and Add Façade. This pattern enables any client calling code to continue to access the desired functionality. Other times you could rewrite parts of the monolith (Replace as Microservice) and Proxy Monolith Components to Microservices, thus any old monolith code will continue to work. Applying Strangler enables the system to continue to provide value as the system evolves. Having many frequent releases helps you monitor its progress while adding new functionality, thus making sure the monolith system continues to function properly. As features are migrated to microservices, parts of the monolith become strangled and these parts can be retired.

Even with the above mentioned benefits there are also trade-offs when applying the Strangler. The main challenge is related to data. A monolithic application typically uses a centralized database, whereas microservices typically follow the Database per Service pattern [2]. The simplest alternative is for an extracted microservice to directly access the monolith database, but that alternative creates undesirable coupling and, if applied, should be temporary. There are different alternatives for “extending the strangling” to the database using database refactoring [5]. A common alternative requires replicating the data across a dedicated microservice data store and the monolith data store. This option increases the design complexity, requires the implementation and constant overseeing of a data synchronization mechanism, and may cause the monolith and/or the microservice to access stale data due to the eventual consistency setup. Another challenge of the Strangler is the extra effort to maintain and govern two types of software architectural styles—the monolith and the microservice—that typically use different implementation technologies, runtime environments, infrastructure elements, and deployment procedures. This technology diversity may significantly increase the total cost of ownership (TCO) for the organization. Finally, because the strangling process is usually a gradual evolution, it can take quite a bit of time to take advantage of the new architecture.

An idealistic goal is to completely strangle the monolith (the monolith is gone). Although you might be able to completely strangle the monolith, there are cases where the benefit of doing so will not be worth the effort. Sometimes, part of the monolith can provide value and feeds the new microservices architecture, but is not worth completely rewriting. For example, some core pieces of the monolith are providing value and not changing. You have wrapped these pieces so that they are easy to use. Perhaps a core piece of code is entangled and quite difficult to rewrite or extract; it could make sense to leave this code as-is. This is especially true if you have addressed the essential problems in the monolith and don’t need to go any further.


################################################################################


  Event Based Architecture Introduction 
========================================

In this section we present a small set of linked patterns that represent the fundamental building blocks of an Event Driven architecture. An Event Driven approach is an important (some would say essential) extension to the services approach that supports decoupling services in a way that is not just compatible with a cloud approach, but in many cases, absolutely required when you are pursuing a Hybrid Cloud approach. For more information on Why an Event Driven approach can help, see this sidebar.

As we are advocating the use of an Event Driven Architecture within a cloud architecture, most likely within a Microservices approach, it’s worth discussing briefly when an event-based or request-response approach is more appropriate. There are no hard and fast rules, but here are some considerations:

A component should use a request-response (HTTP/REST) approach to send a service request when:

- It knows exactly what service it wants to have run
- It wants it run the service exactly once
- It needs to know when the service completes successfully
- It wants to receive the results of the service invocation

A component should use an Event-driven approach to announce an event when:

- It wants to notify many potential consumers that might be interested in an occurance
- It doesn’t need to know what consumers may be interested in an event
- It doesn’t need to know how each consumer may react to an event, or indeed expects that different consumers may react differently.
- The communication is one-way: The announcer doesn’t want to wait for the consumers to react, indeed doesn’t know when they’ve all reacted, and isn’t interested in the results from the reactions

In this style of interaction, the announcer sends a notification and then is unconcerned with the outcome.

This set of tightly related patterns describes the basic issue of how to construct an Event Driven Architecture from component parts. The patterns discussed are:

- Event Driven Architecture extends the services concepts of components and loose coupling with a new concept of event coupling to further enable evolution of families of applications, in which new components can be added to the family, enriching the services provided and the contexts triggering use of those services, without altering interactions between preexisting components and applications in the family.
- Event Backbone provides the shared medium allowing multiple Event receivers to obtain events from multiple Event providers without having to know the identity or physical location of their event coupled partners; the Event Backbone also insulates Event receivers and producers from awareness of the network topology and message transport protocols used to receive, move and deliver events.
- Event Sourcing allows you to represent complex application state by scanning over an event stream.
- Event Joiner helps you tie together related events that form a logical stream of events in order to process the elements in the stream as a group or to deliver them individually to an appropriate processing context where they can be handled.
- Event Deriver can build new events from current and old events based on comparisons to previous event data, including temporal information.
- Event Aggregator helps you to avoid flooding your Event Backbone with low-level events while allowing other applications dependent upon the results of those events to function.
- Event Topic Registry provides a way for decoupled event producers and consumers to share essential metadata about events. If metadata sharing was not possible, event producers would not be able to make event consumers aware of events and event messages, and event consumers would have no way to discover and subscribe to pertinent events.
- Event Consumer decouples components by allowing different components (microservices) to only register for and receive those specific events they are interested in.

The relationships between most of the patterns in this section are shown in the diagram below:




Table of contents
=================

-  Aggregator 
-  Backbone 
-  Consumer 
-  Deriver 
-  Event Sourcing 
-  Event-driven Architecture 
-  Joiner 
-  Topic Registry 
-  Why add Event Driven Architecture? 


################################################################################


  Event Aggregator 
===================

You are building applications using an Event Driven Architecture. You have applications in your architecture that are interested in different types of events. For instance, you may have applications only interested in business events, and other applications that are only interested in high-level infrastructure events. Finally, you have applications that generate many types of events, ranging from the very high level to the very low-level.

In many situations, you have a need to have events of different formats and from multiple sources be fed into the channel for processing. The issue is that often there are disparate categories of events that are processed on the bus. In particular, quite often you see that events can be partitioned into “low” level and “high” level events that are of interest to a disjoint set of applications on the bus.

Consider the following example. In an environment that processes credit card, checking and savings account transactions one of the more interesting issues is how to look for fraud. What you certainly don’t want to do is to send every potential transaction event through the entire fraud-processing application. The transaction volume (in the millions of transactions per day) would overwhelm all other processing. Instead, you want to segment off certain types of events, like purchases over a certain limit, or checks made out to “cash” and then scan them for further processing. Likewise, a sequence of small transactions to the same merchant in a short time period might be an indicator of fraud. So, limiting the total number of events evaluated by some set of criteria is an important consideration, as is scanning for particular patterns across multiple events.

How do we prevent flooding an Event Backbone with many, many events that are not of a type of interest to most other applications connected to that Event Backbone? In other words, how do we keep the events flowing on an Event Backbone pertinent to the applications connected to that bus?

There are many small events which are not of interest to most applications. Specific sequences of these small events are often of interest to business applications.

Therefore,

Define a micro architecture that splits the main Event Backbone into at least two distinct component parts. In the first piece, called the Aggregator, you define a series of processing steps that do the following actions:

This combination of steps is shown in the picture below:



The types of events that are routed to the “main” Event Backbone are of limited and distinct types that are meaningful to applications connected to the main Event Backbone. This division of the Event Backbone will prevent flooding of the “main” Event Backbone by events that are not relevant to other applications.

In effect, applying an Event Aggregator is a form of Federation of Event Backbones. By splitting the backbone into at least two parts - one of which (the Aggregator) is a one-way gate into the other(s), we introduce all the issues that federation involves into the solution. For instance, we must decide how to physically connect the buses, and must address issues of latency and Quality of Service.

Also, if the Aggregator becomes disconnected from the main Event Backbone(s) then we must address the issue of how to detect that and determine how to re-introduce lost events into the main backbone(s). In addition, we are in effect partitioning the events into two different classes - this means that any new applications that depend directly on the low-level events that are filtered and transformed by the Event Aggregator should connect to the Event Aggregator, and not the main Event Backbone.

It is important to note that the filtering and aggregation steps can be implemented in many different programming styles. For instance, while this approach can be implemented algorithmically, it can equally be implemented using a Machine Learning model. The combination of an available historic stream of events (from Event Sourcing) for training and the ability to execute the results of running the Machine Learning model on the real-time stream from the Event Backbone is very powerful for many domains such as bank fraud detection.


################################################################################


  Event Backbone 
=================

You are building applications that will follow an Event Driven Architecture. You have several disparate systems that all generate events and that need to respond to events.

Event driven systems have come in a number of different types and styles. From the very early days of the initial implementations of the Observer Pattern in Smalltalk through the various event systems in GUI frameworks like Microsoft Windows, Java’s AWT and Java Swing through the development of distributed event systems that operate over protocols like Apache Kafka, all event-driven systems have held some simple elements in common:

The issue is that each of these elements has often “locked in” developers to particular solutions. For instance, in order to produce events in an implementation of the Observer Pattern the event provider (the Subject) would have to know about each of the event receivers (Observers) in order to deliver events to them. Likewise, many event systems require that when a receiver registers to receive events that the events that they receive be of the type (or structure) that is provided by the provider – regardless of whether or not that structure is meaningful to them. Finally, when we choose a particular event delivery system such as WS-Notification, we are often restricting ourselves to a particular mechanism of transport – usually HTTP in that case.

How do we develop systems so that the event producers and event consumers need not know about each other’s physical location, need not care about the details of the structure of the events produced or received, or be concerned about the event delivery mechanism?

We do not want to force users into a situation where a technology choice limits their possibilities for communicating events. For instance, if you choose to communicate all events exclusively through IBM MQ, then you limit yourself somewhat in communicating events to business partners on the global internet. Likewise, if you choose only an event notification approach such as WS-EventNotification, you may limit yourself to those applications that support SOAP Web Services.

Finally, you do not want to limit producers or consumers of events to one particular physical location from which to send or receive events. Encoding such location of event infrastructure into the consumer’s code limits its ability to scale and to deal with failure situations.

To address the problem, we must develop an Event Backbone that is responsible for the following things:

- It should support independence of physical location of event consumers and event providers. An Event consumer should not be aware of where the events are originally generated. An Event provider should not be aware of who receives its events, or how many receivers subscribe to its events. This can be achieved through the use of distributed messaging protocols.
- It should allow for multiple independent channels or topics that allow for the separation of different event types - receivers can choose to subscribe to those topics or channels that they choose and no more.
- It should allow receivers and providers to choose their own event structures. In order to support this it should allow for translation of event structures on the bus.

Many architectures use open source products such as Apache Kafka as some or all of the implementation of their Event Backbone. Kafka allows for location independence (it can even be run distributed across multiple sites) and allows for the specification of multiple event structures and topics. Likewise most major cloud providers support their own Event Backbones (see for example AWS EventBridge, IBM EventStreams, or Azure Event Hub). The basic structure of these systems is shown below.



Although not absolutely required to implement an Event Backbone, most of the above systems employ a combination of Publish-Subscribe Channels and a persistent Event Log to form the Event Backbone. The existence of a persistent Event Log makes Event Sourcing possible.

The implementation of an Event Backbone assumes the implementation of Event Consumers that are selective in the events they process. Putting in an Event Backbone will solve the immediate issues of location transparency but can then lead to further issues in overloading applications with a flood of events. The Event Aggregator pattern addresses this issue.


################################################################################


  Event Consumer 
=================

You are building a Microservices Architecture and you need to decouple one microservice from another

How do you allow a microservice to find out when data belonging to another microservice changes?

One of the strengths of the Microservices Architecture can at first appear to be its greatest weakness. A key tenet of Microservices is that they are decoupled from each other and do not share data through a shared database. However, there are many cases in microservices development when a microservice will need to know if the state of another microservice has changed in some specific way.

For instance, if you are building an Airline system, the reservation microservice represents the combination of the fact that a person (a passenger) has booked a specific seat on a specific flight. However, if that flight is cancelled, how does the reservation microservice discover that fact? It could constantly query the flight microservice for its current status, but that would be both inefficient and potentially error-prone if the request is made over HTTP, which is notoriously unreliable.

Therefore,

Declare your microservice as an Event Consumer that can listen in on a channel or topic that indicates that an event has occurred that it is interested in. When the event occurs, the Event consumer will receive a notification of that event.

Event consumers are the recipients of the diverse events they may have indicated a preference for. A sophisticated event driven architecture will cater to the existence of diverse types of consumer that are capable of receiving events of varying degrees of complexity, throughputs, technologies and technical maturity. Events that an EventConsumer listens to must travel over an Event Backbone

You can see how a Microservice can act as an Event Consumer in the diagram below:



One final aspect worth noting is that acting as an Event Consumer can solve one of the problems with the Microservices Architecture. If an application consists of several Business Microservices, each of which own their own data and have their own schema, then you will sometimes run into the problem where two Microservices: Microservice A, and Microservice B each own their own data - but then a requirement comes along later that needs you to represent the equivalent of a SQL JOIN of the data of A and B. One way to solve that would be to allow the new “Join” Microservice (C) to access both databases, but that would be a violation of the Microservices Principles. Instead, a better way to do that is to have “C” be an Event Consumer of both A and B, and for A and B to publish events whenever any of their data changes. That way, C can own its own schema, and be independent of the data representations of both A and B.


################################################################################


  Event Deriver 
================

You are building Event Driven Systems. You have a set of related events that you can tie together into a logical set (perhaps through application of the Event Joiner Pattern) that you are interested in.

When you have a set of related events, there is often a need to respond based not only on the occurrence of a single event, but on the occurrence of several events in a predefined sequence.

For instance, consider a system whose responsibility it is to detect a condition in a server that may indicate that the server is overheating. Let’s say the server contains a temperature sensor. Now, a single temperature measurement (a temperature event or an “over limit” event) may not be interesting. Someone may have just momentarily put their hot cup of coffee on top of the server for a moment and then lifted it off. Or someone may have blocked the air flow into a server temporarily by moving a piece of equipment past a vent.

However, if you have several “over limit” temperature events in a row, over a period of time, then you may have a condition that is detrimental to the functioning of the server. But you can only determine that if you can somehow link each individual temperature event to other events of that type.

How do you respond to a series of related events by generating a new event that depends on the occurrence of many different events over time?

There are several things that complicate this:

- You need to be able to define the sequence of events you are interested in both in terms of the actual events and their properties and in terms of the set of events that is of interest.
- You need to be able to avoid “false positives” that can be caused by too simple rules. For instance, a simple counter of events of a type would not suffice since the relationship between the events may require temporal comparison (for instance, it needs to be no more than 30 seconds since the last event).

Therefore,

Create a process, e.g., an event deriver, that is responsible for listening to events, determining if a particular sequence of events has taken place, and creating a new, derived event as a result.

There are at two different, but closely linked approaches to solving this problem that arrive at the same end. The first is the state-machine approach. In that approach, a state machine instance is created upon receipt of the first event matching a particular correlation id. As new events are received for that same correlation id, the state machine moves along its lifecycle. At various points in the lifecycle of the state machine, new events (Derived Events) can be generated as a result of a state transition, reflecting that a number of linked events have been processed in a particular sequence. As each state is matched, the state itself records information about the conditions that led to its transition (information about the last event).

A second approach that has the same effect is to have a rules engine that evaluates new events that enter the deriver – as new rules fire, they can either produce intermediate results that may affect the result of later rules, or they may produce derived events as a result. In this case, the intermediate results need to include all the necessary information to be able to allow later rules to fire – this may need to include information about the last events received.

A “canonical” use of the Event Deriver Pattern is in detecting financial fraud of various types. For instance, consider the following problem. When a person’s credit card or ATM card is stolen, then the thief may try to use it as often as possible within a short period of time before it is reported as being stolen. Thus, a large number of cash withdrawals from an ATM card in a short period of time may indicate fraud. Likewise, if a card is stolen and the number posted on the internet, then it may be used from many different locations at once. This would also be an indicator of fraud.

You could solve this problem by applying the Event Deriver pattern. An Event Deriver would watch the stream of events looking for particular sets of attributes in the events. For instance, as it sees two ATM withdrawals for the same account either in a short time frame, or widely separated in distance, it may increment a potential fraud count. When the potential fraud count reaches a limit, it generates a new “stolen card” event and places that on the backbone.

An Event Deriver becomes a new source of events that are different than the ones to which it responds. As such, Event Derivers may bridge between different event channels or even different Event Backbones. The Event Aggregator pattern discusses how an Event Deriver is part of an architecture of bridging Event Backbones.


################################################################################


  Event Sourcing 
=================

When you are designing applications that use Events you often find that the approach of representing the state of your application simply in a “point in time” is not compatible with the overall architectural goals that Event Driven Architecture fosters.

Let’s say that you’re building an application to represent a complex, evolving process. A simple example of this may be a car with a driver that’s participating in a car-hailing service (like Uber). All day long the driver will receive notifications of hails, decide whether to respond to those hails, pick passengers up, take passengers to locations, drop them off, and then repeat the process. The problem with a “point in time” representation is that it is hard to keep up-to-date. What’s more, it’s also hard to figure out the logic of how to “undo” things when conditions change.

For example, let’s imagine that a driver receives a request to pick up a passenger. We could say that the current state of the car is that it’s “on a trip”. But what if the passenger cancels the request before they are picked up? Or what if the passenger arrives at a destination only to let the driver know they only wanted to use an ATM machine and then be carried to another location, which was their real destination? The conditions evolve quickly and are hard to represent in a static form.

How do you represent a the state of an application that is constantly and unpredictably changing as a result of evolving conditions?

Trying to represent this complex set of conditions as a single state variable would be difficult - you could implement it as a state machine, but there’s always the chance that your state machine may not represent all of the possible “edge cases” of what can happen in reality - like Passengers cancelling both before the ride begins and after it begins, or the driver deciding they want to cancel the ride for some reason (like fearing their safety).

Therefore,

Record the state of the system as an ordered set of events. Allow the application to query the current state by examining the recorded event stream.

In the case above, you can find the current state of the car by simply examining the last event on the event stream. But representing your application state this way brings other advantages too:

This kind of backward looking through time over an event stream is illustrated below:



This pattern was first elucidated by Martin Fowler in this paper


################################################################################


  Event Driven Architecture 
============================

Loose coupling helps the decomposition of a system or application into independent components, by minimizing the dependencies of each component on the implementations of other components which it uses. SOA formalizes the component service interfaces and interaction protocols so that any individual component can be replaced by any other with similar functionality and the same service interface without compromising the correct behavior of the overall application. Component implementations and implementation changes are hidden from users of the provided services.

The benefit of loose coupling is that an application can be evolved incrementally, replacing individual component implementations one at a time with enhanced versions while preserving the correct behavior of the overall application at each step. Loose coupling as a concept can easily be extended to the use of Events. An event is an announcement of an interesting occurrence of a phenomenon. This idea then leads to a type of loose coupling we call “event coupling”.

The distinguishing characteristics of event coupling are:

- The interactions are “one-ended”. An event producer emits an event; an event processor is notified of events matching his interests.
- Event interactions are notifications of something that has happened. They carry no intent that any particular processing or service is expected in response to the event.
- Event interactions are one-way transmissions. Since there is no implied service processing there are no defined responses or results to be returned to the event producer, ( although subsequent independent event interactions may allow any processors of the event to communicate back to any of the emitters )

Event coupling facilitates the smooth evolution of a family of applications, based on the event coupling characteristics described above.

- A completely new application can be added into the family with a new event handling service interface; by expressing an interest in appropriate events, processing in the new application will be initiated at appropriate times using appropriate event data as input, even though no other application or component in the preexisting family explicitly invokes it.
- New event producing components and applications may be added into the family. Their emitted events will trigger additional appropriate processing by event processors (even though the new event producer is not aware of what this processing is.)
- Intelligence may be added to improve the quality of event based matching and interactions - for example additional duplicate event detection and removal may increase efficiency of component interactions within the application family by eliminating redundant processing before the event processor is invoked.

People need to evolve families of applications so that new applications and services can be incrementally added to the system introducing new paths of interaction between components. The new style of interaction should include:

- Appropriate triggering of processing in newly added components and applications based upon existing occurrences of interest
- Triggering of appropriate processing when new applications and components are added into the family that announce occurrences of interest
- Incremental addition of systemic intelligence to allow more efficient and effective matching between the occurrences reported and the interests and needs of processing components.

How should a component announce notable occurrences so that other components can set themselves up to be informed when relevant items of interest have occurred?

When a source component detects a notable occurrence, it could invoke a service on the target component to cause it to react to the occurrence. But that couples the source to the target, because the source has to know about the target to invoke its service, which means that the source cannot work without the target. It also assumes the source knows how the target will want to react, and that the target will always want to react the same way to every occurrence. It also assumes that there’s only one target; for more than one, the source will have to know about all targets, know which service to invoke in each, and know which occurrences each target is interested in. And then the source will be coupled to all of those targets, such that it cannot run unless it has all of them.

Above is an event source components view of the forces at work. Consider the equally important forces as seen from the event consuming component’s viewpoint.

Consider a component with responsibility to assign corporate internal telephone numbers to newly hired employees. Does this component have to take responsibility to contact each Human Resources office to get information about new hires? For each employee this might require gathering information which became available at different times, and involve collecting this from a possibly varying set of Human Resources offices in the corporation. To bring all this information into the telephone numbering component and for analysis is inefficient; furthermore many other components need to be prompted with the same new hire event for other tasks such as: start paying salary, send welcoming mail package etc. There may be no single business process which understands everything which has to be done when a new employee is hired. Centralizing detection of notable events into an Event Backbone and making this event information available to interested components is much more efficient and allows for flexibility.

Hence we need a solution which supports definition of a mediating Event Backbone to which events can be reported and which supports subscriptions by components to pattern of events in which they are interested. Such an approach decreases event messaging traffic, increases the efficiency of event analysis, aggregation and correlation, and allows the dynamic introduction of new patterns of event information flow into a community of existing applications (without change to the existing applications).

Therefore,

We solve the problem described above by:

- Introducing an Event Backbone which provides which provides a level of indirection and mediations between event producing and event processing components.
- Providing an Event Topic Registry in which components can discover event channels relevant to their needs and lookup relevant channel information
- Allow Event Consumers to subscribe to patterns of events over time and from multiple sources

You see the interaction of these components in the diagram below:



An Event-Driven Architecture extends loose coupling to event coupling in which because the targets are optional; the source announces the occurrence the same way whether the announcement goes to no, one, or multiple targets. It enables each target to receive the notification and decide how to react, if at all, such as which service to invoke.


################################################################################


  Event Joiner 
===============

You are building applications that will follow an Event Driven Architecture.

Events are tricky and often transient things. There are many cases where a single event is only interesting when considered in the context of other events that may have previously occurred or may occur later. Connecting together events where unique identifiers are not consistent (sometimes called event correlation) is handled in many every day processes.

Consider a mortgage application process which is monitored for successful completion. As the application is reviewed by different individuals the mortgage application number is typically used as the unique ID. As background checks are done by a 3rd party, a different unique ID may be used to reference the customer. When the 3rd party returns their findings, this information needs to be correlated with other events which are already being correlated together. The unique ID used by the bank and the 3rd party may not be known until the time the 3rd party is engaged to investigate the individual. It may actually be dynamically determined based on some unique ID generator or it may be a different piece of information from the mortgage application.

Another example would be in the healthcare industry. When a patient comes in for surgery or some type of overnight stay, the hospital needs to track all types of actions which occur around the patient in order to correctly and fully bill the patient. As the patient receives treatment, medicine and meals, all of these activities can potentially be generate events through RFIDs. Many of these events may not have the specific patient ID associated with them as they are specific to the medicine cart or meal cart. These events must be correlated based on different IDs to track that the proper care is being provided and the proper billing is recorded.

Finally, consider the basic problem of connecting log events across different products like a web server, an application server and a database server so that you can view a single customer “transaction” across all three. The issue faced here is the same as the issue faced previously.

How do we connect events separated by time, space, or other distances to other events in order to determine if the combination of events is interesting?

With events coming at different times, from different sources and different technologies and over different protocols, the ability to identify an individual event which is to be considered in the context of other events is a challenge To identify the event with the context, a relationship needs to exist between the incoming event and the context and then recognized when the event is processed. Relationships are typically defined in the format of 0-1, 1-1, 0-n, or n-n with all formats requiring a unique identifier on all relationship objects. For events and event contexts, the relationships are limited to 1-1 and 1-n with 1 event to 1 event context or 1 event to n event contexts. For both types of relationships, a unique identifier must be associated with the event and a unique identifier must be associated with the event context(s).

The unique identifier for the event could be based on a number of things.

The unique identifier for the event context can be set by a number of things.

Each event context must be started with at least one of the above unique identifiers. As additional events are received they will be checked for unique identifiers which match the relationship instance for an active event context. If the event does not have a matching unique identifier, the event will not be associated with the event context.

For many large, distributed solutions which are focused on processing events from a variety of technologies and event emitters, it becomes more difficult to specify a unique identifier in the event which matches the relationship instance for an active event context. As a business transaction flows through the different distributed technologies, session ID or transaction IDs or other metadata is not readily passed. Different protocols do not always have the capacity to pass and/or receive metadata in header packets. Also the way in which information is passed in headers is not defined in a standard way. Also as the business transaction is executed, the business data which was used in the event payload at one point of the application may not be available at a subsequent point. Without the same business data available at that point, a unique identifier which was associated with the relationship instance will not be available and will not be included in an emitted event.

Correlating an event of this type will not be possible unless the appropriate event context has an additional unique identifier that can be matched. The additional unique identifier however, may not be known when the event context is started or within the context, it may not be appropriate to have an additional unique identifier recognized until a specific time. When an additional unique identifier is needed, there are a number of options for specifying the value.

- Specified in a previously received event and set as an additional of unique identifier
- Calculated from a previous event payload or metadata
- Received through a response from a request sent to a service provided a unique identifier.

Therefore,

Build event joiners whose job it is to tie together different events that are part of a united stream. Since each event type may differ in its information content, there should be a different event joiner for each event type.

All Event joiners performs the same general sequence of actions:

Event joiners operate on the “edges” of different event streams, as events move from one type to another. For instance, consider the following problem. If we are trying to correlate log events from an HTTP Server, an Application Server, and a Database Server, then we would need an Event Joiner that begins by looking at the events coming from the HTTP Server. From the information in the HTTP Request a unique correlation identifier will be generated for each unique request. When the log messages are generated in the Application Server, then the joiner working on the Application Server events can use the information in the HTTP request sent to the Application Server to identify which internal thread id corresponds to the correlation id from the HTTP Server and can use this internal linkage to append the common correlation id to the WAS log events. Finally, since the Database Server joiner can use the information about the database request (userid and SQL, at least) to correlate the log events in its event stream to those from the Application server and the HTTP Server.

Once the events have been added to the second event conduit or stored in the persistent event store then other processes can act on that stream, for instance an Event Deriver can react to a series of related events.

This pattern can be used for bringing seemingly unrelated events together into an event context in which they can be correlated in order to generate additional events.


################################################################################


  Event Topic Registry 
=======================

Event producers and consumers are expected to be decoupled, yet they have a shared interest in certain aspects of events and event messages.

Given an underlying EDA assumption that event producers will not directly interact with or interrogate event producers about events and event messages, some means is required for that information to be shared. Access to metadata is also important for prospective event producers. If a new process or context is associated with occurrences of something of interest, then it may become an event producer. Event metadata is needed to be able to publish events of the correct “type” to the correct topic.

Events do not exist in a vacuum. At some point in the lifecycle someone has performed analysis to identify things that occur that are of interest beyond the scope of the occurrences themselves. Identification of these scenarios involves several things: identifying candidate sources of those occurrences, candidate interested parties, candidate data that characterizes the occurrence and potentially additional candidate secondary data that is associated with the occurrences.

The event producer generates events of a specific event type known to the producer. Event consumers can consume events of a specific type known to them. The loose coupling nature of event producers and event consumers and the events types which are involved does not allow for event processing to directly occur. Between the event producer and the event consumer, there is not enough information that is shared to perform event generation and consumption.

How do we decouple the event consumers and producers while allowing them to find each other to share metadata?

There are several issues that make this decoupling difficult:

- How to identify the essential (required) and non-essential (optional) metadata attributes for events, event topics and event types/messages
- How to structure the metadata associated with event types and event topics
- Where to deploy an event registry and how event producers and consumers should locate it.
- Agreeing on a standard interface to the event topic registry
- Ownership over an event topic registry
- Trustworthiness of event topic registry contents
- Normative guidance about how many event types should be associated with an event topic. Multiple event types per topic are likely to create an overly complex scenario.

Therefore,

Create a registry to publish metadata about events that a producer will emit. The registry allows an event consumer to find and subscribe to those events, and then understand and utilize the information found in event messages.

Although a key principle for Event Driven Architecture is decoupling of event producer and event consumer, there is never the less some essential information that needs to be shared between them. The event registry allows this sharing to take place, but in a very indirect and decoupled manner. Similar to a service registry, an event registry contains metadata about events that is needed by both the event producer and event consumer.

The Event Registry pattern has three participants:

Candidate metadata for the registry can include:

- event type name
- event topic name
- event type schema (note: subtypes are possible)
- purpose of event type or event topic - why the event is interesting (still being discussed as to whether purpose should be associated with an event type of an event topic)
- event topic (which 0..n event producers will publish to, and which 0..n event consumers will subscribe to)
- additional information that could be descriptive of event context
- additional information that could be descriptive of the event producer (there could in theory be several producers of the same or similar event, and in some scenarios one might be considered to be of more interest than another, while in another scenario it might be appropriate to subscribe to events from several producers)
- additional information that could be descriptive of event lifecycle (e.g. how long should an even message be considered valid or current, how long would an event message instance be retained in an event store)
- additional information that could be descriptive of nonfunctional requirement or quality of service aspects
- additional information that might associate this event with a complex event pattern (or disassociate it from such a pattern)
- additional information that might be descriptive of “out of band information” associated with an event (i.e. information that is pertinent to the event but is not included in the event message)

An Event Topic Registry can be thought of as a subtype of a Services Registry, in that the subscribers of the Events are similarly dependent upon the schema of the information forwarded to them on the topics as Service Consumers are upon the schemas provided by Services.


################################################################################


  Why add Event Driven Architecture? 
=====================================

One of the more common questions we receive when discussing why you need to add Event Driven Architecture to an existing Services-Based architecture (for instance one based on REST) is “What does this do that we can’t do with traditional REST service calls?” It’s easiest to understand the difference, and the advantages of an Event-Driven approach, through an example. A good example that is easy to understand is the effect on a travel itinerary where a flight runs late. What effects can a flight running late have?

- If you have a connecting flight, you may miss it and need to be rebooked on a later flight
- If you have a rental car reservation, you won’t need it until later
- If you have a hotel reservation and you won’t get to that city until the next day, you won’t need the reservation for tonight
- If you get stuck at a connecting airport overnight, you’ll need a hotel reservation in that city
- If you have a meeting scheduled at your destination, you may need to delay the meeting time or cancel it, in which case you need to notify the other attendees
- Your loved ones, colleagues, pet sitter, etc. may want a courtesy message letting them know you’ve been delayed

These are the consequences and adjustments per passenger or party traveling together. Multiply them times the 100 or so passengers on the flight. There are also consequences for the airlines:

- The plane will be delayed for its next flight. Another aircraft may need to be assigned for the next flight and this aircraft reassigned.
- The flight crew may miss their next flights. The flights must be delayed or replacement crews assigned.
- The flight crew may exceed their work limits, meaning that other flights they work on must be delayed or new crews assigned.
- Crew members who are passengers on the current flight may not be available for their assigned flights.
- The flight’s destination gate can or should be changed. Likewise for the gate, baggage, cleaning, and food service crews scheduled to handle the flight when it arrives.

All these consequences occur because one flight is late. Once the airline knows its flight is late, what can it do to mitigate these consequences?


  A Centralized Solution 
=========================

The Airline could know about the full travel itinerary of each passenger of the flight: not only connecting flights on the same airline, but also flights on other airlines, reservations for rental cars and hotels, and any other travel arrangements. For privacy reasons, the passenger may not be comfortable revealing all of this information to the airline. The airline doesn’t want to be responsible for storing all of this information and for processing updates to frequently changing travel plans. And even if the airline had the most up-to-date itinerary, it doesn’t want to be responsible for having to change the reservations with all of these other travel service providers. The scenario becomes even more complex for updating a passenger’s plans on arrival. What meetings is he scheduled for and who was he meeting with? Who is meeting him at the airport or shortly thereafter? What services, such as house sitting, need to continue until the passenger arrives at his destination? What people are affected and how can they be contacted? However, the airline doesn’t want all of this responsibility. Likewise, the passengers don’t want to divulge all of this information. Luckily, there is a better approach using Events.


  Event Notification Solution 
==============================

The airlines already have a solution to this problem. On most airlines, you can register for notification of changes to a flight’s departure or arrival time. The notifications can be sent by text, voice, or email. This service can be used by passengers on a flight and by meeting passengers for a flight. This notification capability can be combined with programmatic dependency registration and notification messaging to modify plans when a flight is late. The plan modification is distributed to dependents, each adjusting its own part of the plan according to its own rules specific for it particular service.

Some examples of how airline flight notification can help a passenger manage the consequences:

- An itinerary object in the airline’s computer represents a passenger’s flights. It receives notification of the flight delay, determines whether the passenger will miss a connection, and schedules new flights accordingly.
- If the itinerary introduces an overnight stay in a connecting city, the airline reserves a hotel room in that city for the passenger.
- The reservation record in a rental car company’s computer registers for notifications for the renter’s incoming flight. When it receives notification of a delay, according to company business rules, it releases the renter’s car to other customers and reserves a different car that will be available at the flight’s new arrival time.
- The hotel’s computer has a similar reservation. When the airline itinerary announces the incoming flight has changed to the next day, the hotel cancels the passenger’s reservation for that night and makes the room available for other customers.
- The passenger’s travel agency or executive assistant has his full itinerary record. When it receives notification, it can handle verifying rescheduling by the individual services, resolve any discrepancies, and surface any issues to human representatives.
- The passenger’s calendar program (Notes, Outlook, etc.) can register dependency with the airline itinerary and/or travel agency itinerary. When the itinerary changes, the calendar (perhaps with human assistance) can look for conflicts, start rescheduling meetings, and notifying meeting participants.

The flight notifications can also help the airline manage its own resources:

- The aircraft has an itinerary representing its flights. If a delay means it will miss a connection, the airline can employ business rules to delay the aircraft’s subsequent flights or find replacement aircraft.
- The crew members (pilots, flight attendants, etc.) have itineraries, hours worked, and rules representing constraints on hours worked and schedules. Upon notification of a delay, it can delay the crew members’ subsequent flights or reschedule the crew members and find substitutes.
- Crew members riding as passengers have subsequent work itineraries represented as meetings. When a flight is delayed, these meetings/itineraries must be rescheduled.
- Ground crew members have meetings scheduled to service the aircraft, meetings that must be rescheduled when a delay occurs. Likewise, resources like gates and fuel trucks are scheduled for meetings that must be rescheduled.

This solution depends on each service vendor being able to react to notifications and reschedule accordingly. But if that’s in place, all the airlines must do it:

- Maintain a schedule for each flight
- Maintain an itinerary for each passenger composed of flight schedules
- Provide the means to register dependency on either of these objects (flight and/or itinerary)
- Provide notification to an object’s dependents when the object changes

In this way, event notification can be used to provide complex coordination amongst separate vendors working together to provide coordinated services. Each vendor is responsible for handling notifications when they occur and providing notifications when needed. No vendor has more information than it needs, except to know what resources it depends on (such as a rental car company or hotel needing to know a customer’s incoming flight).


################################################################################


  Coexistence Patterns Introduction 
====================================

A major issue with adopting cloud is that in most Enterprises, the applications that most need to run on the cloud are not new. Enterprises rarely get to build brand new “greenfield” applications. Instead, most applications are “brownfield” meaning that they were built for one set of purposes and under a particular set of assumptions. Unfortunately, those were rarely the assumptions that apply on the cloud.

This set of patterns deals with the problem of how to deal with the realities of the refactoring process when you have to rewrite applications to work on the cloud, particularly when you are rebuilding them as microservices. These are complements to other patterns in the pattern language as they only apply in specific cases where you are already applying other, more basic patterns from Cloud Native Architecture and Event Based Architecture. In particular, they focus on the problem of how to keep the system operating during the refactoring process when the old and new systems must live side by side, often sharing the same data.

- Data Virtualization is often a good first step in that it “puts off” the refactoring of data and allows you to begin with refactoring code to microservices while keeping the underlying data representation the same.
- Command Query Responsibility Separation is a basic pattern related to other patterns in an Event Driven Architecture that helps you to separate microservices into separate parts for Reading and Writing, which is an important aspect for microservices refactoring.
- Data Replication is crucial for keeping the different databases (for the Read Model and Write Model) in CQRS in sync.
- Playback Testing is an approach that helps you with determining that you can complete the strangulation process for a microservice
- Entity Comparator is needed in some cases to help you determine that Playback Testing is succeeding when the team has limited knowledge of the existing system being replaced.

The relationships between these patterns and the patterns they rely on is shown below:




Table of contents
=================

-  CQRS 
-  Data Replication 
-  Data Virtualization 
-  Entity Comparator 
-  Playback Testing 


################################################################################


  Command Query Responsibility Separation 
==========================================

You are building an application that is following a Microservices architecture. The team is not operating in a complete greenfield - there are existing sources of functionality or data that must be reused in order to complete the application on time and within budget. In particular, you cannot transition all at once to a Scaleable Store because critical data is stored in a large, monolithic database.

How do you deal with the fact that you can’t usually transition all at once between existing monolithic data stores and the database-per-microservice approach?

The problem is that reading from data is different than writing data. A service implementation usually has a specific “projection” of a set of relational data that represents a specific view of the data. That view can be usually cached using any of the data caching patterns described in this pattern language. The issue is that writing to the database often involves writing to multiple tables with complex business rules dictating how the information being written needs to be validated and updated. It’s that latter code, often encoded in legacy applications, that is difficult to change.

Therefore,

Use Command Query Responsibility Separation (CQRS) to provide different read models and write models to the same data allowing two different routes to allow you to write microservices as adapters to existing data while allowing you to change the responsiblity of data management over time.

CQRS separates the “command” operations, used to update application state (also named the ‘write model’), from the “query/read” operations (the ‘read model’). Updates are done as state notification events (change of state), and are persisted in the event log/store. On the “read model” side, you have the option of persisting the state in different stores optimized for how other applications may query/read the data. You see this in the diagram below:



One of the key aspects of CQRS is that when it is used for modernization purposes that it requires ta Data Replication approach in order to keep the Read Model and the Write Model in synchronization. Let’s imagine that we have set up the following approach by combining existing patterns. We have created a new Read Model that is a projection of a data set in an existing application by creating a brand new Domain Microservice. We have also created a new Write Model that is an Adapter Microservice that translates from the new API to the existing API of the old application. This will require us to set up some type of Data Replication between the two in order to have the projection of the existing data keep up with changes to the Write Model.



The most common way of setting up this Data Replication in this case would be by introducing an Event Backbone between the existing application and the microservice that is serving as the Read Model. In this way, the new Read Model can subscribe to changes made to the existing system, and update its data accordingly.



The update events can be created directly in the existing application (if you have the ability to modify the existing application) or if you do not, you can use a technology like Change Data Capture to record changes to the application’s underlying database. Many existing Change Data Capture tools such as IBM Infosphere Change Data Capture and Oracle Goldengate support connecting to a Kafka Event Backbone directly, as do open source platforms like Debezium.

What’s more, you can even take this farther. By introducing Event Sourcing you don’t even necessarily need a database for your Read Model that represents the point-in-time representation of the Read Model. Instead, we can simply recreate the current state by reading the event sequence either stored directly on the Event Backbone or in a longer-term archival event database.

CQRS was introduced to most people in this paper


################################################################################


  Data Replication 
===================

You are following a Cloud Refactoring strategy using the Strangler Application and you find the need to slowly refactor a database over time so that each Domain Microservice can in the end own its own data. However, since this process may take months or even years, you will find yourself in the situation where you have different clients that are querying and updating the data in the original, monolithic database, and also the data in the individual distributed databases for each microservice.

How do you keep the data synchronized between the two different sets of databases?

The problem with trying to keep two different databases (especially those that are implemented in different technologies) is that you loose some of the ease of development that happens when you are using a single, monolithic database. Some of these issues are in development only (such as the fact you may have two different programming models for accessing and updating the databases) but many of them are operational as well. The fact that the two databases may be from different vendors, or even implemented with different technologies (such as an Oracle database being replaced by a Cassandra database) means that the job of keeping the applications that use them in sync becomes more complex.

Therefore,

Employ a Data Replication approach that updates both sets of databases whenever one changes.

Data Replication is a strategy that can work in a couple of different approaches, but which approach you choose depends on the particular situation you find yourself in. In situations where data cannot ever be different between the two databases, a Synchronous Replication approach is often the best. However, all Synchronous replication approaches share the same drawback - they increase the time it takes to finish any update transaction on either database, since you have to wait until you receive confirmation of the update on the target database before the transaction can complete on the original database. What’s more, most synchronous approaches are going to be technology-specific, that is, you will have to choose a replication tool that supports your particular database. So, for instance, if you want to replicate part of an Oracle database to another Oracle database, you would use Oracle GoldenGate to provide real-time synchronous replication.

Asynchronous replication schemes have the advantage that they can be more flexible in how you manage the data transformation between the origin and target database. For instance, Change Data Capture approaches that connect to Kafka allow you to route the change logs (often expressed as Events) from the origin to to multiple different receivers that could transform that data into their own target format.


################################################################################


  Data Virtualization 
======================

You are building a Microservices Architecture and you are refactoring applications to Microservices using an approach like the Strangler Application to pull out microservices a few at a time on a domain by domain basis. You are in a situation where it is easier to refactor the business logic first before you refactor the data, but you need to be able to plan out a path for refactoring the data.

How do you allow your microservices code to act as if each owns its own data while in the transition period before the data is fully refactored?

Refactoring is always challenging in a brownfield environment, and is even more so when a large application has a correspondingly large and complex database that is shared by many parts of the application or (even worse) many separate applications. In many cases where multiple teams are involved, it is often easier to start with refactoring code (which may be owned by an application team) than it is data (which may be shared among several teams or owned by separate Data team). In that case, you still need to be able to make it seem to each Microservice that they are independent, even though for a time they are not. You want to let each Microservice determine its own unique schema and database queries and updates, while at the same time letting the underlying data remain unchanged for the remainder of the monolithic application that has not yet been refactored.

Therefore,

Employ a Data Virtualization strategy that separates the underlying data representation from the data representation exposed to the application.

Data Virtualization as an approach can be implemented in many different ways. There are a multitude of commercial products that implement Data Virtualization over several open source and commercial databases. This includes the IBM Cloud Pak for Data and the IBM Virtualization Manager for z/OS that also covers traditional mainframe databases and file formats, in addition to products like Data Virtuality and Informatica that cover a wide range of data options.

If you are working on a Relational Database, perhaps the simplest Data Virtualization approach is just to begin with building Views in your database either Materialized or Standard, which can give you multiple different viewpoints on the same underlying data. Sam Newman covers this as a pattern in his book but the downside of this is that even though this gives you freedom to use the new queries in your code, it is limited to originating relational databases only. If your originating database is not relational (e.g. is a flat file or other legacy database format) you can’t build a view from it and must instead use a Data Virtualization tool.


################################################################################


  Entity Comparator 
====================

You are refactoring to a Microservices Architecture by using the CQRS pattern and are applying Playback Testing. Your situation is complex in that simply checking for the final state of the system after running a number of transactions through the playback would not be sufficient. This may be because you don’t truly understand the way in which the current system functions, or because you do not have a full test suite for the existing system.

How do you determine if the new refactored system is working correctly when you aren’t sure what transaction results should be a-priori?

In many refactoring and replacement situations, particularly with older systems, not only are the developers that built the system long gone, but the code may not even be able to be fully analyzed for all possible functional outcomes. In many older systems, including COTS systems, the source code may be unavailable, or may be written in a language (such as ASM) that is not easily understood.

However, even if the code cannot be easily understood or analyzed to build a functional test suite, what can usually be understood is the data (particuarly data in flat files or a database) that the code produces. What’s more, the data has the advantage of being persistent - you can often write new programs that run asynchronously that the team can use to read the values of the persistent data and report on it to other programs.

Therefore,

Build an Entity Comparator that plays back each transaction one at a time and then compares the state of the corresponding affected entities in both the existing and new systems then logs a report of the comparison values discovered throughout the entire playback sequence.

Applying this pattern will help your team gain confidence in the functional test coverage of the new application even when it is impossible to determine all of the functional requirements of the existing application.

This pattern is only implemented when you are also implementing the Playback Testing pattern.


################################################################################


  Playback Testing 
===================

You are modernizing an existing application and are applying the Command Query Responsibility Separation pattern and employing Event Sourcing. You are doing this over a period of time, using the Strangler Application pattern in such a way that there is always a period of coexistence between the old system and the new (refactored) system. Likewise, you are changing underlying data structures and/or switching to new Scalable Stores as part of your refactoring effort.

How do you ensure that the new microservices maintain the same functionality as the old system, especially when the amount of detailed end-to-end application knowledge of the existing application may be limited?

The Strangler pattern always features a period of co-existence between the new and old systems, but one of the unexpected difficulties of this co-existence period occurs just prior to a new microservice going live. One of the basic principles of refactoring (first described in Fowler’s book) is that before you begin any refactoring effort, an essential precondition is that you have a solid set of tests of the existing system that you can also run against the new system. However, when refactoring legacy systems, you cannot assume that this solid set of tests covering all potential code paths exists.

Adequate test coverage cannot be assumed for systems that were not built with detailed unit testing in mind. Automated test tools only became commonly accepted in development in the 1990’s, and Test Driven Development only emerged in the 2000’s, so systems built prior to that often do not have full automated test suites. In many cases, teams do not have the detailed knowledge of the underlying code in order to build adequate unit tests. So it becomes difficult for teams to determine if a refactored system really will respond in the same way as the existing system. What complicates this is that in many cases, even subtle bugs in the existing system have become enshrined in workflows and UI’s - in such a way that they have become hidden or undocumented features rather than bugs.

What is needed is a mechanism for ensuring that the behavior of the new system can be reasonably determined to be “the same” as the old system when a portion of the old system is re-implemented in the new system. Therefore,

Capture live transactions from existing calls over a period of time from the old system as Events. Transform those Events as necessary to match the API of the new system, and then play them back as Events on the new system to ensure that the behavior of the old and new systems match.

There are three issues in implementing an approach like this. The first is capturing the events. If you are following a CQRS approach using Event Sourcing, then one possibility is to capture a set of existing calls to the Write Model, especially if the Write Model is just serving initially as an Adapter Microservice to the existing system. The calls can be wrapped up as Events and then added to the Event Backbone in this case. If there are still calls going to the underlying existing system that do NOT go through the Write Model, then this becomes a little more complicated, as you may need to construct your Events from log messages or other data capture mechanisms.

Once the captured Events have been added to the Event Backbone the next decision is how long a period of time for which you need to capture the events for playback. Often creating a database for these events may be required if playback will cover a longer period than a few hours. In any case, the Events will have to be transformed to the format consumable by the new (refactored) Write Model - if you are capturing them from an adapter-based Write Model, then this is already done for you, but otherwise will be required if you are capturing them from any other source.

The final stage of this is playing back the captured events on the new (refactored) write model. One key decision to make is then determining how to determine if the state of the new write model reflects the state of the existing system after each transaction is played back and executes. In the simplest case, this can be done at the end of the playback (perhaps comparing summary and total information, by pulling reports, for instance) but in more complex cases, a step-by-step Entity Comparator approach may be required.

The simplest version of this pattern (with capture through adapter-based Write Model and playback comparison through summaries) is shown below:



Note that the goal is to complete the Playback Testing successfully so that the existing System can be completely eliminated and replaced with the new (refactored) Write Model as shown below.




################################################################################


  Cloud Native DevOps Introduction 
===================================

In this section of our pattern language, we address the major development and operational issues that come with adoption of the cloud. It is naive to assume that a team can adopt cloud-native technologies without changing any of their development and operational processes. That would be like putting a Jet Engine inside a Model-T Ford. The way you manage and control your software must be adapted to the way in which your software will be built.

- Cloud Native DevOps is the root pattern for this section. It shows how to bring together several required concepts for delivering microsevices-based applications.
- Container Per Service is the right level of distribution for most microservices-based applications. It strikes a sweet spot between monoliths and fully distributed objects.
- CI CD Pipeline is the minimum level of automation required to perform Continuous Integration and Continuous Delivery of Microservices.
- Red-Black Deploy is the standard approach for doing services deployment in a distributed system (e.g. one hosted across more than one data center or with multiple hosts in a single datacenter) that allows you to deploy without service disruption.
- Canary Testing allows you to roll out possibly disruptive changes in a controlled way to a small group of users.
- Feature Toggle provides you with a way in your code (although controlled by configuration) to dynamically roll new features into a production environment in a limited way.
- Autoscaling is a way of taking advantage of cloud-provider or cloud-platform mechanisms to allow your application operations teams to keep from worrying about detailed issues of ongoing capacity management.

The relationships between this set of patterns are shown in the diagram below.



Likewise there are also considerations on how you conceive the overall deployment architecture of your system that impact the overall operational profile of your system.

- Automate VM Deployment is fundamentally required in order to make sure that you have a consistent approach to seting up the PaaS or container environment that your microservices will run within, in addition to any additional IaaS services that container environment may depend on.
- Three Data Centers is the appropriate number of zones that you need to deploy a system across in order to reach the “sweet spot” of minimum cost and maximum reliability.
- One Region is the minimum geographical distribution of datacenters for a redundant infrastructure.
- Overlay Network is the best way to connect disparate datacenters together into a seamless cloud infrastructure.
- Topology Aware System is fundamental to being able to build resiliant applications in a distributed cloud model

The relationships between this set of patterns is shown below:




Table of contents
=================

-  Aggregate Descriptor 
-  Automate VM Deployment 
-  Autoscaling 
-  Canary Testing 
-  Container per Service 
-  One Region 
-  Overlay Network 
-  Pipeline 
-  Service Registry 
-  Three Datacenters 


################################################################################


  Aggregate Descriptor 
=======================

You are building an application that will need to run in the cloud that is composed of many parts. You may be using a Container Orchestrator to schedule running your containers. You want to be able to deploy your complex application into your cloud environment without having to constantly change the way in which deployment is carried out as small details (such as code versions) change.

How do you define a reusable architecture independent of particular application artifacts?

Useful applications are almost never composed of a single component. At a minimum, most applications require a database, possibly require a messaging system of some sort, and may require other aspects, such as load balancers or other front-end proxy or security components. On the cloud, it’s important to describe some of the deployment metadata that allows the application to take advantage of the unique features of cloud computing, such as Autoscaling. In the autoscaling case, there is a need to specify the upper and lower bounds of scaling in order to ensure appropriate resliance while not allowing the application to become too expensive to run.

The problem comes in as you start thinking about how you can bring all of these things together. There are two possible, but inadequate solutions:

The problem with the first option is that it is both error prone and difficult to repeat. No one wants to have to use a GUI or type parameters in multiple times to deploy an application every time there is a change. Likewise, the problem with the second is that idempotency becomes an issue. You can write shell scripts, or scripts in other languages to deploy application components, but you have to include checks in every routine to decide what to do if the component is already deployed, or you can risk either disaster in losing existing configuraiton or data, or out-of-control costs as you end up with multiple deployments of everything. The answer has to lie somewhere in-between.

Therefore,

Define an Aggregate Descriptor of services, runtimes, and the connections between them. This Aggregate Descriptor is executed by a Deployment Engine that reads the descriptor and creates the necessary containers and other aspects of the runtime system. The resulting combination of Descriptor and Engine should respect idempotency and not repeat deploying what is already at the same level and version.

Examples of implementations of this pattern include Terraform templates and HELM charts.


################################################################################


  Automate VM Deployment 
=========================

You are building a cloud-native system that will be deployed across several cloud datacenters, probably across more than one cloud vendor, or across public and private clouds.

How can you effectively manage deploying the VM’s that a container or PaaS solution (such as Cloud Foundry, or Kubernetes) run on in a multi-vendor, multi-cloud environment?

Managing base images for multiple cloud vendors by hand is error prone as the various base images can diverge. When relying on a single cloud vendor, maintaining a base image (in AWS speak, an “AMI”) is still workable; with multiple cloud vendors and the networking differences they often bring, this becomes impossible even at a small scale.

Therefore,

Use an automation tool like Chef or Puppet to configure machines; in fact, you should prefer to use cloud vendor-independent orchestration tools such as TerraForm to help up bringing them up.

Automation is not unique to this set of patterns, but it becomes extra important when using multiple vendors. Instead of relying on vendor-dependent tooling to manage and protect networks of machines, machines need to be interconnected with an  Overlay Network, machines in different vendors’ datacenters get slightly different settings because networks and available memory/cpu sizes differ, and so on.

Even with a handful of machines, this quickly becomes very error prone when doing manually. Whereas many good DevOps practices can be postponed by teams for a while, when applying the patterns in this language, VM Automation becomes critical very quickly.


################################################################################


  Autoscaling 
==============

You’d like your application’s capacity to adjust elastically based on load. You are using a Stateless Runtime.

How do you plan for handling unpredictable load while at the same time minimizing the per-unit cost of application hosting?

Development teams are notoriously bad at predicting demand on their applications.

What’s more it takes time to gather real customer usage data to develop a capaciity model that is based on actual usage. By the time you realize when your peaks are, it may be too late for the application because it may have crashed from overuse.

Therefore,

Use Autoscaling to dynamically adjust the number of identical runtime instances to only the number required by current traffic loads.

For example, Kubernetes supports the Horizontal Pod Autoscaler that will automatically adjust the number of replicas running in your cluster, likewise Amazon Elastic Beanstalk scales web applications written in a variety of languages to deal with shifting load conditions.

One issue that all autoscaling frameworks have is that they depend on good baselines for application performance in order for the scaling policies to be set correctly. Performance Testing early is critical to be able to set that baseline and to understand when changes to the application affect the baseline.


################################################################################


  Canary Testing 
=================

You are following a Cloud-Native DevOps approach and may be deploying an application with a Microservices Architecture. You are deploying new and improved services at an ever increasing rate, but you have a large number of users that can only withstand a minimal amount of disruption during the process.

How do you determine that a new service version can be put into production without undue risk of disruption to the production system users?

The Microservices idea brings along with it the fact that each service can be deployed through its own pipeline, but importantly at its own rate. That means that as a team becomes more proficient and higher-performing, that the time it takes to develop and deploy new service versions decreases, which could increase the number of times that a user could be disrupted by a bad rollout.

At one level, the reduced blast radius of decisions within a Microservice means that each change would be smaller, and thus potentially less disruptive, but the possibility of disruption, at an increased rate remains. What is needed is a way to not only minimize the potential scope of decisions, but also minimize the potential number of users that could be disrupted by a change.

Therefore,

Employ a Canary Testing Approach through which a new service or service version is only rolled out to a small number of users initially. If the change does not prove to be disruptive, then roll out the service to the remaining user population.

This approach is called “Canary Testing” because of the analogy to the way canaries were taken in cages down into mines in the 19th century. If the canary died, the miners knew that the air in the mine was no longer breathable, and left as quickly as they could before they were overcome by poisonous gases. In this analogy, a small group of users (possibly just internal users) are treated as canaries - if they have a bad experience, the rollout is stopped and the problem is mitigated while leaving the larger group unaffected. If the test is successful, then the rollout continues to the larger group.

Canary tests can be used in conjunction with Red-Black Deploys in that often you would start with a Canary test before rolling out to a progressively larger set of groups. Canary Tests can be automated and made part of a standard CI/CD Pipeline and are built into many CI/CD products such as CloudBees as well as being part of Service Mesh open source projects like Istio. Products such as LaunchDarkly combine Canary Tests with Feature Flags and other rollout approaches.


################################################################################


  Container per Service 
========================

You are building an application with a Microservices Architecture. You want to use containers within a Container Platform like Kubernetes, or a Platform as a Service like Cloud Foundry.

In a Microservices architecture, how do you distribute services across different containers?

In a Dr. Dobb’s article (which was also incorporated into his book Patterns of Enterprise Application Architectures) Martin Fowler coined Fowler’s First Law of Distributed Objects – “Don’t Distribute your Objects”. Wise words - ever since object Oriented Technologies became common, there was a tendency to try to set the boundary of Object distribution far too close to the metal as it were - a tendency to make every object in a systems design a distribution boundary. As Fowler notes, the performance implications of this are legion - this is a step way too far in the distributed direction.

However, the corresponding direction - “Never distribute your objects” is also a step too far. One of the downfalls of early distributed object technologies like CORBA and EJB was that distribution at the object level fell entirely out of style. Entirely self-contained monoliths - with the network boundary drawn entirely at the UI (e.g. HTML) level became far too common.

Contributing to this was the coplexity of managing highly complex distribued systems. Trying to deploy a distributed system was hard work, and one of the simplest ways of making that work easier was to build a self-contained monolith. Part of what contributed to this work (and this perception that managing distributed systems was hard) was the time and effort it took to manage large farms of disparate VM’s.

However, Containers are lighter weight than VM’s. Thus, many of the operational considerations that led you to want to minimize the numbers of VM’s in your system (e.g. long startup time, long system stabilization times) do not apply in a conderized environment.

Despite this, containers are not a complete panacea. With the lighter weight in terms of resource consumption and faster startup time comes a somewhat lower general availability. Each individual container may have a lower uptime number than a corresponding VM.

Likewise, assuming that you would consider a container to be the equivalent of a thread is not necessarily the right approach either. Containers do have a finite, even if small overhead. And the startup, while very fast, is not instantaneous.

Therefore,

Create a Container per Service. If necessary, to minimize runtime footprint, group Microservices vertically.

Examples: See Building Microservices


################################################################################


################################################################################


  Pipeline 
===========

You are either building a new application with a Cloud Centric Design or reimplementing an application as part of Cloud Refactoring.

How do you ensure that your application can be built, tested and deployed in an automated and standardized way?

Therefore,

Define a Pipeline of standard build and deployment stages.

A Pipeline allows code to flow through a consistent, automated sequence of stages where each stage tests the code from a different perspective. Each stage requires the necessary automation to not only run tests but also provision, deploy, setup, and configure the stage. Code should progress through the stages in an automated fashion with as little human intervention as possible.

The pipeline should employ a number of different strategies that make deploying the artifacts pushed through the pipeline simpler. One key strategy is the idea of a Red-Black Deploy that allows for easy rollback. Canary Testing is another useful approach that allows you to validate that your deployment will succeed at scale. Another strategy that is often helpful is the Feature Toggle which aids with the issue that you may want to push features out to different stages faster than you can certify them in that environnment.


################################################################################


  Service Registry 
===================

You are building applications using a Microservices Architecture. You have many different Domain Microservices comprising your application.

How do you decouple the physical location of a service instance from its clients so that the client code will not have to change if the address of the service changes?

You don’t want to have to change client code each time you update a service, for instance, if you update the version of the service, which may change the URL Likewise, You don’t want to have to hard-code the physical addresses of a service into client code – that would make it difficult, for instance, to move code between development and production when the physical addresses of the systems change.

When a system is stable, knowledge about the system is identical no matter where you look. The system has converged and is working at optimal efficiency. However, the more complex a system grows (in terms of the number of machines, the amount of services endpoints, etcetera), the more often changes will be made to it. With configuration systems that rely on (generated) files and application restarts, it will take a while for every change to propagate, and if this time is long enough and system changes are often enough, it is easy to see that the system potentially never converges. This leads to reduced availability, and therefore is economically unsound but also impacts the amount of “damage” a system can take before it becomes completely unavailable.

Therefore,

Use a Service Registry to map between a unique identifier and the current address of a service instance in order to decouple the physical address of a service from the identifier. The Service Registry pattern was first introduced in Daigneau and remains useful in a microservices implementation – perhaps even more so given the number of individual services in a Microservices Architecture.

At its simplest, a Service Registry is just a repository for information about services. However, many implementations add other metadata about the service along with the physical address, such as a generic server name (useful when you have multiple instances of a service) and health information such as status or uptime.

A service registry takes care of quickly spreading new knowledge around the cluster and often only needs a handful of nodes in each datacenter to be resilient. Application APIs are available to actively inform application instances of configuration changes they would be interested in, so that applications can quickly respond; convergence therefore can take place in seconds instead of hours, greatly reducing the time tha the system is in a transitional state and thus minimizing the amount of time that the system has less than optimal resilience.

As the service registry should be Topology Aware, it need to be setup in a topology aware fashion itself; either only returning local references (this can easily be done with any system by running it separately per datacenter) or by returning local references by default and remote references when requested (this needs support in the service registry).

For example, Consul is a service registry system that contains a large number of very desirable features. For example, it has a DNS interface which makes integrating applications a breeze, and it is based on the Raft consensus protocol which makes the code simple to verify and reason about. Most important, though, is the way how Consul handles multiple datacenters.

Consul typically runs as a small daemon on every system. This makes a bootstrap process of “how do I find my service registry?” go away: it sits on localhost at well-known ports. Only a handful of nodes participate in the consensus algorithms, they are called “servers” and usually they are run on dedicated machines. Within a datacenter, the servers use Raft to elect a leader among them, and this leader receives all queries; node-local Consul agents are configured with one or more bootstrap nodes and then join a gossip network to replicate the state of the network (who are the members, where are the members, and who is the leader).

Optionally, Consul can join multiple clusters by so-called WAN bridges. This, in effect, creates a new gossip network between the servers over the WAN, enabling services in one datacenter to forward queries to another datacenter when the queries cannot be locally resolved; for example, this is the case when a client explicitly requests a service lookup in a remote datacenter. There is also support for enumerating known datacenters and nodes in each datacenter, so that it is possible for subsystems that need it to gain complete knowledge of the configuration at a given point in time; this can be used for example to configure high availability clusters automatically, based on the location of available participants in the cluster.

Other examples of Service Registries include Netflix Eureka and the Kubernetes API server.

Not all projects will require a Services Registry – if you have only a handful of services in your application then the setup and management of the registry infrastructure is often more trouble than it’s worth. But if you have more than a half a dozen services then it may become useful when alternative ways of managing service location (such as configuration files) become cumbersome.


################################################################################


  Three Datacenters 
====================

You are building a system that is going to be hosted in the Cloud. You have the option of spreading your application across any number of regions or zones in the cloud. You need to make your system resiliant to the possibility of failures that are inherent in the cloud model.

What is the best number of datacenters to deploy to in a system built with minimum cost but maximum redundancy?

One datacenter doesn’t make a resilient system, but do we need two, or three, or twenty? The more datacenters you have to operate, the more complex operating them becomes; but less datacenters increase the consequences of any single one going down. If you want to guard against losing a single datacenter, then the remaining datacenters need to have plenty of reserve to absorb the load of the one that is now removed from the platform; it’s easy to see that you need $ n $ datacenters each having $ n/(n - 1) $ capacity, from which it follows that the total spare capacity carried ($ 1/(n-1) $) goes down with an increasing $ n $.

On the other hand, every data center requires a set of “overhead” machines–for monitoring, infrastructure, etcetera–which can be regarded as fixed relative to the amount of systems running in that datacenter. With more datacenters, the total of the fixed overhead per datacenter goes up, while the total of the spare capacity carried goes down–often, the overhead increase wins it from the spare capacity decrease. Furthermore, you often want to run consensus systems over your datacenters requiring a quorum decision; these systems almost always work best if there is an uneven number of participants involved in order to avoid split-brain situations.

Therefore,

Choose the minimum amount of datacenters that cannot cause a split brain situation but still will give redudancy: Three. Have good reasons to go above that, and consider always having an uneven number to make quorum consensus simpler.

There are a number of conflicing forces here at work. Even in a cloud, datacenters often have special machines that manage infrastructure and thus do not contribute compute and/or storage to systems running the business logic; examples are login bastion hosts, machines collecting logs and metrics (but see [Outsource Metrics and Logging], machines responsible for running software deployment services, etc. This, in a sense, is fixed overhead per datacenter which needs to be amortized over the machines that do contribute capacity. Especially for smaller systems, this overhead may make adding too many datacenters infeasible. With more datacenters, however, each datacenter contributes less capacity to the total capacity required and thus the spare capacity that needs to be carried around to protect against the outage of a datacenter becomes smaller. The detailed calculations are outside the scope of this pattern, but it should be relatively simple to find, for a given system, a point where the spare capacity plus the total overhead is at a minimum. For example, if the total capacity required is 60 machines, and the fixed overhead per datacenter is 5 machines, then at $ n = 4 $ or $ n = 5 $ the total overhead ($ n * 5 $) plus the spare capacity ($ total * (1 / (n - 1))$) are both 40 - with $ n $ above or below this point, this sum is higher[^2].

The other consideration is more technical and less a matter of punching cost figures into spreadsheets. Various patterns in this language argue that to make the system work as a mostly seamless whole across datacenters you must consider what happens if one goes away. Distributed systems very often rely on not being able to end up in a split-brain situation, meaning a situation where a network outage between parts of a system results in the parts of the system each continuing to work, mutating data, and ending up with conflicts when the parts are re-joined. A simple way to achieve resistence is by making sure that the members of a distributed system only work when there’s a simple majority, or quorum, visible; there can be only one simple majority and thus only one part of a system that keeps working under a loss of network connectivity[^3]. Simple majorities work best if there is an uneven number of participants; in the absence of this condition, some participants’ votes in the system need to be weighed as more important to make sure that a split-brain situation cannot occur; these weighing factors often make systems more brittle and harder to maintain, so if possible an uneven number of datacenters is the simplest solution.


################################################################################


  Scalable Store Introduction 
==============================

The cloud has brought a wealth of new options for data storage. Long gone are the days when the only data storage option was an Enterprise relational database for all applications, regardless of whether or not the type of data that was being stored was suited for a table-based representation or not. However, with new choices comes new potential problems. In particular, the non-functional requirements that an Enterprise relational database addressed (resiliancy, backup, performance, security) now become more important, as the number of ways in which these requirements can be addressed increases.

A key part of the microservices architecture is that wherever possible each microservice should control or “own” its own data. However, microservices are also expected to be scalable and stateless. This combination of requirements leads to the need for Scalable Stores, which is the root pattern of this section.

- Scalable Store addresses how to avoid scalability bottlenecks in a microservices architecture by using databases that are naturally distributed and able both to scale horizontally and to survive the failure of any database node.
- Key-Value Store allows you to do simple lookups by acting in principle, like a hash map, with accompanying O(1) performance for many use cases.
- Document Store allows you to store native schemaless JSON data in a database optimized for storing that type of information.
- Table Store recognizes the fact that relational representations are still sometimes the right way to store and manage certain types of application data.
- Columnar Store helps you manage data that is optimized for certain types of queries that commonly occur in Analytics.
- Polyglot Persistence is critical in helping grant teams building microservices the independence to choose the right tool for the job.

Certain architectural concepts are critical to understanding how Scalable Stores scale across multiple datacenters.

- Synchronous Replication is the solution for when data cannot be lost at all, at any time and must be constantly consistent.
- Asynchronous Replication is the solution for other cases - when data should not be lost but may be inconsistent over a period of time.
- Global Locks are required when implementing systems that require consistency of data across multiple datacenters.

The relationships between these patterns is shown in the diagram below:




Table of contents
=================

-  Asynchronous Replication 
-  Columnar Store 
-  Document Store 
-  Global Configs 
-  Global Locks 
-  Key-value Store 
-  Polyglot Persistence 
-  Scaleable Store 
-  Synchronous Replication 
-  Table Store 


################################################################################


  Asynchronous Replication 
===========================

You are building a system that has data in multiple distributed locations. You need to make sure that the data in those locations is consistent.

How do you guaranteee that data is consistent across multiple instances of a scalable store, but at the same time keep from having to require that all transactions wait until the data is updated in all locations?

Data can be replicated by transmitting changes that occur between transactional checkpoints. Often, data can have gaps and still be valid; data for analytical purposes (for example, user behavior metrics) can potentially miss minutes but still be valuable enough to serve as a source for information. Especially for this kind of data it is often not acceptable to wait for it to be safely replicated as this may put unacceptable delays on interactive user flows.

Therefore,

Send data that can be lost or re-created in asynchronous batches to other datacenters, outside the scope of any transactions (either by using systems that forego transactions, or by sending the data after the transaction has been completed at the source). Messaging systems like Kafka can be helpful here to transport data between datacenters (“fire and forget”).

Asynchronous replication will come at a cost: data will be lost if the source datacenter fails. There is no way around this: some transactions that have just been committed in the originating datacenter before it went down will not have propagated out of it. These transactions will be lost forever if we assume that the original datacenter is lost forever. This is the reason that this pattern stresses its usefulness for data that can be lost. Which data can be lost in the case of a datacenter outage (a very irregular event) is a business decision and therefore not in scope of this paper. An example may be a company that hosts weblogs: it may decide that, in case of a datacenter failover, losing some comments and posts is annoying to the user, but from a business perspective entirely acceptable.

If the business critically depends on certain data, then this data should not be replicated this way; Synchronous Replication should be used instead.

For instance, MySQL has been wildly popular with large scale websites because of its built-in asynchronous replication. Executed statements and/or row changes are written to a binary log on the master and can be fetched by slaves. The log is written on transaction commit and thus only available to slaves outside the transaction scope; this means that replication can cross unreliable network links without impacting the observed speed of on-line transactions against the master.

A lot of “eventually consistent” storage systems work by accepting writes at one node in the cluster, and then replicating it across multiple nodes so that eventually, a quorum of nodes agrees on the data. Cassandra is a good example, because it can be operated to be aware of mutliple dataenter topologies. Data can be written to a local node of a datacenter-spanning cluster, and it will eventually arrive at nodes in other datacenters, asynchronously.


################################################################################


  Columnar Store 
=================

You are building an application that needs to store data in such a way that you can calculate statistics or perform aggregate queries from it. This especially occurs in a Microservices Architecture when you are pulling data together from several microservices in order to perform queries or aggregate data.

How do I store data when I’m often interested in a subset of the columns of all rows?

Traditional Table Stores store data by row. Thus, doing a query over any column will require you to do either a linear search, or to use an index to do a hashed search. For most applications, that is fine - the tradeoff between being able to insert and update data quickly vs. a slight increase in query time is acceptable. However, many types of analytics applications put a premium on query time, and do not require fast updates or inserts. In this case, the row-oriented storage approach of a Table Store is not optimal.

Therefore,

Use a Columnar Store that groups data by column, not by row. Column stores can very quickly operate on subsets of data as these subsets are stored close to each other.

A major disadvantage of a Columnar Store is that updates and inserts may take longer than in other types of Scalable Stores. They should also not be used when the data is loosely structured or unstructured - in a sense, you can think of a Columnar Store and a Table Store as being related in that they both perform best on highly structured data with a fixed, or very slowly changing schema.

Examples of Columnar Stores include Cassandra and Amazon RedShift.


################################################################################


  Document Store 
=================

You are building applications with a Microservices Architecture. You are using RESTful API’s and representing the contents of your HTTP requests and responses as JSON (Javascript Object Notation) documents.

How do you most efficiently store and retrieve the data corresponding to your HTTP responses?

-  You don’t want to have to translate the on-the-wire representation of an entity into another form just to store it in a database. 
-  You want to build your solution quickly, and don’t want to have to add lots of additional libraries or code to manipulate a database. 

You don’t want to have to translate the on-the-wire representation of an entity into another form just to store it in a database.

You want to build your solution quickly, and don’t want to have to add lots of additional libraries or code to manipulate a database.

Store your JSON documents in a data store that is designed to quickly and efficiently retrieve and store JSON documents – a Document Store.

In a microservice, the requests and responses that for the values accepted and returned by the microservice are most commonly presented as JSON values. JSON has emerged as the lingua-franca of REST, quickly eclipsing and replacing XML for most service schemas. When a resource is thus represented naturally by a JSON document developers want to use the most efficient database they can for storing and retrieving that information. Increasingly, this choice is a Document Store such as MongoDB or CouchDB.

For instance, in MongoDB the basic construct is a collection of JSON documents – and adding a JSON document to that collection is as simple as obtaining a reference to the collection and calling an add method. This type of simplicity is what makes this approach attractive to developers building microservices in Javascript.


################################################################################


  Global Configs 
=================

Context: configuring services

Problem: how do I configure services that potentially run across multiple datacenters?

Solution: use a global configuration store, for example a Consul or Zookeeper cluster spread out over all data centers.


################################################################################


  Global Locks 
===============

There are various circumstances where work needs to be coordinated between datacenters. For instance, Synchronous Replication in an otherwise transaction-less key/value store could result in write conflicts; another example is where a single process should work on data and the process should only be started in (potentially) a different datacenter when the original one goes down. Coordination can also mean turning an asynchronous system into a synchronous one by waiting until the replication has happened; preferably, other systems shouldn’t be writing at that time.

Therefore,

Have a system to implement locking across datacenters. With these global locks, it becomes possible to coordinate systems and actions between datacenters. A transaction can be scoped by taking a lock; writes to otherwise transaction-less systems can then be serialized on the lock. A single process to do work can be elected similarly: one process holds a long-running lock, and when it fails, the lock is released and other processes then have a chance to obtain it and start performing work.

Like synchronous replication, locking across datacenters is expensive and not transparent to the locking process. Locks can fail, they take some time to obtain as they must be coordinated with instances of the locking system running in multiple datacenters, and the name or key to lock on must be chosen carefully to balance between locking contention (lock names too coarse grained) and risk of conflicts (lock names too fine grained). It’s a tool in the distributed system designer’s toolbox, but not one that should be retrieved at the first hint that it might solve a problem.

Zookeeper is a distributed consensus system originally implemented by YaHoo! to coordinate their massive (for that time) networks. YaHoo! wrote the system to perform leader election, locking, and to act as a service registry. It has been around for a long time and can be considered battle-hardened, and it performs very well as a global locking service.

One way to run Zookeeper is to have 5 nodes over Three Datacenters. Zookeeper does not benefit from more machines (as the leader of the cluster is the bottleneck), and in this way the outage of any datacenter will leave a quorum intact; furthermore, two machines in different datacenters can go down with no impact. This is a general pattern for data replication that is applicable to replicated systems like Cassandra and Kafka as well; contrary to datacenter outages, machine outages are common, and with a 3 node setup a single node failure would immediately put the system at risk of not being available.

Interacting with Zookeeper is quite low level and requires a lot of knowledge on how the various primitives are implemented; in essence, it is more a toolbox than an end-user application. Various users have published “recipes” for interacting with Zookeeper in library form, and their use is recommended– Apache Curator is a leading software component in this field.


################################################################################


  Key Value Store 
==================

You are building an application with a Microservices Architecture.

How do you store data that is naturally accessed through a simple key lookup such as cache entries?

-  You want to use the simplest tool for the job at hand. 
-  You want to make your accesses (Cache lookups or other accesses) as fast as possible. 

You want to use the simplest tool for the job at hand.

You want to make your accesses (Cache lookups or other accesses) as fast as possible.

Store your data in a scalable Key-Value Store. The principal advantage of a key value store over other types of Distributed Store is its simplicity. Most Key-Value Stores act, in principle, like a hash map.

For example, Redis has simple GET and SET commands to store and retrieve string values. What’s more, for simple key lookup operations, Redis offers O(1) performance.

If, on the other hand, you used a more complex store type such as a Document Store for storing cache entries, then you would find that the performance of such solutions is often not as good. That is because other distributed store types optimize for more complex cases such as searching by the contents of the documents stored.

There is no magic in using a Key-Value store. In many cases such as EhCache, they are in-memory stores, and as such only can provide Consistency and Availability but not Partition tolerance (as per Brewer’s CAP Theorem). This limits the size of the cache. The mechanism used by many others (e.g. Redis with clusters) is to map keys to specific distributed nodes, and to route requests to the appropriate server, which then stores that corresponding set of values in memory. In order to maintain Availability, this means that you must have copies (slaves) of each node. This means that by the CAP theorem you can gain Partition tolerance, but at the potential loss of Consistency while the master synchronizes with the slave.


################################################################################

You are building applications using a Microservices Architecture.

How do you encourage teams to use the most optimized storage mechanism for the particular data structures, query and update requirements that each microservice requires?

Teams building microservices are in an interesting and unusual position. At one end, microservices themselves are about independence - independence to release new features on their own cadence, independence to change their internal implementation without affecting clients or requiring them to change, and independence to make their own decisions about clustering and HA.

At the same time, teams are never entirely independent. Even within a team, people want to follow common practices and standards in order to make their code understood and maintainable by other team members. When a team is part of a larger organization, the desire to follow common practices and standards becomes even more important, as managers are concerned about the ability to supplement or replace team members that leave, become ill, or are otherwise unavailable.

The problem is that many organizations go too far in the commonality direction, especially as regards persistence. Traditional development approaches often featured a shared data repository, usually a relational database. Once a team heads down that path, it becomes difficult to leave, as the investment in terms of skills, licenses, management and especially the data itself grows every larger.

But microservices offer a different option. Just as moving from a monolith to separate microservices allow teams to focus only on the specific business problem that each microservice is designed to solve when working with that microservice, the same principle of tighter focus allows teams to think more closely about the management and representation of the data they are persisting.

Therefore,

Apply Polyglot Persistence. Let each team choose their own persistence mechanism (perhaps from a pre-selected set) that is appropriate to their particular business problem and implementation challenges.

Not every problem is suited to the same type of persistence mechanism. Documents are not searchable when they are stored in Key-Value stores. Relational Databases are not particularly well suited to the problem of storing unstructured or loosely-structured data. Polyglot Persistence addresses this problem and allows teams to make their own decisions.

However, Polyglot persistence does not solve all problems - it creates a few new ones as well. In particular, especially when in a refactoring situation applying the Strangler Pattern you may find that synchronizing your data between different types of store becomes more challenging. In cases like this you may need to employ an Event-based solution using an Event Backbone or a Data Virtualization approach that makes non-relational stores look like relational stores to external clients.


################################################################################


  Scalable Store 
=================

You are developing an application using Doimain Microservices. However, your application will need persistent state to represent current and previous user interaction.

How do you represent persistent state in an application?

-  Storing application data inside the memory of an application runtime leads you to require routing solutions like sticky sessions that make building highly available systems difficult. 
-  You don’t want to limit the scaling of your application by forcing the application to store state inside a data store that cannot scale linearly. 

Storing application data inside the memory of an application runtime leads you to require routing solutions like sticky sessions that make building highly available systems difficult.

You don’t want to limit the scaling of your application by forcing the application to store state inside a data store that cannot scale linearly.

Put all state in a Scalable Store where it can be available to and shared by any number of application runtimes.

The key here is that the database must be naturally distributed and able both to scale horizontally and to survive the failure of a database node. For the last several years, that has driven developers to the concept of “NoSQL” databases as described in [Sadalage]. Examples of this type of database include Redis, Cloudant, Memcached.

However, quite recently a number of new distributed databases based on the relational model and collectively called “NewSQL” have also become available. These include options like Clustrix, MemSQL, or even MySQL cluster. NoSQL databases are typically less efficient at SQL-like queries because of differences in approaches to query optimization. So, if your application depends on SQL-centric complex query capability then a solution such as a NewSQL database or a distributed in-memory SQL database may be more efficient.

In the end it does not matter which particular database you choose – so long as you choose the right database structure and retrieval mechanism for the job you are trying to perform. If the driving forces in your application are scalability and redundancy either a NoSQL or NewSQL database would suffice, but other application requirements will determine the particular type of database you must choose.

Result Caches and Page Caches are usually implemented with a Scalable Store.

If your Scalable Store does not fully implement Synchronization across multiple Datacenters (e.g. Three Datacenters) you may need to implement Global Locks.


################################################################################


  Synchronous Replication 
==========================

Some data is important enough not to lose. It can represent a transaction, or data that the system cannot recreate (because it originates externally, for example it came in through an API). Often, the data carries a monetary value or an external obligation.

How do you guarantee that data is never lost when building a distributed cloud-based system?

Outages of any kind come with losses. Preventing or reducing the cost of these losses costs money, so these measures can be seen as an insurance premium. It is important to make a quick calculation of costs of loss and benefits of prevention of loss, but sometimes the balance will be that data cannot be lost even under very rare circumstances. This means that transactions need to span multiple datacenters to insure against losing data that is in-flight in a datacenter when it goes down.

Therefore,

Replicate data between datacenters synchronously when loss of data is deemed costly. Synchronous replication can severely impact the responsiveness of systems, but is an essential tool. In effect, the transaction should not complete until the data has landed in a quorum subset of the systems involved. Doing quorum writes obviously reduces availability, so whenever synchronous replication is involved there is a need to make sure that transactions are re-tried.

Synchronous replication is a measure of last resort. It is expensive, slows systems down, has the risk of reduced availability, and forces applications to be aware of all that. An application that uses a local MySQL master cluster that asynchronously replicates to another data center can be blissfully unaware of that happening–for all practical purposes, the database behaves like this replication isn’t happening and therefore behaves like a single local instance. When synchronous replication becomes part of the picture, however, a single transaction will become much slower (because of the round trip times involved), has a higher risk of failing (because of network connectivity issues), and thus the application should be written to take this into account. Often, attempts to hide this complexity in a library fail as business logic needs to be adapted to handle new kinds of failures; this increase in business logic complexity puts a premium on synchronous replication.

If there is any way that the system can get away with asynchronous replication, that should be the preference. The cost of losing a couple of in-flight transactions in a once-a-year event should be clearly weighted against the overhead of adding synchronous replication.

For example, Cassandra is an elastic Key-Value Store that has the interesting property of being datacenter aware. Cassandra nodes can be tagged with a datacenter (or even a rack) name, and the system can be instructed to replicate data across datacenters. With strong write consistency, Cassandra won’t complete a write operation until at least a quorum of all datacenters have reported back that the write has been written to disk. In this sense, one can be safe that when Cassandra reports back that a write was successful, the data has been durably stored in multiple datacenters.

One drawback of Cassandra is that writes are atomic operations, but there is no locking. This means that conflicting writes are possible. There are ways to lessen the impact of conflicting writes which are out of scope of this pattern as they are very Cassandra-specific, but generally speaking conflicting writes are bad and should be avoided.

The way to avoid conflicting writes is to serialize them. A good way to do this is to use Global Locks around the operation. The lock assures serialized writes; a read-before-write in the lock will detect write conflicts, and Cassandra will assure replication across datacenters.


################################################################################


  Table Store 
==============

You are building a cloud-native application, particularly one following the Microservices Architecture. You may be either creating a new, greenfield application or following Cloud Refactoring principles to refactor an existing application.

How do I represent data that is going to be queried arbitrarily and updated frequently?

Some applications go beyond basic “CRUD” operations in that they require the ability to arbitrarily query on data. This is particularly true of applications that have a reporting element, especially in cases where the reports change frequently. What’s more, many applications change their data frequently - there are hundreds, if not thousands of updates made every second.

Likewise many existing applications are already using a table format for their data. If the level of pain caused by ORM is low (for instance, the application is a standard CRUD application and is using a simple ORM like Hibernate) then there is no pressing reason to force a different data representation.

However, on the other hand, there are many applications in which the level of pain caused by ORM is “high”. In those cases, the downsides of the table model may outweigh the benefits. In these cases, the only reason that teams often choose a relational databases are because of technology inertia (e.g. it’s what they know) or corporate or organizational guidelines.

Therefore,

Choose a “classical” RDBMS when the data is naturally table-oriented, most columns are usually needed for operations, and the data has a heavy use of inter-table relations.

There are literally dozens of examples of relational stores, running from classical databases like Oracle and DB2 to more recent open-source systems like MySQL and PostgresDB.


################################################################################


  Container DevOps Introduction 
================================

This part of our pattern language is concerned with the problems inherent in building and delivering software using Containers, particularly those issues that arise during the process of mapping docker images and containers into the stages of a software development lifecycle. The language assumes that the reader will be building applications following an agile approach that is characterized by Continuous Integration/Continuous Delivery

Containers are one of the most rapidly adopted software technologies of the last several years, with extraordinary growth in adoption (see PortworxSurvey). This rapid adoption is the result of an impressive increase in developer productivity and ability to delivery cost reduction resulting from container adoption (see Synopsis). Containers are the instantiation of immutable images that describe a process that will run in its own virtualized memory and processor space.

A registry is a service for storing and retrieving Container images. You can think of it as being like a source-code control system (e.g. Git) for images. There are two general types of registries; a public registry is one that provides this service to many customers where the images are publicly available and searchable. Examples of this include Docker Hub, the Amazon Elastic Container Registry, and the IBM Container Registry service. A private registry is one that serves a single customer. Both types of registry may be cloud hosted, although private registries are sometimes also deployed on premises. For instance, in Docker you can deploy your own registry services and store your images locally or in any other location running docker (such as a hosted private cloud).

A repository is a collection of related images that have unique tags. A tag is an alphanumeric identifier for an image within a repository. For instance, Docker Hub allows you to create new repositories via the “Create Repository” function. This named repository then becomes a common name that is used as part of the identifier of images within a docker push or docker pull, e.g. docker push user/repository-name:tag. Other registries also support similar approaches to creating repositories.


  Patterns in this section 
===========================

- Container Build Pipeline is the root pattern of this section of the pattern language. A DevOps pipeline is a core concept for Continuous Integration/Continuous Delivery. An issue many teams face is where to introduce docker into their delivery processes. Starting with an automated delivery pipeline for building and deploying your container images leads to the other patterns in this section.
- Pipeline Vulnerability Scanner enables you to perform static vulnerability scans as a stage within your Container Build Pipeline in order to scan your container image(s) for any known vulnerabilities and stop the deployment and report the issue if any issues are found.
- Registry Vulnerability Scanner gives you the ability to scan images after they are built, so that new vulnerabilities that are detected after a build can be detected and addressed.
- Multiple Vulnerability Scanners addresses the issue that different vulnerability scanners use different approaches and pull threats and malware definitions from different repositories. Teams should hedge their bets by scanning images in multiple ways.
- Birthing Pool is a way to avoid placing an untested image into an environment shared with other development stages, allowing malware present in that image to affect those other stages.
- Public Image Registry is a solution for making images available to others who may be outside of your development organization.
- Private Image Registry is a solution for making images available to those within your organization, particularly useful in cases of intellectual property restriction or security restriction.
- Approved Image Repository is the location for approved images once they have been through the scanning and vetting process.
- HA Container Registry is important because a registry is only useful when it can be accessed. Encountering a single point of failure on docker host startup will result in your entire container architecture being unavailable.
- Public Registry Proxy is a way of improving performance of image pulls in some use cases by locally caching images nearer to the container hosts.

The relationships between the patterns in this section can be found in the diagram below:




Table of contents
=================

-  Approved Image Repository 
-  Birthing Pool 
-  Build Pipeline 
-  Cloud Hosted Image Registry 
-  Container Registry 
-  Multiple Vulnerability Scanners 
-  Pipeline Vulnerability Scanner 
-  Private Image Registry 
-  Public Image Registry 
-  Public Registry Proxy 
-  Registry Scanner 
-  Single Image Registry 


################################################################################


  Approved Image Repository 
============================

You have developed a new internal application that will run on containers. The solution has dependencies on third party images hosted in a Public Image Registry. In general, developers should not have to worry about the use of third party images. However, there is a balance that needs to be struck between ease of use and security.

How do you prevent random images, or multiple versions of existing images, from being pulled from a Public Image Registry making your overall Docker installation estate more vulnerable and harder to maintain and operate?

In general, you want to give developers the freedom to use open-source images from the Internet. However, this freedom comes at a cost. For instance, a complete lack of governance could mean that you have multiple versions of an operating system across many applications - all of which would need to be tracked, maintained and patched.

Licensing is also a potential issue. An unapproved image could be used which could create an open-source licensing issue for your application. Likewise, an unapproved image could be used which could create a commercial licensing issue for your organization.

Maintenance of unapproved images can also cause a potential problem. A team may inadvertently create a dependency on an ungoverned image from an amateur programmer that may not maintain it, or an ungoverned image could disappear and then cause your application to fail.

Finally, security is an issue; an ungoverned image dependency could be modified and have malware installed on it.

Therefore:

Create an Approved Public Image Repository within your Private Image Registry where you have pre-downloaded all images and versions that are approved to be used in your estate. You should lock down your container hosts so that they cannot pull images directly from Public Image Registries and must come through your Private Image Registry. Only approved image registry administrators should be able to add images to the repository.

It is critically important that images should be run through a Pipeline Vulnerability Scanner (in fact, they should be run through Multiple Vulnerability Scanners) before they would be added to the Approved Public Image Repository. That ensures that developers can begin from a starting point that is known to be secure.

The governance of the Approved Public Image Repository can be configured at multiple levels including:

- Organization
- Program
- Project
- Application

Many Private Image Registry solutions possess the ability to configure such patterns and rules as part of the product.

Using an Approved Public Image Repository increases the security and governance of your solution while giving developers the freedom to use the best of breed tools and open-source images.


################################################################################


  Birthing Pool 
================

You have a new cloud native application that is based on a container technology (such as Docker) and you want to ensure that container images with known vulnerabilities are not deployed to your environment. However, some vulnerabilities within a container image cannot be picked up by a static vulnerability scanner as they can only be found in a running container.

How do you detect such vulnerabilities without deploying a potentially vulnerable image and into a test or production environment?

Vulnerabilities present in a running container should be isolated to a micro-segmented network where the impact cannot be replicated to other machines. However, if we place an untested image into an environment shared with other development stages, then malware present in that image may affect those other stages.

Therefore,

Create a new environment as part of your overall CI/CD process consisting of an isolated environment called a birthing pool. Run dynamic vulnerability scans within this environment in order to limit the exposure of other Docker runtime environments to potential malware.

Up to this point, we have been considering that the isolation provided by Docker itself, in that each image is functionally isolated from other images by the container execution environment is adequate for all types of vulnerabilities that may be found in an image. However, that may not be true. There is the possibility that a side-channel or container infrastructure attack (such as forkbomb, see Baset) may interfere with the operation of your container execution environment and thus affect other Docker images. Thus the need exists to have at least two separate container execution environments, each segmented from the other, in order to eliminate this possibility.

An example of all of the different parts of an end-to-end container development process, including a separate environment for active vulnerability scanning within a birthing pool, is shown in Figure 3: Stages including Birthing Pool.



Figure 3: Stages including Birthing Pool

You should be able to create the new birthing pool environment from scratch using infrastructure as code techniques and tools, such as Terraform. The good news about this approach is that since we are creating an uncustomized, “off-the-shelf” installation of a common runtime such as Kubernetes or Docker Enterprise, that this can be done without introducing the kind of configuration drift caused by team-level specialization we discussed earlier. The birthing pool should contain no sensitive data. This allows you to run your container within the birthing pool and allow you to run a vulnerability scan in this isolated environment and detect any vulnerabilities that can only be found through dynamic scans. The introduction of the birthing pool means that a dynamic scan can be performed without exposing other aspects of the application estate to the vulnerability. One potential implementation of this, using separate Kubernetes clusters for each different environment in Figure 4: Isolation of the Birthing Pool.



Figure 4: Isolation of the Birthing Pool

Images should pass through a Birthing Pool before they are placed by a Container Build Pipeline into an Approved Image Repository.


################################################################################


  Container Build Pipeline 
===========================

When using containers as a tool for deployment, a development team faces a series of choices. Container technologies like docker are often adopted by teams that are in the process of evolving their practices away from more traditional development approaches toward more agile approaches. As such, these teams often have established build processes that they need to replace, or processes they should retarget to take advantage of the new capabilities that containers offer. At the same time, the opportunity presented by adopting containers also means that teams can modernize their development, deployment and testing processes to take advantage of new tools such as Jenkins or Hudson that introduce more modern approaches.

How can development teams exploit the capabilities provided by containers and adapt their build and testing processes to take full advantage of those capabilities?

Containers introduces several concepts that make it challenging for teams to directly adapt existing practices directly to functioning with them. One of the most challenging for teams that are used to traditional approaches is the concept that Docker containers, especially those in production, should be immutable. Immutable production systems are not a new concept in software engineering - in fact, a big part of the attraction of infrastructure-as-code was the idea that systems could be made immutable if only they could be constructed entirely from the ground up from a repeatable code base instead of having to be constructed from an ad-hoc mixture of code and far-too-mutable physical or virtual infrastructure.

However, the infrastructure-as-code approach was often not adopted entirely throughout an entire software development lifecycle. In more traditional software development approaches, an environment will be an entire system, often built as one or more Virtual Machines or physical environments, that serve as a location in which one particular step in a software engineering lifecycle, such as application build, integration testing, or acceptance testing will take place. What we have commonly seen is that development teams will have a distinction between “lower” environments in the development lifecycle, such as build and unit test, which are very open and mutable, and “higher” environments such as an acceptance test environment, that become progressively more locked down.

What this leads to is a situation where lower environments are often constantly “in flux” and that changes in configuration are not picked up in later environments, causing problems that are fixed in earlier environments to recur unexpectedly later. What’s more, the inconsistency of the mechanisms for defining and configuring environments results in wasted time and needless repetition of work.

Therefore,

Build a Continuous Integration and Continuous Delivery Pipeline, using common tools such as Jenkins, in which the output of each pipeline run will be an immutable Container image.

Jenkins is an open source tool that is used throughout the software development industry to define and build Continuous Integration and Delivery pipelines. Jenkins is built on the concept of a stage, which is a conceptually distinct subset of a pipeline. Each stage is built of steps that can execute within conditional logic to automate common tasks such as building a Java Jar file using Maven, or running unit tests with an automated tool like JUnit. Thus each stage can conceptually map to a physical or virtual environment of the type described above such as “Build” or “Unit Test”.

The key here is that you can use a tool like Jenkins combined with Containers to entirely eliminate the need for any of these unique physical or virtual environments. Instead you will build an image from a dockerfile in the initial setup of the pipeline, and then push this image to the image registry upon successful completion of the pipeline stages. The image is entirely rebuilt on each new run of the pipeline.

This concept is central to the approach being used by more modern Kubernetes-centric DevOps Pipeline tools like JenkinsX and Tekton. Tekton is an open-source framework for building CI/CD tools and processes. The steps (and constituent tasks) of a pipeline in Tekton all run as Pods in Kubernetes. JenkinsX is built on Tekton and gives you full GitOps and pipeline automation implementation. However, the output of these is always an image that has been constructed through the steps in the pipeline.

This approach will fix the problem of reintroducing errors into later environments by entirely removing manual configuration changes from the process. In this approach you can’t change the configuration of an image either intentionally or accidently within a single stage – you have to introduce any configuration changes into the Container build process at the very beginning and then let the changes propagate through the entire build pipeline. So for instance, let’s consider the simple case of changing the version of a Java runtime environment (JRE). In a traditional approach, with separate physical or virtual machines for each development lifecycle stage, updating this configuration would require changing each environment separately, either manually or through a scripted infrastructure-as-code tool such as Chef or Puppet. In the Container approach, you would change the dockerfile once to include the new definition, and then re-run the pipeline to repeat all the automated steps from the beginning – creating a new, immutable image at the end.

This pattern is well established as a best practice within the Docker community. For instance, the Docker documentation article on Development Pipelines describes a recommended development pipeline very much in line with the recommendations of this pattern. Likewise the IBM article on Kubernetes DevOps is just one of several examples of such pipelines being built for container projects.

At the heart of this pipeline will be the problem of dealing with images appropriately. The first issue to consider with publicly hosted images is that since they are coming from a public repository that they could, potentially, contain malware or other issues that would introduce vulnerability into your system. Thus the need for Pipeline Vulnerability Scanning becomes absolutely critical. This results in the need to introduce special stages. such as a Birthing Pool into your Pipeline in order to make sure that you are into introducing new types of vulnerabilities into your systems.


################################################################################

Title: Cloud Hosted Image Registry

Context:

You have developed a new internal application that will run on containers and you need to host the image in a container repository so that the container technology (such as docker) can pull and run the image on your container environment.

Problem:

You are unsure if you should use a Cloud Hosted Image Registry or whether you should create your own Private Image Registry within your PAAS or IAAS environment.

Forces:

- You don’t have any contractual SLA’s where you must guarantee the availability of your overall solution
- There are no restrictions for legal reasons on the location for hosting of the container images
- There are no security or sensitivities on hosting the container images outside of your environment
- Hosting your own registry increases operational maintenance (Backup, Restore, Patching)
- Hosting your own registry increases infrastructure estate (volumes, firewalls, load balancers)
- Hosting your own registry increases testing (availability, DR, performance)
- Creating additional infrastructure increases time to market, cost and project timelines
- Additional Infrastructure and application estate opens up a wider threat surface area,

Solution:

You should host your container images in a Cloud Hosted Image Registry (such as IBM Cloud, docker cloud, google, quay) rather than attempting to roll your own solution. You should only roll your own image if there is somer sort of NFR that prevents you from using a cloud service.

Results:

Using a Cloud Hosted Image Registry speeds up your time to market and reduces your operational spend by focusing on your application rather than focusing on scaffolding.


################################################################################


  HA Container Registry 
========================

You have an Service Level Agreement (SLA) for your cloud application (such as 99.99 availability) and you need to ensure that you meet your SLA’s and that you have a fast time to recovery.

The issue is that in the cold start of a container host, the local cache of the host’s image registry will be empty and will require the host to refetch any images from the container registry. This situation is even more likely in a disaster recovery scenario (as it’s likely that hosts will have been restarted).

How do you ensure that your container startup will not fail at the worst possible time by having them rely on an unreliable image registry?

The SLA on the overall solution will dictate how much effort you need to put into the availability of your registry solution. A key factor in that is the pull time of an image from a container registry; for instance if the average pull time of an image is significantly greater than the Return To Operations (RTO) objective of your registry, then you will not notice disruptions lasting less than the RTO. A potential complication to determining how well you can meet your overall solution SLA is that a Container Registry as a Service provider may or may not have it’s own SLA

Therefore:

Ensure that the container registry has high availability (e.g. has been redundantly deployed in a multi-region, multi-availability zone way) with a matching SLA.

This is a best practice that has been documented for Docker in several places such as Labourdy. Many companies advertise commercial container registries (such as Portworx) that are highly available, but it is rarely stated why you should care that this is so. You should also ensure that you are using a Public Registry Proxy in your container registry to ensure fast fetches of 3rd Party Images. You should also ensure that your cloud provider (or if managed by your client organisation) is able to meet the SLA’s that you are committing to. If the container registry is dead and your cache is cleared then you’ve just lost that data center.

Ideally you should follow practices of bulk-head isolation and ensure that you have isolated and affinitized registries to your regions. In that way, if you lose your registry then it’s only lost for a single region, not for every region. Of course, the affinity should only extend to those cases where it is required for performance reasons; if you can fetch an image from a distant registry when your local registry is down and that can still be done within your overall solution SLA, that is a valid option.

Using a High Availability Registry (with matching SLA) for a container registry (including a Public Registry Proxy) will ensure that the Image registry remains available even in a disaster scenario. Likewise you will experience increased availability of the image registry, as there will only be a single dependency instead of depending upon multiple providers, each with different SLA’s.


################################################################################


  Multiple Vulnerability Scanners 
==================================

You are developing a new application that is hosted in a container and you want to ensure that container images with known vulnerabilities are not deployed to your environment.

Not all vulnerability scanners use the same vulnerability databases and some are more complete than others. How do we ensure we don’t rely on a single source of vulnerability information for discovering and reporting issues?

Vulnerability Scanners come from different vendors and open source teams and as a result, different vulnerability scanners will be updated at different rates. Likewise, different vulnerability scanners use different databases and a specific vulnerability may not be reported in all databases. It is also possible that any particular vulnerability scanner may fail to detect a vulnerability, either because of bugs or because a malware author can attack a vulnerability scanner itself.

Therefore:

Use Multiple Pipeline Vulnerability Scanners such as Clair and the IBM Vulnerability Advisor at multiple points in your pipeline.

The best pattern in such a scenario would be to use an open-source scanner such as Clair as part of your CI/CD pipeline and use a different vulnerability scanner for your image registry.

Applying multiple vulnerability scanners to address deficiencies or blind spots in a single scanner or database is a well-known best practice in the security industry documented in SoftwareSecured. Using multiple vulnerability scanners within your Docker Build Pipeline gives you an increased level of security as there is a reduced risk of vulnerability not being found due a single scanner not being aware of the vulnerability.


################################################################################


  Pipeline Vulnerability Scanner 
=================================

You are building a new cloud native application that is hosted in a container. You want to use container images that are obtained from existing public registries in order to take advantage of the work of others in the Docker community. However, you also want to ensure that container images with known vulnerabilities are not deployed to the image registry from the CI/CD Pipeline.

How do you prevent Docker image with known vulnerabilities from being uploaded into your image registry in the first place?

There are several issues that are brought up by the use of an image registry. First of all, you don’t want your Public Image Registry or Private Image Registry to contain vulnerable images; you want issues to be sorted out before the image is deployed to the registry. You don’t want to place the responsibility entirely on the developer; you want them on focusing on writing code. Likewise, you don’t want developers to be forced to go through a lengthy and arduous security review.

Therefore,

Ensure that your CI/CD pipeline has a vulnerability scanner included in one or more of the stages in your pipeline. The scanner will scan your container image for any known vulnerabilities and stop the deployment and report the issue if any issues are found. The static vulnerability scan should check public vulnerability databases such as CVE at a minimum.

The types of scanners that we are referring to are often referred to as static scanners in that what they do is examine the configuration of a Docker image in looking for known vulnerabilities to operating system, language runtime or middleware. This is different from an active scanner, which examines the activity of a running docker image for known malware behavior. An example of the overall build process (a potential model for a Docker build pipeline) including a pipeline vulnerability scan is shown in Figure 2: Docker Build Process.

 Figure 2: Docker Build Process

Let’s look at Figure 2: Docker Build Process in detail. At the beginning of the process, you have a number of different input triggers that can result in the need to update an image – in all cases, what this amounts to is changing the dockerfile to introduce a configuration change, or updating the application code executing within the dockerfile that will be built as part of the CI/CID process. When you do so, this acts as a trigger to the CI/CD build pipeline – in which one step in the pipeline will be a vulnerability scan. That scan must complete successfully before the image can be published to a temporary registry in preparation for running dynamic scans (see Birthing Pool).

However, it’s not enough to simply include a vulnerability scan as part of a DevOps pipeline, you also need to alert your DevOps, Site Reliability Engineering or run teams of any vulnerability scans as and when they occur. Especially as vulnerability databases are constantly updated, it may be true that an image that has passed an earlier scan and already been deployed to production may fail a later scan in an earlier stage against an updated database.

You want your images to be scanned regularly; not just when code changes, in case a new vulnerability is discovered post-release. Therefore it is important to have triggers such as notifications from your security team of new vulnerabilities that can begin a new pipeline run. Likewise, you don’t want to purely dependent on the vulnerability scanner of your image registry.

Open-Source scanners such as Clair can provide such functionality and can be easily integrated with CI/CD pipeline tools such as Jenkins. There are multiple other examples of such tools, such as the open source Docker Bench Security and the commercial IBM Vulnerability Advisor.

Using Vulnerability Scanners is another well-established practice in the Docker development community, cited specifically as a best practice in 7 Threats and Sumologic.

Using a Pipeline Vulnerability Scanner integrated into your CI/CD pipeline increases the overall security of your solution by ensuring that your image is secured from known vulnerabilities and prevents vulnerable images being uploaded into your Public Image Registry or Private Image Registry in the first place.

You should also have a policy that prevents any unsecured images being deployed into production. Only allowing teams to only pull from an Approved Image Repository is one way of doing this.


################################################################################


  Private Image Registry 
=========================

You have developed a new internal application that will run on containers and you need to host the image in a container image registry so that the container technology (such as Docker) can pull and run the image on your container environment.

How do you gain the benefits of a Public Image Registry (such as Docker Hub) without making all of your images available to the general public?

There are several reasons why you may not be able to use a Public Image Registry. As noted in that pattern, your image may have to contain private information such as license keys or internal network structures. In this case, making your image available in the public would expose your architecture and make it more likely that your system will be hacked. Likewise, you may not want to reveal IP or secrets; if it is made available to the public, there is always a risk your image could be reverse engineered.

Therefore:

Host your container images in a Private Image Registry such as Nexus, or a private registry of your cloud platform. The Private Image Registry supports all the same protocols and behaves exactly like a Public Image Registry. This means that you have to configure your container servers to pull from the URI of the Private Image Registry instead of the default Public Image Registry (e.g. Docker Hub).

Using a Private Image Registry gives you all the benefits of a public registry but keeps your container images private and secured from users or applications that you should not have access to it.

This pattern has been described as a best practice by many different organizations, such as BobCares CenterDevice and Macadamian.


################################################################################


  Public Image Registry 
========================

You have developed a new container image that you wish to make available to the wider development community. This may be a new container image you wish to make available to your customers, a new open-source application that you wish to make available, or an improved installation of an existing open-source product that you wish to contribute back to the community.

In each of these cases, you should assume that there is no IP restriction and that you wish to make the image available publicly. Likewise, there should be no commercial or licensing implications to distributing the image.

How do you make all supported versions of your image available to the general public in a manner that allows you to easily fix bugs or vulnerabilities, distribute new versions and simplify installation?

There are several reasons that lead developers to want to use Docker images in the first place. For instance, many commercial development teams have high support costs because customers incorrectly install software installations due to complex instructions.

Likewise, development teams wish to be able to quickly and reactively provide frequent patches to existing versions with minimal impact. Critical to this is the ability to distribute new versions of your application and make them available quickly and securely.

Therefore:

Publish all supported versions of your image, correctly tagged with the right version, to a Public Image Registry such as Docker Hub. By centralizing the image distribution you are able to harden the image, fix images, fix bugs and then provide an updated version of the image for your application as you fix issues.

If you need to release a new version of your application then you are able to tag the new version of your image and make it available immediately allowing consumers to choose when they should use the new version of your application.

Consumers of your image will be able to just pull the version of the image that they need instantly without having to perform any complex installations.

Publishing images to a Public Image Registry allows you to centralize the distribution of your application in a secure manner and allows you to provide frequent releases of your application or image in a secure fashion for those images that can publicly distributed. However, the benefits of a Public Image Registry can only be realized if it is available, thus the need for an HA Container Registry.

When publishing images to a public registry you should ensure that in your published image that you don’t break any licensing concerns by using unlicensed software or IP, and that you do not include any sensitive data such as Keys, Passwords, Infrastructure information such as internal IP’s in your images. Any requirement to include these types of information in your images will instead require you to use a Private Image Registry instead.

Likewise, you will want to use a Pipeline Vulnerability Scanner to ensure you don’t pass vulnerabilities to consumers of your images.


################################################################################


  Public Registry Proxy 
========================

You have developed a new internal application that will run on containers. The solution has dependencies on third party images hosted in a Public Image Registry.

How should you handle access to images in Public Image Registries when you are concerned about the overall performance, availability and the overall attack surface of your estate?

Although you have no issue with the usage of the third party images so long as they are approved for usage, pulling these images from a Public Image Registry can have several issues. First, many images are quite large and take significant time to pull. Second, there are SLA’s on the overall solution including the CI/CD pipeline. Finally, there are SLA’s on the time to recover in a disaster recover scenario that must also be met.

In addition, in many situations, Security is paramount and locking down external access for container hosts is important. In these situations, Auditing and governance of which container images (and versions) are commonly used across the estate

Therefore:

Configure your Private Image Registry to proxy all images from the public repositories rather than allowing the container hosts to pull the images directly from the Public Image Registry.

Using a Public Registry Proxy will increase the overall availability of your solution as all container images are cached locally on your container registry meaning that if you need to pull your container (such as a restart of the container host) then the local server cache does not need to pull the image from the Public Image Registry. In addition this increases the security of your environment as access to the outside world can be locked down (i.e. each server will not require a connection to the public hub). Finally this will increase the overall availability of the solution, as there are fewer dependencies on external clouds.


################################################################################


  Registry Vulnerability Scanner 
=================================

You have a new cloud native application that is based on a container technology (such as docker) and you want to ensure that your image is clean and secured from any vulnerabilities such as malware or known security vulnerabilities. In order to prevent vulnerabilties being deployed into your Container Registry you utilize a Pipeline Vulnerability Scanner to check for vulnerabilities on creation of the image during the CI/CD pipeline.

What if a vulnerability is created post deployment of the image into the Container Registry? How do you ensure that vulnerabilities are detected?

You don’t want to only rely on the Pipeline Vulnerability Scanner to pick up all vulnerabilities in-case the CI/CD pipeline is bypassed and a developer uploads an image directly. Likewise, you don’t want scans to be performed manually but want them to be scanned periodically. You want an image to be scanned not just at a build time just in-case a new vulnerability is discovered post-release

Therefore:

Ensure that your image registry has a vulnerability scanner that will both scan your container image for any known vulnerabilities on upload of your image and at a regular intervals. The vulnerability scan should check public vulnerability databases such as CVE at a minimum.

Using a vulnerability scanner integrated with your image repository increases the overall security of your solution by ensuring that your image is secured from known vulnerabilties.

You should also have a policy that:

- prevents any unsecured images being deployed into production
- alerts your devops, SRE or run teams of any vulnerability scans as and when they occur
- provides a scan report at regular intervals that can be reviewed by your devops, SRE or run teams

Cloud Hosted Image Registries such as Dockerhub, IBM Cloud, and quay.io all contain this capability by default.


################################################################################

Title: Single Image Registry

Context:

You have developed a new internal application that will run on containers. The solution has dependencies on third party images hosted in a Cloud Hosted Registry.

Problem:

Although you have no issue with the usage of the third party images (they are approved images for usage), you are concerned about the overall performance, availability and the increased attack surface of your estate by creating a depenency on several Public Image Registries where there are no SLA’s. How should you handle access to these images?

Forces:

- The container images are quite large and take significant time to pull
- There are SLA’s on the overall solution including the CI/CD pipeline
- There are SLA’s on the time to recover in a disaster recover scenario
- Security is paramount and locking down external access for container hosts is important
- Auditing and governance of which container images (and versions) are used across the estate

Solution:

You should ensure that the applications within your estate only pull from a single registry rather than multiple registries.

This can be achieved through the following methods:

- Using a Public Registry Proxy as part of your Image Registry
- Using an Approved Public Image Repository as part of your Image Registry

Results:

Using a single image registry will increase the overall availability of your solution as all container images are stored locally on your container registry meaning that if you need to pull your container (such as a restart of the container host) then the local server cache does not need to pull the image from other registries. In addition this increases the security of your environment as access to the outside world can be locked down (i.e. each server will not require a connection to the public hub). Finally this will increase the overall availability of the solution as there are less dependencies on external clouds.


################################################################################


  Organization and Process Patterns 
====================================

This section covers a small set of Organizational and Process Patterns that are required in order to successfully implement the more technical patterns that are foudn elsewhere in this pattern language. It is not our intention in this Pattern Language to document all of the different aspects of Agile Development that are required to be successful with Adoption of the Cloud; there are other books such as Forsgren, Skelton and Reznick that document those aspects already.

Instead, we would consider the set of patterns that we outline here to be the minimal set of constraints that are required in order to have a good chance of succeeding in a Cloud Adoption. We discovered these patterns when we realized that it was impossible to talk about the more technical patterns without them being placed within the context of this minimal set of Process and Organization Patterns. That small subset includes:

-  Two Pizza Team is fundamental in that it describes an important aspect about Cloud Adoption that is often overlooked - that humans work best in small groups. There are two extremes that we have seen that both result in failure; trying to let developers “go it alone” on the cloud without support, and trying to start massive, enterprise-wide projects for cloud adoption all at once. Two Pizza team describes how any successful transformation has to be built from small teams. 
-  Stream Team describes the basic team structure of a team building an application for the cloud. Teams need to be autonomous, cross-disciplinary and importantly, small enough to focus on one (and only one) particular aspect of a domain that they understand very well. These teams need to be tied together to produce larger applications, but the basic unit still needs to be small enough to share lunch. 
-  Platform Team is also foundational in that it recognizes the fact that Stream teams cannot go it alone without ensuring that they work together within a common platform, be that in a public cloud, private cloud, or even a Hybrid Cloud Model. 
-  Frequent Releases is a basic process principle that underlies many of the Agile practices and is the reason behind many DevOps patterns. One of the main reasons to keep teams to a size of a Two Pizza Team and employ architectural principles like the Microservices Architecture is to make Frequent Releases possible. 

Two Pizza Team is fundamental in that it describes an important aspect about Cloud Adoption that is often overlooked - that humans work best in small groups. There are two extremes that we have seen that both result in failure; trying to let developers “go it alone” on the cloud without support, and trying to start massive, enterprise-wide projects for cloud adoption all at once. Two Pizza team describes how any successful transformation has to be built from small teams.

Stream Team describes the basic team structure of a team building an application for the cloud. Teams need to be autonomous, cross-disciplinary and importantly, small enough to focus on one (and only one) particular aspect of a domain that they understand very well. These teams need to be tied together to produce larger applications, but the basic unit still needs to be small enough to share lunch.

Platform Team is also foundational in that it recognizes the fact that Stream teams cannot go it alone without ensuring that they work together within a common platform, be that in a public cloud, private cloud, or even a Hybrid Cloud Model.

Frequent Releases is a basic process principle that underlies many of the Agile practices and is the reason behind many DevOps patterns. One of the main reasons to keep teams to a size of a Two Pizza Team and employ architectural principles like the Microservices Architecture is to make Frequent Releases possible.


Table of contents
=================

-  Frequent Releases 
-  Platform Team 
-  Stream Team 
-  Two Pizza Team 


################################################################################


  Frequent Releases 
====================

You are building a complex application for the cloud, probably following a Microservices Architecture, but possibly Implementing Monolith First. You have a team that is working on several features at once.

How do you ensure that your team does not become mired in the complexity of testing many changes at once, with the accompanying difficulty in merging changes from several different streams?

Practice Frequent Releases in order to deliver business value to the customer in small increments, more often.


################################################################################


  Platform Team 
================

You are building one or more applications for the Cloud and you are following a Hybrid Cloud Model which results in you having several different environments; some on public cloud, and possibly some on private cloud. You need to enable your Stream Teams to be productive, without weighing them down with unnecessary infrastructure management.

How do you create an organizational structure that enables Stream Teams to focus on creating business value through building applications and not become mired in the minutiae of cloud platform management?

There is always an ongoing tension in companies adopting the cloud between how much autonomy to give teams developing applications, and how much control to centralize into the IT departnement. Due to the fact that many Public Cloud projects are started by the Line of Business rather than by Centralized IT, this tension is exacerabated as IT has sturggled to find its place in the new world. As a result, centralized IT often tries to reassert control by developing centralized private cloud services, either as a landing zone in a public cloud, or as an on-prem private cloud, but when these cloud environements are created without the direct involvement of the line of business teams, they often fall in to the trap of “if you build it they will come”, where no one ever comes to the newly created environments.

Therefore,

Create a common Platform Team that is responsible for developing and managing the components of a shared platform up through but not beyond the Container Orchestration layer

The Platform Team is, importantly, a Team and not an environment. They may be responsible for multiple environments, and may set standards for those environments, and build reusable assets that can be used by many teams, but they do not “put all of their eggs in one basket”. The kinds of reusable assets produced by the Platform Team could include Base Images that are used by the Container DevOps pipelines, but also template pipelines that the Stream Teams can consume and customize.

This pattern is discussed in depth in Team Topologies. One thing to keep in mind is that a Platform Team should not grow too large - it should ideally be no larger than a Two Pizza Team. If the team begins to grow larger than that, you need to consider if the platform itself should not be split into multiple streams, each with their own Stream Team. This can result in a fractal split of Platform, as shown in Team Topologies.


################################################################################


  Stream Team 
==============

You are either building a new application with Cloud Centric Design or applying Cloud Refactoring in order to modernize an existing application to run on the cloud.

How do you organize an application team to be able to successfully build an application for the cloud using Cloud Native DevOps principles that also takes into account the constraints that architectural principles like the Microservices Architecture place on a team?

The Two Pizza Team sounds simple in theory - just make sure your teams don’t get much larger than 10-12 people (or if they do, have a go-to pizza tekeout that provides very, VERY large pizzas). However, there is a lot implied in applying the Two Pizza Team idea to application development.

For most of the lifetime of Information Technology as a discipline, people have been organized by roles. At first glance, that makes sense; after all, if you are going to do a testing phase, then you obviously need Testers, so shouldn’t all of the Testers sit together and be organized under the same management structure in order to most efficiently apply these scarce human resources to the projects that need them the most? However, this conclusion is based on a number of assumptions that simply do not hold true anymore. First of all, the very idea of a separate “Phase” for activities such as Testing (or Development, or Operations, etc.) is based upon the assumption that development proceeds in a linear fashion across multiple aspects - that all the architecture and design happens first, followed by coding and development, followed by testing, and then finally by production deployment and operations. In the past, when release cycles were measured in months or even years, this could be said to bre true. However, as Forsgren has pointed out, one of the most reliabile indicators of how well a team functions is how often they release code to production. Today, release cycles for most software should be measured in at most days, if not hours.

When the Testing “Phase” disappears, what then becomes clear is that separating people by role is also questionable. Agile Practices such as Test Driven Development, and improvements and advances in Test Automation have made testing of nearly all types the domain of the developer - perhaps more accurately, developers have been asked to acquire the skills that were formerly reserved to Testers. The key observation is that handoffs between different roles takes time - if you can remove formal handoffs (which often require changes in tools, notation or data format) between roles then the overall pace of development accelerates.

So, if you want a team to move quickly, you need to reduce the time it takes to move between the different SDLC aspects. The best way to do that is to put all of the team mebers together, and to have the team members each participate (wherever possible) in every aspect to eliminate handoffs altogether.

However, scaling is stil a problem - even if your team members can perform each of the SDLC aspects, there is only so much work that a team sized according to the Two-Pizza rule can do. There needs to be a different way of dividing work if not by role.

Therefore,

Divide the overall work up into multiple, independent (or loosely coupled) work streams, divided along domain lines. Define one or more self-sufficient Stream Teams (one per workstream) that each are responsible for a well-defined portion of the application, and that are sized as Two Pizza Teams. Stream teams are responsible for building, testing, and deploying their portion of the application from end to end.

The advantages of dividing teams along domain lines, or more loosely, by saga, where a saga is a group of closely related User Stories, are accentuated when teams follow a Microservices Architecture. Conway’s Law states that the communications paths of your Software Architecture will reflect the communications paths of your organization. If you want to build a loosely coupled, domain-based software architecture, then organize your teams into autonomous units, each oriented around a particular aspect of your problem domain.


################################################################################


  Two Pizza Team 
=================

You are implementing a Cloud Native Design or implementing Cloud Refactoring. You need to decide how big your teams can get in order to keep team productivity at its highest peak.

How large should a team get before it becomes too unwieldly?

There is a tension that everyone who has been in any software development team of any size has ever experienced - how big is too big and how small is too small? There are small software projects that can be accomplished by a dedicated team of one or two people, but most software projects will require a larger team. However, as Fred Brooks first pointed out in The Mythical Man Month as the size of a software development team increases, the rate at which the code can be produced actually slows down as the number of potential communications paths between people (which is reflected in the software, aka Conway’s Law) increases.

What’s more, there is an abundance of social science research that shows that humans simply perform better in smaller groups, and that our ability to form trusted relationships tends to max out in the low double digits, for instance, see Dunbar or also Mueller. However, software is a complex entity. There are a lot of steps required to plan, write and test a large program. One of the things that Agile development has shown is that handoffs create friction. Reducing handoffs between formal teams is critical to achieving rapid releases. So, including all of the necessary skills within a team is crucial - you can’t “outsource” a critical step in the software development process and still remain Agile.

What we have seen in practice, is that there are two alternative approaches to building teams on the cloud that we have seen that both result in failure; first trying to let developers “go it alone” on the cloud without support, and next, trying to start massive, enterprise-wide projects for cloud adoption all at once. In the first case, you are not providing developers with all of the tools and support (particularly with learning) that they need. In the second case, you find that the large numbers of people in existing, often role-based teams results in organizational inertia - no one is willing to make the changes necessary to be successful with new approaches and tools.

Therefore,

Make teams small, self-sufficient entities that have all necessary expertise on board (development, operations, testing, business, etc.) but keep them to a size where two pizzas are sufficient to provide lunch/dinner for the whole team.

Anecdotally, this limitation to team size is attributed to Jeff Bezos.

When you look at the result of forming a team of this size and composition, what you see is that the ability to form trusted relationships in the smaller group facilitates the adoption of new approaches and new tools and processes. What’s more, you see that the team can be large enough to support some specialization, which is required in software development, but not so large as to enforce it - encouraging cross-training and the spread of skills around the group.

Keeping teams at this size will also encourage Frequent Releases, as the team will not grow so large that conflicts become too common and resolution of code conflicts becomes a major issue. However, this will only work with a strict division of labor that keeps teams out of each other’s way as the division of your organization both into domain-focused Stream Teams and Platform Teams will require.


################################################################################


  Image Building Patterns 
==========================

In other parts of this work we have discussed the advantages of containers as a technology for building applications to be deployed into a Hybrid Cloud Environment. We have also introduced the concept of Container Orchestration projects like Kubernetes and their prevalence as a part of the Cloud ecosystem. Finally, we’ve covered specialized issues involved in building DevOps Pipelines for applications that are to be delivered as Docker (or other container technology) Images. In this section, we will adddress another fundamental set of problems in developing container-based applications; that is building these images and following best practices for image development.

These patterns are important because, like nearly any other technology, it’s easy to get lost along the way of adopting container images and end up not gaining the benefits that the technology promises. In particular, as we have discussed earlier, teams adopt container technology because it promises:

Unfortunately, these benefits do not come for free. You can easily end up with very large container images that contain unnecessary elements that can bloat your resulting containers. Likewise, it’s possible to add unnecessary process startup and other resource requirements that can slow down your image startup from milliseconds to seconds or minutes. And finally, it’s not just possible but far to easy to render the promised security benefits moot and allow malware or simply badly coded applications to escalate privileges and gain improper access to the resources of the container host.

We cannot solve every possible problem that can lead to this loss of benefits, but there are several common approaches that can help to guarantee that your images will remain small, fast and secure. The patterns that capture those approaches are:

- Application Runtime Image is the place to begin in that it shows how to build most application images from a few basic images without letting container inheritance get out of hand.
- Minimal Base Image builds on this basic principle by ensuring that you think about what your image is based upon before starting development
- Efficiently Layered Image deals with the basic question of how many literal layers your final application image should contain and how to minimize your layer depth and memory footprint.
- Least Privilege addresses how to secure your container image by not granting unneeded permissions to resources
- Multistage Build discusses how the process of constructing the artifacts in an image can be separated from the final image for production
- Visualize Image Dependencies reinforces the importance of visualizing the set of dependencies of different types among images to track the possible impact of changes within the set of images


Table of contents
=================

-  Application Runtime Image 
-  Least Privilege 
-  Multistage Build 
-  Visualize Image Dependencies 


################################################################################


  Application Runtime Image 
============================

You are practicing Container DevOps and building many container images using a Container Build Pipeline. You want to build an image for your application that is as small as possible, that will occupy a minimal amount of memory and also allow for fast startup. However you also don’t want to spend a lot of time and effort in constructing your image if at all possible.

How do you take best advantage of the layered approach of containers yet not end up with enormous images?

Containers derive a great part of their power from the layered approach of a container image. You can build new images from existing container images and thus leverage the work done by others. Images inherit from each other, which makes it possible to have all “Ubuntu” applications run from a standardized image, all “Java” applications run from an image derived from that standard, and only the application binary distribution is the final layer that makes the container run the resulting application-specific image.

The benefits of layering in this way are many:

- Basic layers only need to be vetted once for each version
- As a cloud server will locally cache layers (subject to disk usage policies), the basic layers for your applications are likely already on the machine when it is launched, and only the application-specific layer needs to be fetched. This speeds up upgrades and launches of new applications.

However, there are drawbacks around this as well; a lot of the drawbacks are similar to the drawbacks of deep class hierarchies in object oriented programming:

- As an application developer, you may not always know what exactly resides in the shared base layers
- As a maintained of a base layer, you may not know all the applications that use your layer and therefore triggering upgrades (for example after a security patch) becomes challenging
- The deeper the hierarchy, the more complex management and correct patching policies become.

Therefore,

Build images in layers by inheriting from other images but don’t go overboard with very deep hierarchies; staying with around three conceptual layers - an OS base layer, a runtime layer, and an application layer are often enough.

Containers run in a host environment. The container host consists of two conceptual layers:

Then each container running on the container host consists of three conceptual layers:

This diagram illustrates all five layers:



We should note that we are explicitly referring to these as “conceptual” layers as distinct from the actual “literal” layers that comprise an efficiently layered image. That is because even a conceptual base layer may consist of several literal layers laid down during the build of the image. For instance, often a very early command in any Dockerfile (at least in debian-based images) is to download the latest updated package information with “RUN apt-get update”. That will create a literal layer with that new package information. The Dockerfile will then contain other commands to install specific packages which create additional literal layers. However, it’s easiest to think of all of these together as being a single conceptual layer.

You should note that the “three layers” rule is loose - for instance, three layers would be all you need for an application built using Spring Boot. However, there are often reasons to add additional literal layers between these layers. For instance, if you were using an Application Server like Tomcat or OpenLiberty, you may well have four conceptual layers, each of which is represented by an image the next is built from; the OS, the JDK, the Application Server, and your Application.

All this layering leads to the need to have a system in place that allows you to find and automatically regenerate base layers. For security, you will want to keep track of what applications are using what base layers (either through tooling in your Docker registry or by scanning your version control system). Having a system that allows you to Visualize Image Dependencies is helpful in this regard. You also want to clearly and extensively tag base layers so it is clear what they contain (for example: ‘ubuntu-java:14.04-1.8u123’ for an image that a java application can use).

The next place to go for building an application is in determining what it takes to build an Efficient Layered Image. What’s more, even this basic approach, while a good place to start, may still lead you to want to separate your images during the build phase by applying Multistage Image Build which is an approach for minimizing the size and security attack surface of an image that has additional steps in the build process.


################################################################################


  Least Privilege 
==================

You are building a container image that includes third party code (perhaps open source libraries, or a third party application). You want to ensure that this code running in your container cannot access resources that it is not supposed to be able to access (such as the file system of the host).

How do you ensure that your containers are not open to container privilege escalations?

Privilege escalation is the process of obtaining more permissions for a resource (such as a process). Malicious code does this through exploiting weaknesses in different entry points into the Linux kernel. While code scanners can usually detect known vulnerabilities and malicious code in third party libraries, it only takes one undetected zero-day exploit to render your system insecure.

Therefore,

Follow the principle of least privilege. A process should have only those access rights that are needed to accomplish each task and no more.

The last two are especially important in limiting the possibility of privilege escalation. First of all, begin by creating the user and group in the Dockerfile. An example of this is would be to use:

```
RUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres

```

```
RUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres

```

Next, you want to use the USER command to change to the non-root user you just created.

```
USER postgres

```

```
USER postgres

```

The downside (if it can be considered a downside) of this approach is that this will only work if the process can run without root privileges. However, requiring root privileges in Unix or Linux can be considered a bug for most software. However, if a preparation or build step requires escalated privileges, then you can perhaps consider combining this pattern with Multistage Build to escalate privileges in the temporary build image that is created, and then only using the newly created user (with fewer privileges) in the final production image that the build creates. In any case, you want to avoid constantly switching users within your Dockerfile - you want to avoid switching users at all if you can, and if you cannot, at least group all of the commands that have to execute under a particular user together.

Building images up from standardized minimal base images is a way of minimizing the amount of privileges you need to grant in that there is less of a need for additional process execution during the build phase.


################################################################################


  Multistage Build 
===================

You are building an application with containers. You have a complex build process that requires several tools to complete the build. However, these tools are no longer needed once the container has been built, as they are not required at runtime.

How do you keep the size of your docker files down to a manageable level, and at the same time reduce the potential attack surface of your docker containers by making sure that they only contain what is absolutely necessary?

Let’s consider the following problem. You are building a Minecraft server that has a special plugin that allows for an online agent to participate in chats. Now, creating a Minecraft server may seem like a frivolous example, but in fact, Minecraft is one of the most commonly used Java applications in existence, with an extensive development community, and has a build and extension process that is quite similar to many Enterprise applications written in Java. As a result, it makes a nice stand-in for a complex Enterprise application. In order to extend Minecraft, you run the base Minecraft server along with a predefined “plugins” directory - very similar to the extension process for Eclipse or many other applications written in Java.

The process to building a server in Linux is:

After the Build Jar has run, a new server Jar file (named spigot-.jar) is created. That Jar file looks in the "plugins" directory for other Plugin Jar files when it executes.

This process has been encoded in the Dockerfile below:

```
FROM ubuntu:20.04
MAINTAINER Kyle Brown “brownkyl@us.ibm.com”
RUN apt-get update
RUN apt-get install -y git
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y default-jdk
RUN apt-get install -y wget
RUN mkdir minecraft
RUN wget "https://hub.spigotmc.org//jenkins/job/BuildTools/lastSuccessfulBuild/artifact/target/BuildTools.jar" -O minecraft/BuildTools.jar
RUN git config --global core.autocrlf input
RUN java -jar minecraft/BuildTools.jar --rev 1.16.1
RUN echo "eula=true" > eula.txt
RUN mkdir plugins
ADD HelloWorld.jar /plugins/HelloWorld.jar
CMD java -XX:MaxPermSize=128M -Xms512m -Xmx1024m -jar spigot-1.16.1.jar nogui
EXPOSE 25565

```

```
FROM ubuntu:20.04
MAINTAINER Kyle Brown “brownkyl@us.ibm.com”
RUN apt-get update
RUN apt-get install -y git
RUN DEBIAN_FRONTEND=noninteractive apt-get install -y default-jdk
RUN apt-get install -y wget
RUN mkdir minecraft
RUN wget "https://hub.spigotmc.org//jenkins/job/BuildTools/lastSuccessfulBuild/artifact/target/BuildTools.jar" -O minecraft/BuildTools.jar
RUN git config --global core.autocrlf input
RUN java -jar minecraft/BuildTools.jar --rev 1.16.1
RUN echo "eula=true" > eula.txt
RUN mkdir plugins
ADD HelloWorld.jar /plugins/HelloWorld.jar
CMD java -XX:MaxPermSize=128M -Xms512m -Xmx1024m -jar spigot-1.16.1.jar nogui
EXPOSE 25565

```

Now here’s the issue we have to consider - we’ve just added multiple tools to the image that are not needed at runtime! A version of Java will still be needed in the final image - not only is is needed for the build stage, but also to invoke the Java command to start the Minecraft server at the end. However, the JDK is not really needed - instead, a JRE (which adds 239MB less to an image size) would suffice. Git is not needed in the end, nor is wget, both of which could potentially contain vulnerabilities that could be exploited.

Now, one way you could do this (which was actually common early in the development of docker images) is to combine all of the commands to obtain a tool, use a tool and then delete the tool (and its associated artifacts) in one long Unix command line. An example of this is shown below

```
FROM ubuntu:20.04
MAINTAINER Kyle Brown “brownkyl@us.ibm.com”
RUN apt-get update &&\
    apt-get install -y git &&\
    DEBIAN_FRONTEND=noninteractive apt-get install -y default-jdk &&\
    apt-get install -y wget &&\
    mkdir minecraft &&\
    wget "https://hub.spigotmc.org//jenkins/job/BuildTools/lastSuccessfulBuild/artifact/target/BuildTools.jar" -O minecraft/BuildTools.jar &&\
    git config --global core.autocrlf input &&\
    java -jar minecraft/BuildTools.jar --rev 1.16.1 &&\
    rm -r Bukkit &&\
    rm -r CraftBukkit &&\
    rm -r Spigot &&\
    rm -r BuildData &&\
    rm -r work &&\
    rm -r minecraft &&\
    apt-get purge -y --autoremove git wget
RUN echo "eula=true" > eula.txt &&\
mkdir plugins
ADD Tutorial.jar /plugins/Tutorial.jar
CMD java -Xms512m -Xmx1024m -jar spigot-1.16.1.jar nogui
EXPOSE 25565

```

```
FROM ubuntu:20.04
MAINTAINER Kyle Brown “brownkyl@us.ibm.com”
RUN apt-get update &&\
    apt-get install -y git &&\
    DEBIAN_FRONTEND=noninteractive apt-get install -y default-jdk &&\
    apt-get install -y wget &&\
    mkdir minecraft &&\
    wget "https://hub.spigotmc.org//jenkins/job/BuildTools/lastSuccessfulBuild/artifact/target/BuildTools.jar" -O minecraft/BuildTools.jar &&\
    git config --global core.autocrlf input &&\
    java -jar minecraft/BuildTools.jar --rev 1.16.1 &&\
    rm -r Bukkit &&\
    rm -r CraftBukkit &&\
    rm -r Spigot &&\
    rm -r BuildData &&\
    rm -r work &&\
    rm -r minecraft &&\
    apt-get purge -y --autoremove git wget
RUN echo "eula=true" > eula.txt &&\
mkdir plugins
ADD Tutorial.jar /plugins/Tutorial.jar
CMD java -Xms512m -Xmx1024m -jar spigot-1.16.1.jar nogui
EXPOSE 25565

```

However, this is an unsatisfactory solution in a number of ways. Not only is it possible to leave tools behind (for instance in this instance, note that the script cleans up wget, but leaves Git behind installed in the image) but it is difficult to debug problems when they occur.

Therefore,

Use a multistage build that only retains the final outputs of the build process in the resulting container. Likewise, fetch and manage both build tools and secrets in an intermediate image layer that is discarded along the way.

Docker engine 17.05 introduced the idea of a “multistage build” that effectively creates a throwaway image that can be used for the build step, is accessible long enough to copy out the relevant built artifacts, and then is destroyed. This required a little additional syntax to the FROM command, allow you to first of all declare a second image (with an additional FROM command in the Dockerfile) and then also to allow you to reference that intermediate image from the final image with the COPY command.

```
FROM ubuntu:20.04 AS builder
MAINTAINER Kyle Brown “brownkyl@us.ibm.com"
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    default-jdk \
    git \
    wget
WORKDIR minecraft
RUN wget "https://hub.spigotmc.org//jenkins/job/BuildTools/lastSuccessfulBuild/artifact/target/BuildTools.jar" -O BuildTools.jar
RUN java -jar BuildTools.jar --rev 1.16.1

FROM ubuntu:20.04
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y default-jre
WORKDIR minecraft
COPY --from=builder /minecraft/spigot-1.16.1.jar .
RUN echo "eula=true" > eula.txt
RUN mkdir plugins
ADD Tutorial.jar /plugins/Tutorial.jar
CMD java -Xms512m -Xmx1024m -jar spigot-1.16.1.jar nogui
EXPOSE 25565

```

```
FROM ubuntu:20.04 AS builder
MAINTAINER Kyle Brown “brownkyl@us.ibm.com"
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y \
    default-jdk \
    git \
    wget
WORKDIR minecraft
RUN wget "https://hub.spigotmc.org//jenkins/job/BuildTools/lastSuccessfulBuild/artifact/target/BuildTools.jar" -O BuildTools.jar
RUN java -jar BuildTools.jar --rev 1.16.1

FROM ubuntu:20.04
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y default-jre
WORKDIR minecraft
COPY --from=builder /minecraft/spigot-1.16.1.jar .
RUN echo "eula=true" > eula.txt
RUN mkdir plugins
ADD Tutorial.jar /plugins/Tutorial.jar
CMD java -Xms512m -Xmx1024m -jar spigot-1.16.1.jar nogui
EXPOSE 25565

```

Note that what happens is that this Dockerfile separates the process of creating an image for building the Jar file (the top half of the file, which is defining the “builder” image) from the process of creating an image to run the resulting file (the bottom half, which is what is returned from docker build as the final image). The difference in size is stark. The previous Dockerfile, when built, results in an image which is 907MB in size. The second Dockerfile results in an image which is only 556MB in size. Given that one of the main reasons why you choose to use Docker images is the ability to pack in more instances in the same amount of memory, in this example you’ve just roughly doubled the number of containers built from this image that can be run on the same Kubernetes node.

In order to control the overall size of the image, you want to begin from a Minimal Base Image. You always need to follow the principle of Least Privilege when starting processes and requesting access to resources.


################################################################################


  Visualize Image Dependencies 
===============================

You have a medium to large estate of container images where each image is dependent on multiple images.

The underlying layering mechanism that containers use are meaning it’s difficult to see how an individual image is made up of the various base images within the estate. In addition it’s hard to see all the images that utilize the core base images.

How can you understand the dependencies in your image model?

- You need to understand which images in the estate are orphaned post version upgrades
- You need to understand all of the images that are affected by patching an image
- You need to understand which third party images are used or unused
- You need a simple overview of all images in the estate
- Licensing and auditing compliance

You need to develop an image dependency model. For each container image you need to document the metadata and the dependencies of the image from the and the way in which each image depends on other images (by inheritance or by reference).

Therefore,

Build an image dependency model as a visualization of the dependencies.

An image dependency model will allow you to see:

- Every image in your estate
- The version of the image
- The registry that the image resides in (and whether it’s public or private)
- All images the image is dependent on

The following diagram shows an example visualization of a dependency model.



While diagrams like this can be drawn by hand, it is best if they are provided by a tool as part of your architecture documentation. One such open source tool is DockViz. You can also see other dependencies between images by using tools like WeaveView.


################################################################################


  Basic DevOps Introduction 
============================

Continuous Integration and Delivery has become the de-facto standard for most Agile and Lean processes. Many of these development processes help teams respond to unpredictability through incremental, iterative work cadences and through empirical feedback. Regular delivery of reliable working software that has the highest value to those using or benefiting from the software is becoming more important for the success of many companies. Actually, the first principle behind the Agile manifesto is: “our highest priority is to satisfy the customer through early and continuous delivery of valuable software.” That has led to the acceptance of DevOps, which advocates automation and monitoring at all steps of software construction, from integration, testing, releasing to deployment.


Table of contents
=================

-  Automate As You Go 
-  Blue-Green Deployment 
-  Canary Deployment 
-  Correlation ID 
-  Feature Toggle 
-  Log Aggregator 
-  Microservices DevOps 
-  Quality Delivery Pipeline 
-  Query Engine 
-  Red/Black Deploy 


################################################################################


  Automate As You Go 
=====================

      (aka Red-Black or A/B Deployment)

“The first rule of any technology used in a business is that automation applied to an efficient operation will magnify the efficiency. The second is that automation applied to an inefficient operation will magnify the inefficiency.”— Bill Gates

At the start of agile projects there are many pressures to get something out to the end-user and to get initial reactions and feedback. It is important to establish a frequent delivery cadence and tools to make that possible. In creating this environment, quality-related items need to be considered as well. As a system evolves, it is essential to regularly evaluate the system to make sure that key qualities are being met.

How can agile teams create tooling and an environment to assist with quick feedback about important qualities of the system and make their current status accessible and visible to the team?

❖ ❖ ❖ Not focusing on important qualities early enough can cause significant problems, delays and rework. Remedying performance or scalability deficiencies can require significant changes and modifications to the system’s architecture. However, focusing too early on system qualities in the development cycle can lead to overdesign and premature optimization [Knuth].

Agile teams primarily focus early in a project implementing functional requirements. There is often a priority to doing the minimal necessary to getting something working so as to get customer’s reaction to the system’s functionality as soon as possible. This can lead to taking shortcuts or a lot of quick and dirty manual tasks such as testing to quickly get the product out. Although there are benefits to getting fast feedback, as the project grows, a growing number of manual tasks slows the team down, making it harder to safely validate and evolve the system.

There is often a temptation to use the latest and greatest tool that has been recently hyped. However, you have limited time and resources and there is a lot of pressure to get something out as soon as possible. Automation takes time and energy to set up and sometimes the payoff is not immediately apparent. Some tasks, specifically quality testing and validation, can be hard to automate if the architecture was never designed to be testable.

There is a risk that focusing too much on automation could cause the team to get caught up in tooling and spend too much effort and time on automation. Another risk is that by automating too many tasks you consequently slow down your build and deploy pipeline by testing too frequently or at the wrong time in the pipeline.

Setting up environments and automating the testing of system qualities can require specialized skills and expertise. Also, you may not know what to measure about qualities until you have sufficiently defined the functionality. You want to avoid automating or putting effort into tasks that may not add value later. Being agile, you want to do things just in time.

❖ ❖ ❖ Therefore,

Create an environment and use tools to automate fundamental things that add value as soon as you can. Do not put off automation tasks until late in development.

Some automations are important to do from the start. Early on the most essential things to automate are the build, integration and test environment configuration. Then automate functional tests and system quality tests. But that’s only the start. There are other things you can automate such as acceptance tests, performance metrics, code smell detection, application security checks, and architectural conformance. Also if you have repetitive, tedious or error prone tasks, and if you can automate those as well. As you automate tasks, they become part of the cadence of your project.

The more you automate repetitive manual tasks, the more time it frees you up to do more. It also allows time to spend on exploratory testing. Automation also allow you to more safely evolve the system. Automation lets you do work in smaller batches, making fewer mistakes and getting quicker feedback. With automated tests, you will know when something goes wrong with those items you are testing. You can run automated tasks more often making sure that important qualities are still being satisfied.

If you need to validate performance under load before you release, you might need to spin up and create a specific environment to test the system performance. This could require setup of databases and networks, etc. Doing this by hand every time can be error prone and take time. By creating a virtual environment with scripts that automate this setup, you can make performing this task much easier. As you see you are having to repeat manual tasks and that automation can help, it is time to add an automation task to your backlog.

When making a decision to automate, it is helpful to think about how long a particular automated task takes to execute and how frequently it is performed. You should consider automating infrequently performed tasks as well, especially if they are error prone or involve a lot of steps. If a task is expensive to automate and you do it infrequently, a manual checklist script might be the better alternative. For example, if you have a manual task that you perform once a year that takes one day but can be automated in five days, your return on time spent to automate will come only after five years.

The time it takes to perform any automated task figures into your consideration of whether to install it into your normal build and deploy process or to take it off of that workflow. If you can get early, quick feedback using less time-consuming automated tasks, those automations may prove as valuable as more thorough, longer running tests or automations. For example, a security scan that runs penetration testing against a deployed app can be automated, but might be too slow to be part of your automated build process. A static code analysis that includes looking for some security defects might not be as comprehensive as the penetration scan but can still provide useful feedback.

There are different testing cycles, especially for system qualities. Some tests, such as unit tests, will be run frequently, maybe hourly. When you check-in code, there will be some simple integration tests that will run. But other tests, if run at that time, might slow down the check-in process too much. So these quality or regression tests might be run nightly or even less frequently.

Automation considerations will influence your architecture style, your choice of frameworks, interface design, and many design details. For example if you decide that you will have many automated tests, you will want to design your system so these can be easily added. And while you will want to make parts of you system testable in isolation, you also need to consider how to perform meaningful integration tests.

Knowing what automation is necessary, and when automated tests and tasks should be run, is important. System Quality Specialists can provide expertise to assist with automation. Some quality tests are hard to set up and take a lot of time to run. Sometimes you have to be clever to set these up correctly and decide when good times are to run them. It is important that the results of the automation is visible to the team. This can be done via System Quality Dashboards and System Quality Radiators. However, you do not want to be overwhelmed with too much information that can be overlooked and ignored. The teams needs to decide what feedback is useful and the frequency that it gets updated.

There are a broader set of activities, beyond simply building and running tests continuously that need to be part of a continuous integration pipeline, such as deployment and IDE integration [Duv]. One of these activities that is valuable during continuous integration is Continuous Inspection. Continuous Inspection includes ways of running automated code analysis to find common problems before integration. Continuous Inspection also describes many additional automated tasks that can help insure that certain qualities and architecture constraints are being met [MYGA].

Automation tools are highly dependent upon the architecture and platform that is being used. For example if you are developing with Java, you might consider SonarQube, PMD, Checkstyle, FindBugs, JaCoCo, Jenkins, Maven and Eclipse with various plugins such as JUnit and code analysis tools.

When selecting tools, it is also useful to evaluate and select one or more tools that can perform static analysis on your code base. Tool evaluation criteria should include the programming and scripting languages used in your software projects versus the ones supported by the tool; whether the tool provides an API for developing customized verifications; integration with your IDE; integration with your continuous integration server; and the appropriateness to your project of the built-in verifications provided by the tool. Specialists can assist with tool selection.


################################################################################


  Blue-Green Deployment 
========================

      (aka Red-Black or A/B Deployment)

You are doing continuous delivery [Humble10] as part of DevOps by making deployment as automated and quick as possible. You are releasing to a live environment that has potentially many users.

**How can we deploy reliably and with confidence of not negatively impacting many users?((

Building and releasing into production environments may require several steps, such as transferring and replacing deployment artifacts (e.g., executable files, JAR files, Docker images), updating property files, altering database structures, reconfiguring infrastructure elements (e.g., message routing in an API gateway). This process can be tedious and error prone.

Automating the testing and deployment process is challenging, due to its complexity and uncertainty, thus requiring a lot of expertise and effort.

Lack of validation or testing of your release can be dangerous and costly.

It can be expensive to duplicate the entire production environment. Any other alternatives will be only a simpler replica, possibly not emulating all the issues of production.

New releases have various risks that can negatively impact the business (e.g., loss of funds), if problems arise. New releases that cause problems need to be rolled back quickly and reliably.

Therefore,

When releasing, have two environments that are nearly identical. One is the live environment. Release into the non-live environment, after validating the release, switch all network traffic to the new environment disabling the previous live environment.

This type of deployment process is referred to as “blue-green deployment”. Blue-green deployments require two nearly identical production environments (called “blue” and “green”) where deployments are made The two environments can be, for example, two physical or virtual machines, two nodes on a Kubernetes cluster, to mention a few.

At any time only one of the blue-green environments is live. For example (see Figure 1), let’s say the green environment is currently being used for live production. When you have a new release, you deploy your system and do your final testing in the other (blue) environment. Once the software is working in the new environment (blue), you switch all live access so that all incoming requests now go to the new tested (blue) environment. The previous production environment (green) is now idle and ready either for a rollback, for emergency use, or for the next release. As you have new releases, you continue to switch back and forth between the blue and the green environment.

Figure 1: Blue-Green Deployment

Switching the live version from v1 to v2, or rolling back from v2 to v1 can still be complicated, as we should allow requests currently being processed to finish successfully before the transition. Your design might handle this through some replication and ramped release strategy, or possibly putting the systems into a read only stage during the transition.

Advantages of Blue-Green Deployments

Foremost, blue-green deployment significantly decreases the downtime for rolling out a new version. Blue-green deployment enables quick version rollback: after deploying to one environment, if a problem is discovered you can easily switchback and start using the previous environment. You may have to cleanup some transactions that happened during the rollout of the failed environment.

With blue-green deployments you always have a backup environment ready in case the production environment becomes unavailable. Having the two environments may also allow independent maintenance in the infrastructure. For example, let’s say blue and green environment are two separate VMs. When blue is active, you may perform upgrades to the green environment VM. Then when green becomes active, you can perform the upgrades on the blue environment VM. The application is not affected.

Disadvantages of Blue-Green Deployments

Blue-green deployments require organizations to have two identical sets of production environments, which can lead to significant added costs and overhead without actually adding capacity or improving utilization. As an alternative, there are other strategies that can help such as canary or rolling deployments. Canary deployment releases the new system to a small limited number of users, while rolling deployment staggers the rollout of new code across servers, usually to a server with limited number of users first.

When the new version requires database schema changes and/or data migration, employing blue-green poses an additional design challenge. There are two main alternatives:

- When there is a single centralized database. make the database structure changes backward compatible, that is, the old code will be able to access the new database structure. Complementarily, the new version code should be backward compatible, that is, the new code will be able to access the old database structure.
- Make each version access a separate database (separate DB server or separate logical space/owner within the same DB server). In this case, the data is replicated across the old version DB and the new version DB. Therefore, a data synchronization mechanism must be established. This is an eventual consistency setting that has the additional drawback that an application may consume stale data.

Blue-green deployment can be problematic when the new version contains API changes that make old client applications incompatible. In this case, we might need an interceptor placed between the clients and the old and new application to perform message transformations to deal with the API changes.

** THESE ARE RELATED PATTERNS I USE SOME Stars to separate **

Blue-green deployment can use feature toggles to emulate a form of canary deployment, by toggling on/off certain features for certain users roles.

Blue-green deployment can be used in conjunction with canary deployment. In other words you can push the new release completely to the second environment. And then route selected users from the first environment to the second environment in a canary fashion.

Note this pattern were first introduced and evolved from a AsianPLoP 2019 paper by Joseph Yoder, Ademar Aguiar, Paulo Merson, and Hironori Washizaki.


################################################################################


  Canary Deployment 
====================

      (aka Staged Deployment)

You are doing continuous delivery as part of DevOps by making deployment to different environments as automated and as quick as possible. You are releasing to a live environment that has potentially many users.

How can we get feedback on the new release, verify if it is working properly, and get early reactions from users?

Building and releasing into production environments can be tedious and error prone.

Automating the testing and deployment process is challenging and requires a lot of expertise.

Lack of validation or testing of your release can be dangerous and costly.

New releases have various risks that are important to validate before release to all users.

New releases made available to all users can can severely hurt the confidence on the application and negatively impact the business if they are flawed,

Therefore,

First deploy the change to a limited number of users or servers to test and validate the release. This could include verifying the release works properly and/or getting acceptance feedback from your users. After you have validated the release, then roll the change out to all servers or users.

This limited release is called canary deployment. Canaries used to be used in coal mining as a warning system making sure there were no toxic gases before miners entered the mine. In a sense we are doing the same thing with canary deployment. Before releasing to a wide audience, the system is first deployed to one or more canary servers (see figure 2). These might be for trusted internal users. After the release is validated, make it available to other users and servers. Validation includes getting feedback from canary users and also monitoring runtime properties of the new version. If significant flaws are found, the release of the new version to all users is cancelled.

Figure 2: Canary Deployment

Advantages of Canary Deployments

Canary deployment allows finding problems before they are pushed out to all users. These problems can be bugs in the new release, unsatisfactory latency or throughput, poor user experience, security breaches, among others. The final result is improved quality of new releases.

Disadvantages of Canary Deployments

Canary deployments require you to control what users see the canary release and hence requires a supporting infrastructure for moving users from the main system to the canary system, and vice-versa. This supporting infrastructure has to be configured and governed, and may include routers, network proxies, authentication mechanisms, and configuration files.

The canary deployment represents a delay on the rollout of the new release to general users.

Similar to blue-green deployment, canary deployment incurs the cost of having two production environments.

Alternatives to Canary Deployments An alternative to canary deployment is to deploy the new version to everyone with feature toggles turned off for the new features. Then you selectively enable features to different users. As you validate the release, you increase the number of users access to the new features. This approach requires a special implementation using “feature toggles” combined with canary user identification.

❖ ❖ ❖ * * *

You can also use blue-green deployment to push the release out to the green server for example, and then only move a few users from the blue server to the green server for the canary deployment. Then as you validate the release you can move more and more users to the green server.

Note this pattern were first introduced and evolved from a AsianPLoP 2019 paper by Joseph Yoder, Ademar Aguiar, Paulo Merson, and Hironori Washizaki.


################################################################################


  Correlation ID 
=================

You are building an application in a Microservices Architecture. Your application may use Backends for Frontends and multiple Business Microservices and Adapter Microservices, and may have complex call graphs as a result.

Building Microservices is the easy part. Operating Microservices, and debugging problems when they occur is the more challenging aspect. By multiplying the number of processes in your overall system (which happens when adopting a Microservices Architecture) you increase the overall operational complexity by increasing the number of discrete processes, and thus different logs with different entries, that may be created. The result is that instead of a call graph being abl to be debugged by simply comparing timestamps in a simple log, a single business transaction may consist of a more complex web of calls moving in and out of several disparate processes.

How do you debug a complex call graph when you do not know where in the set of microservices along that call graph the problem may have been introduced?

-  You don’t want to overly burden your developers with complex requirements for logging and monitoring 
-  You need to be able to trace a call regardless of how complex or simple that call is. 

You don’t want to overly burden your developers with complex requirements for logging and monitoring

You need to be able to trace a call regardless of how complex or simple that call is.

Therefore,

Consistently use Correlation ID’s to tie multiple microservice calls together. A correlation ID is a simple identifier (usually just a number) that is passed in to every service request and passed along on every succeeding request – when any service logs something for any reason, the correlation id is printed in the log entry. This allows you to match or correlate specific requests to one service to other service requests in the same call chain.

Implementing correlation id’s correctly requires four consistent actions:

Create correlation ID if none exists and attach it as a header to every outgoing Service Request

Capture the incoming correlation ID on every incoming request and log it immediately

Attach the correlation ID to the processing of the request (perhaps using a threadlocal variable…) and make sure that the same correlation ID is passed on to any downstream requests

Log the correlation id and timestamp of *any* messages that are connected to that thread of processing

Once you have implemented a Correlation ID you can then use a Log Aggregator to gather together all of the logs across all of the dependent systems in your Microservices Architecture and can perform a query for the Correlation ID against the aggregated log. When placed into timestamp order the results of this query will show the call graph of the solution and allow you to put errors into the appropriate context in terms of which microservice instance and thread handled the calls, and what the parameters to the calls were at the time that the problem was encountered.

Correlation ID was originally called out in Hohpe


################################################################################


  Feature Toggle 
=================

You are building a cloud-native application using a Microservices Architecture and need to roll out the deployment of new functionality.

How can we control the roll-out of potentially risky new functions?

You want to enable early testing of your software without requiring every aspect of the software to be complete before testing begins.

You want to avoid having to have “feature branches” in your source code repositories, allowing for main-branch development.

Build a software switch in the microservice that can enable or disable the functionality without having to redeploy. Have a mechanism to ramp-up the new functionality as needed (percentage of traffic, per user, per account, geographically, etcetera).

Feature toggles are (in the simplest form) implemented as conditional code that simply looks for the presence or absence of a flag (perhaps a configuration, a global variable, or a value passed in as an HTTP header) before either enabling or disabling a new feature.

This article gives more information on implementation patterns of Feature Toggles. Products such as Launch Darkly implement Feature Toggles along with other incremental deployment features.


################################################################################


  Log Aggregator 
=================

You are building applications using a Microservices Architecture and need to be able to debug problems that cross the different components of that architecture.

How can you effectively view and search all of the different log files that emerge from all of the different distributed runtimes that comprise your collection of microservices?

-  A Microservices Architecture can span dozens or even hundreds of individual application server processes. This results in hundreds of individual log files. 
-  Microservices are often (but not always) implemented using cloud solutions that limit the lifetimes of the individual application server processes. When you use a solution like Cloud Foundry or docker, when an individual server container dies, any in-memory state within that cont ainer is lost. 

A Microservices Architecture can span dozens or even hundreds of individual application server processes. This results in hundreds of individual log files.

Microservices are often (but not always) implemented using cloud solutions that limit the lifetimes of the individual application server processes. When you use a solution like Cloud Foundry or docker, when an individual server container dies, any in-memory state within that cont ainer is lost.

Therefore,

Use a Log Aggregator that pulls all of the files together into a single location. The Log Aggregator will “listen for” or tail each individual log file and forward the log entries to an aggregated collection point as they are made.

Examples of Log Aggregators include: Splunk, Logstash, and the Cloud Foundry Loggregator.

Once you have collected log entries together, then the database of the entries can be searched in order to make debugging more meaningful. For instance, you can look for occurrences of the same log entry, perhaps a particular error message across multiple log files from multiple servers in order to see if a problem is sporadic or widespread. This means you need a Query Engine to be able to search the set of logs.

But the most helpful debugging approach is to take entries that are connected by a single thread of control that spans multiple microservices and correlate them together to find problems that cross a network of microservices in a particular call graph. That problem can be solved through the combination of a Log Aggregator and Correlation ID’s.


################################################################################


  Microservices DevOps 
=======================

You are building a complex application with a Microservices Architecture. You need to be able to put that system into production.

How do you balance the needs of developers (which want to work on small, easy-to-modify artifacts) and operations teams, which need to minimize the impact of changes on critical systems in order to maintain availability?

-  If you deploy microservices in the same way you deployed a monolithic application, you will not obtain the benefits of a microservices architecture. 
-  If you let each team go off and follow their own approach without setting guidelines on how microservices are deployed, modified and managed, it will be impossible to debug the resulting system. 

If you deploy microservices in the same way you deployed a monolithic application, you will not obtain the benefits of a microservices architecture.

If you let each team go off and follow their own approach without setting guidelines on how microservices are deployed, modified and managed, it will be impossible to debug the resulting system.

Therefore,

Follow a Microservices DevOps approach that allows you to isolate each Microservice as much as possible, while being able to easily and quickly identify and resolve issues with the Microservices.

- Begin with the fundamental principle of deploying one Container Per Service.
- Apply unique CI/CD pipelines to each Microservice to allow you to build and deploy each microservice individually.
- Follow Container DevOps approaches to ensure that the output of each stage is a valid, trusted container.

One important consideration is that while Microservices are fundamentally about independence, having shared, common deployment and operations approaches allows you to set guidelines on how microservices interrelate and interact with each other. They also faciliate team members moving between teams as needed.

Since changes to a microservice can occur at any time, other microservices need to be isolated from the results of that change through a Services Registry.

What’s more, microservices cannot function completely independently – since each microservice will depend on other microservices or other downstream services (such as a Scalable Store) you need to apply techniques such as a Log Aggregator, Query Engine, and Correlation ID’s to understand the interaction between different microservices and debug issues.


################################################################################


  Quality Delivery Pipeline 
============================

Continuous Integration has become a key factor for most software development teams as a means to safely evolve the code ensuring that we have not broken anything that we have to interact with. A step to be successful with this is to setup servers and scripts for continuous integration. Continuous Delivery [Humble] is an extension of this by taking the automatic or semi-automatic promotion of builds and process them into something that can be delivered. The goal is to take changes from version control to production by making deployment to different environments as automated and as reliably as possible.

How can we create a process that helps us deploy while ensuring the desired qualities are met?

Automating the testing and deployment process is challenging and requires a lot of expertise. Building and releasing into production environments can be tedious and error prone.

Businesses can get very busy trying to meet the requirements where there is barely time, resources or people to do what is needed to keep a project afloat.

Lack of quality validation or testing of your release can be dangerous and costly.

Therefore,

Develop and automate a delivery pipeline composed of steps to get and build the current version, run various quality validations on the build, and when successfully validated, push the build into a staging or production environment.

Figure 3 outlines the core of what a minimum pipeline should be. In a sense it is analogous to pipes and filters where each step in the pipeline is a quality filter such as code quality, technical debt, security, etc. It is important that the pipeline be small and fast, instead of long and slow. Automating As you Go [YWW16b] makes the process quicker and more reliable.

The minimum pipeline must first get the code and build it. Then there is a step for some form of minimum quality testing such as unit and/or integration testing. If the build and test steps are successful, the system can be pushed into a staging environment for QA testing. Finally, if all steps were successful, the system can then be pushed into the production environment. Although this minimum pipeline works, it may not be “ideal” for gaining confidence as there can be many other critical qualities that need to be validated before releasing into production.


  “Ideal” Quality Pipeline 
===========================

A pipeline that offers more confidence would add more involved testing steps and also validate many other important qualities. Some of these qualities might be code quality while others might be reliability, security, performance, or other architectural qualities. The following (Figure 4) is an example of a more complete quality pipeline.

This pipeline has steps for building the system (clone/clean/build), unit and integration tests, security checks through Fortify, and other code quality checks through Sonar. Additional architectural checks can be included in Sonar such as those outlined through Continuous Inspection [Merson]. Validating the dependencies are healthy through a pre-health test before release minimized potential issues when going into a production environment. Building monitors as part of the release and include as part of the pipeline is key and can help teams “monitor” the results of the release.

* * *

The following is a set of related practices to gain confidence and assist with a quality pipeline:

- Small Incremental Releases - keep releasing small changes into production.
- Automate as you go - automate what you can as soon as you can.
- Continuous Inspection - getting regular feedback.
- Blue-Green Deployment - separate deployment from release.
- Staged Releases - first in a safe environment and spread out.
- Canary and Rolling Deployments - gradually roll out your release..
- Health Checks - Smoke Tests to make sure things are ok, etc.

The following are some advantages of Quality Delivery Pipeline:

- Freeing teams up from needing to think about delivering quality makes them more productive as important quality validations are built into the pipeline;
- Some important critical tasks can be done more quickly and with confidence.
- You can ensure that what gets delivered meets a minimum quality.

There are also some potential disadvantages to Quality Delivery Pipeline:

- Automating all quality checks might be very difficult,.
- Creating a quality pipeline can require a lot of technical expertise.

Automate as you Go can helps ensure the Quality Delivery Pipeline is run frequently, and error prone steps are avoided. The Quality Delivery Pipeline becomes a form of Continuous Inspection during the release cycle.

Note this pattern were first introduced and evolved from a SugarLoaf PLoP 2018 paper by Joseph Yoder, Ademar Aguiar, and Hironori Washizaki.


################################################################################


  Query Engine 
===============

You are building a system made up of many cooperating services, some of which you have created on your own with a Microservices Architecture, some of which may be obtained from third parties or a cloud provider.

How do you view all of the different log entries that each service produces and make sense out of the structure of what is being logged? How can you extract information from all of the data in these log entries?

You need to be able to create searches that can (for instance) correlate the set of calls that start with a single user request.

Therefore,

Use a query engine to treat the set of logs as a database. Formulate queries that allow you to extract related log entries and make sense out of call graphs that cross multiple service boundaries.

The combination of LogStash (which acts as a Log Aggregator to collect and transform log entries) and ElasticSearch (which allows full-text search of the collected entries) is often cited as the most common way to make sense out of the distributed logs created by a Microservices design.


################################################################################

Red/Black Deploy aka Blue/Green Deploy ===

When doing continuous deployment in a Microservices Architecture, especially with a CI-CD Pipeline, new service versions need to be rolled out quickly and safely. Classical methods of upgrading code did often require downtime, but user expectations have moved beyond that - services are expected to be able to be available.

How can we safely and quickly deploy new versions of services?

With cloud native environments, it becomes feasible to run both versions of the service in parallel. Classically, this would be too expensive as having a spare set of servers just for upgrades does not make business sence. These restrictions don’t hold on a cloud environment where instances can be brought up and torn down on a whim.

Therefore:

Deploy the new version on a new set of instances, running the current and new version in parallel. When the health of the new version has been verified, a simple load balancer switch can change to the new version; when problems arises, the same switch can be made back to the old version that is still active. Some time after everything checks out, the old version is torn down.

Even when applications don’t need to be available 24/7, modern development strategies often encourage a system where new versions are deployed as changes are ready. Waiting until a potential downtime window, even if possible, breaks this pattern and requires teams to work overtime and/or night shifts. So even though at first glance this strategy would only be useful for systems that cannot see downtime, it applies to a broad set of use cases and supports teams that prefer to roll out more releases each with smaller sets of changes, which is a proven method to reduce the risk associated with software upgrades (citation needed).

This pattern is known as either “red/black” or “green/blue”, but both names refer to the same strategy. The “red” or “green” cluster is the active one, the “black” or “blue” cluster is the stand-by one that runs the version that is not active.

On the public cloud, instances are paid for by the minute. It therefore makes financial sense to adopt a strategy that may seem wasteful at first. In effect, the cost of short bursts of redundant instances serves as an insurance premium against downtime. With modern deployment methods, the time that two instances are running is limited to half an hour or an hour and the additional cost is negligible.

The strategy does not prescribe the issues associated with rolling back versions, which gets progressively harder when changes in the data that the service stores are made. It is important that when red/black is adopted, that data changes are made in a manner that allows rollbacks.

The first place we encountered this pattern was in this Netflix blog, in the section “Multi-region Deployment Automation”.


################################################################################
'''